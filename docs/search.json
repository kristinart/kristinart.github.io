[
  {
    "objectID": "posts/2023-12-13-ej-redlining-la-blog/index.html",
    "href": "posts/2023-12-13-ej-redlining-la-blog/index.html",
    "title": "Environmental justice impacts of historical redlining in Los Angeles County",
    "section": "",
    "text": "1930 map of Los Angeles County neighborhoods divided into Home Owners’ Loan Corporation (HOLC) grades A (green; desirable) to D (red; undesirable). Photograph courtesy of Mapping Inequality"
  },
  {
    "objectID": "posts/2023-12-13-ej-redlining-la-blog/index.html#footnotes",
    "href": "posts/2023-12-13-ej-redlining-la-blog/index.html#footnotes",
    "title": "Environmental justice impacts of historical redlining in Los Angeles County",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGee, G. C. (2008). A multilevel analysis of the relationship between institutional and individual racial discrimination and health status. American Journal of Public Health, 98(Supplement_1), S48-S56. [DOI: 10.2105/AJPH.98.Supplement_1.S48]↩︎\nNardone, A., Rudolph, K. E., Morello-Frosch, R., & Casey, J. A. (2021). Redlines and greenspace: The relationship between historical redlining and 2010 greenspace across the United States. Environmental Health Perspectives, 129(1), 017006. [DOI: 10.1289/EHP6729]↩︎\nHoffman, J. S., Shandas, V., & Pendleton, N. (2020). The effects of historical housing policies on resident exposure to intra-urban heat: A study of 108 US urban areas. Climate, 8(1), 12. [DOI: 10.3390/cli8010012]↩︎\nEllis-Soto, D., Chapman, M., & Locke, D. H. (2023). Historical redlining is associated with increasing geographical disparities in bird biodiversity sampling in the United States. Nature Human Behaviour, 1-9. [DOI: 10.1038/s41562-022-01389-3]↩︎\nEllis-Soto, D., Chapman, M., & Locke, D. H. (2023). Historical redlining is associated with increasing geographical disparities in bird biodiversity sampling in the United States. Nature Human Behaviour, 1-9. [DOI: 10.1038/s41562-022-01389-3]↩︎"
  },
  {
    "objectID": "posts/2023-12-17-houston-blackouts-ej/index.html",
    "href": "posts/2023-12-17-houston-blackouts-ej/index.html",
    "title": "Geospatial extent of power outages in Houston, Texas caused by Winter Storm Uri in 2021",
    "section": "",
    "text": "Power lines damaged by Winter Storm Uri in February 2021. Photograph sourced from https://www.crawco.com/blog/energy-companies-deal-with-the-aftermath-of-winter-storms-uri-and-viola"
  },
  {
    "objectID": "posts/2023-12-17-houston-blackouts-ej/index.html#footnotes",
    "href": "posts/2023-12-17-houston-blackouts-ej/index.html#footnotes",
    "title": "Geospatial extent of power outages in Houston, Texas caused by Winter Storm Uri in 2021",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe Great Texas Freeze: February 11-20, 2021 (2023). National Centers for Environmental Information. National Oceanic and Atmospheric Administration. [https://www.ncei.noaa.gov/news/great-texas-freeze-february-2021]↩︎\nDoss-Gollin, J., Farnham, D.J., Lall, U., & Modi, V. (2021). How unprecedented was the February 2021 Texas cold snap? Environmental Research Letters, 16 (6). [DOI 10.1088/1748-9326/ac0278]↩︎\nThe Great Texas Freeze: February 11-20, 2021 (2023). National Centers for Environmental Information. National Oceanic and Atmospheric Administration. [https://www.ncei.noaa.gov/news/great-texas-freeze-february-2021]↩︎\nLi, Z., Li, X., Wang, Y., Quiring, S.M. (2019). Impact of climate change on precipitation patterns in Houston, Texas, USA. Anthropocene, 25. [DOI10.1016/j.ancene.2019.100193]↩︎\nLi, Z., Li, X., Wang, Y., Quiring, S.M. (2019). Impact of climate change on precipitation patterns in Houston, Texas, USA. Anthropocene, 25. [DOI10.1016/j.ancene.2019.100193]↩︎\nKaufmann, R.K., Kelly-Fair, M., Schroer, C. (2024). Distributive energy justice: Who lost power in Texas during the 2021 winter storm? Energy Research & Social Science, 109. [https://doi.org/10.1016/j.erss.2024.103416]↩︎\nKaufmann, R.K., Kelly-Fair, M., Schroer, C. (2024). Distributive energy justice: Who lost power in Texas during the 2021 winter storm? Energy Research & Social Science, 109. [https://doi.org/10.1016/j.erss.2024.103416]↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kristin Art",
    "section": "",
    "text": "Hi there!\nMy name is Kristin and I am an environmental scientist who loves playing with data! I created this website to share my professional skills and personal hobbies with people like you, so thanks for stopping by.\n\n\n\nEducation\nMS in Environmental Science & Management (expected 2024) University of California, Santa Barbara\nBS in Biological Sciences (2019) University of Southern California\n\n\n\nExperience\nEnvironmental Trainee (2023-present) Environmental Protection Agency, Washington, DC\nEnvironmental Analyst (2020-2022) San Francisco Estuary Institute, Richmond, CA"
  },
  {
    "objectID": "about.html#growing-up",
    "href": "about.html#growing-up",
    "title": "About",
    "section": "Growing Up",
    "text": "Growing Up\n\n\n\n\n\n\n\n\n\n\nI grew up in San Leandro, California and Reno, Nevada. My family moved to Reno right before my sophomore year of high school, which felt like the end of the world as a fifteen year old girl. Little did I know then that living in Reno (45 minutes away from Tahoe) would help me discover a love for the outdoors and send me down the best career path I could imagine.\nIn 2011, I transferred to a magnet high school, where I took community college courses for dual credit. It was there that I found a mentor in my microbiology course professor; I would not be where I am today without her encouragement, support, and passion! Thanks to her, I began interning at the Desert Research Institute in 2012, where I spent my time meticulously measuring Antarctic krill under a microscope (too bad I did not know about podcasts back then - I could have listened to so many true crime episodes).\nAfter receiving my Associate of Science in 2013, I decided I was ahead of the game and could afford to take a gap year. That year turned into 3.5 years away from academia… I spent the first 2.5 years volunteering with a service program focused on community development; through the program, I traveled across the contiguous US, Alaska, Hawaii, Panama, and Korea and met tons of amazing humans. After coming home, I spent another year working in the service industry and applying to transfer into a university."
  },
  {
    "objectID": "about.html#back-to-the-east-bay-and-into-water-quality-work",
    "href": "about.html#back-to-the-east-bay-and-into-water-quality-work",
    "title": "About",
    "section": "Back to the East Bay and into Water Quality Work!",
    "text": "Back to the East Bay and into Water Quality Work!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA couple weeks after graduating, I began working as an Environmental Analyst at the San Francisco Estuary Institute (SFEI). Not only was I ecstatic to be back in the East Bay Area, but the position was a wholesome blend of fieldwork, labwork, and data analysis. My team, the Nutrient Management Strategy, utilized coupled biogeochemical-hydrodynamic modeling and monitoring to understand the current and future health of the San Francisco Bay Area. I was part of the monitoring team and became the field lead of a network of water quality monitoring instruments in the San Francisco Bay called the Moored Sensor Program. This position involved everything from coordinating with US Geological Survey (USGS) collaborators for regular fieldwork to technical troubleshooting of our instruments to QA/QC of the measured data in Python. Speaking of Python, this was my first introduction to the language and boy did I struggle to learn it. It was only with the continued patience and help of my genius supervisor (and many hours on Stack Overflow) that I became proficient in the language. It’s funny to look back on now because I have since fallen in love with data science and hope to spend the majority of time during my next career position coding.\n\n\n\n\n\n\nPretending to captain the research vessel (2021)\n\n\n\n\n\n\n\nEarly morning sunrise in Alviso Slough (2021)\n\n\n\n\n\n\n\nPreparing a research buoy for deployment (2022)"
  },
  {
    "objectID": "about.html#graduate-school-and-my-shift-to-data-science-in-the-energy-sector",
    "href": "about.html#graduate-school-and-my-shift-to-data-science-in-the-energy-sector",
    "title": "About",
    "section": "Graduate School and my Shift to Data Science in the Energy Sector",
    "text": "Graduate School and my Shift to Data Science in the Energy Sector\nIn September of 2022, I began pursuing a Master’s in Environmental Science & Management at the Bren School at the University of California, Santa Barbara. I’ve focused my coursework on energy, climate, corporate sustainability, and data science. I particularly enjoy analyzing and communicating data from the energy sector. I spent the summer of 2023 interning at the Environmental Protection Agency federal headquarters in Washington, D.C., where I did data analyses and visualizations of power sector emissions for the Clean Air Markets Division. Now, most of my focus is concentrated on finishing our group master’s project, which aims to analyze the parity between potential supply and demand of hydrogen as an alternative fuel source in California."
  },
  {
    "objectID": "about.html#hobbies",
    "href": "about.html#hobbies",
    "title": "About",
    "section": "Hobbies",
    "text": "Hobbies\nIn my free time these days, I love to mountain bike, camp, cook, swim, read, and puzzle. In 2020, I went on my first bikepacking trip and fell in love. Since then, I’ve gone on 7 or so multi-day trips with amazing friends in places like Tahoe National Forest, the Lost Coast, Catalina Island, and more. Please enjoy the photos below as we collectively pretend we are in the mountains instead of behind a desk. I also love spending time with my cuddly cat, Lilo.\n\n\n\n\n\n\nBikepacking Catalina Island (2023)\n\n\n\n\n\n\n\nLord of the Squirrels trail in Whistler, Canada (aka the best trail ever; 2022)\n\n\n\n\n\n\n\n\n\nBikepacking the Tahoe Rim Trail (2021)\n\n\n\n\n\n\n\nBikepacking Catalina Island (the first time - 2021)\n\n\n\n\n\n\n\n\n\nBikepacking the Stanislaus National Forest (2022)\n\n\n\n\n\n\n\nScotts Lake in Toiyabe National Forest (2022)\n\n\n\n\n\n\n\n\n\nBikepacking the Anza-Borrego Desert (2023)\n\n\n\n\n\n\n\nBikepacking the Lost Coast (2021)"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Projects",
    "section": "",
    "text": "Predicting Residential Energy Usage based on Weather\n\n\n\n\n\n\n\nR\n\n\nQuarto\n\n\nGit\n\n\nEnergy\n\n\nMachine Learning\n\n\n\n\nMachine learning models to predict energy use\n\n\n\n\n\n\nMar 22, 2024\n\n\nKristin Art\n\n\n\n\n\n\n  \n\n\n\n\nVizualizing California’s Green Hydrogen Supply and Transportation Sector Demand\n\n\n\n\n\n\n\nR\n\n\nQuarto\n\n\nGit\n\n\nSpatial\n\n\nViz\n\n\nEnergy\n\n\n\n\nData viz for science communication\n\n\n\n\n\n\nMar 6, 2024\n\n\nKristin Art\n\n\n\n\n\n\n  \n\n\n\n\nGeospatial extent of power outages in Houston, Texas caused by Winter Storm Uri in 2021\n\n\n\n\n\n\n\nR\n\n\nQuarto\n\n\nGit\n\n\nSpatial\n\n\nEJ\n\n\nEnergy\n\n\n\n\nSpatial analysis using satellite data\n\n\n\n\n\n\nDec 16, 2023\n\n\nKristin Art\n\n\n\n\n\n\n  \n\n\n\n\nEnvironmental justice impacts of historical redlining in Los Angeles County\n\n\n\n\n\n\n\nR\n\n\nQuarto\n\n\nGit\n\n\nSpatial\n\n\nEJ\n\n\nPollution\n\n\n\n\nPresent-day effects of past injustices\n\n\n\n\n\n\nDec 13, 2023\n\n\nKristin Art\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I enjoy analyzing and visualizing data to tell stories about how electricity generation relates to emissions, ambient air quality, and environmental justice. I use both statistical analysis (including statistical machine learning algorithms) and spatial analysis to do so. I prefer to work in the R language and am in the process of populating the projects tab of this website with more of my recent work."
  },
  {
    "objectID": "about.html#graduate-school",
    "href": "about.html#graduate-school",
    "title": "About",
    "section": "Graduate School",
    "text": "Graduate School\nI am currently a second-year Master’s Student at the Bren School of Environmental Science & Management at the University of California, Santa Barbara. During my time at Bren, I have focused my coursework on data science while officially specializing in Corporate Sustainability and Energy & Climate. My main focus right now is finishing up my year-long master’s group project, which aims to analyze the parity between potential supply and demand of hydrogen as an alternative fuel source in California. Our client for the project is the California Governor’s Office of Business & Economic Development and I serve as both a Data Analyst and Financial Manager."
  },
  {
    "objectID": "about.html#environmental-protection-agency",
    "href": "about.html#environmental-protection-agency",
    "title": "About",
    "section": "Environmental Protection Agency",
    "text": "Environmental Protection Agency\n\n\n\n\n\n\n\n\n\n\n\n\n\nDuring the summer of 2023, I interned at the Environmental Protection Agency federal headquarters in Washington, DC. More specifically, I worked in the Emissions Monitoring Branch of the Clean Air Markets Division of the Office of Air and Radiation. While there, I completed analyses and visualizations of power sector data for the Clean Air Markets Division and the Emissions Monitoring Branch. For instance, I analyzed the number of electricity-generating units (EGUs) in US power plants that are associated with complex and multi-stack configurations to inform how emissions from these EGUs should be handled in federal policies like the Good Neighbor Rule. I also drafted a public webpage to communicate how consumption-based electricity generation rates differ from production-based rates in the EPA’s Emissions and Generation Integrated Resource Database (eGRID) by developing interactive visualizations."
  },
  {
    "objectID": "about.html#san-francisco-estuary-institute",
    "href": "about.html#san-francisco-estuary-institute",
    "title": "About",
    "section": "San Francisco Estuary Institute",
    "text": "San Francisco Estuary Institute\n\n\n\n\n\n\nPretending to captain the research vessel (2021)\n\n\n\n\n\n\n\nEarly morning sunrise in Alviso Slough (2021)\n\n\n\n\n\n\n\nPreparing a research buoy for deployment (2022)\n\n\n\n\n\nBefore starting my graduate program, I worked at the San Francisco Estuary Institute (SFEI) as an Environmental Analyst for 2.75 years. Not only did I love being back in the East Bay Area (where I grew up), but I also appreciated that the position was a wholesome blend of fieldwork, lab work, and data analysis. My team, the Nutrient Management Strategy, utilized coupled biogeochemical-hydrodynamic modeling and monitoring to understand the current and future health of the San Francisco Bay Area. I was part of the monitoring team and became the field lead of a network of water quality monitoring instruments in the San Francisco Bay called the Moored Sensor Program. This position involved everything from coordinating with US Geological Survey (USGS) collaborators for regular fieldwork to technical troubleshooting of our instruments to QA/QC of the measured data in Python.\nSpeaking of Python, this was my first introduction to the language, and boy, did I struggle to learn it. It was only with the continued patience and help of my genius supervisor (and many hours on Stack Overflow) that I became somewhat proficient in the language. It’s funny to look back on now because I have since fallen in love with data science."
  },
  {
    "objectID": "about.html#university-of-southern-california",
    "href": "about.html#university-of-southern-california",
    "title": "About",
    "section": "University of Southern California",
    "text": "University of Southern California\nBefore working at SFEI, I attended the University of Southern California (USC) between January 2017 - December 2019. I graduated with a Bachelor of Science in Biological Sciences and an emphasis in Marine and Environmental Biology. Not only was I lucky enough to receive a full ride (since I am a first-generation college student from a low-income family), but I also assisted two research labs while there. In the research labs, I helped multiple PhD students study the growth and toxicity of phytoplankton species under various climate conditions. Although I have since steered my career away from this topic, I still love geeking out about phytoplankton and harmful algal blooms."
  },
  {
    "objectID": "posts/2024-03-06-h2-data-viz/index.html",
    "href": "posts/2024-03-06-h2-data-viz/index.html",
    "title": "Vizualizing California’s Green Hydrogen Supply and Transportation Sector Demand in 2030",
    "section": "",
    "text": "Power lines damaged by Winter Storm Uri in February 2021. Photograph sourced from https://www.crawco.com/blog/energy-companies-deal-with-the-aftermath-of-winter-storms-uri-and-viola"
  },
  {
    "objectID": "posts/2024-03-06-h2-data-viz/index.html#introduction",
    "href": "posts/2024-03-06-h2-data-viz/index.html#introduction",
    "title": "Vizualizing California’s Green Hydrogen Supply and Transportation Sector Demand in 2030",
    "section": "",
    "text": "I created the infographic below to explore the spatial distribution of hydrogen supply and demand across California in 2030. More specifically, I compared estimates of green (renewable) hydrogen production through wind- and solar-based electrolysis against projections of demand for hydrogen fuel (not necessarily renewable) from the transportation sector in 2030.\nI developed the infographic to answer the following questions:\n\nWhat is the spatial distribution of electrolytic hydrogen supply and transportation sector demand in 2030?\nHow does electrolytic supply and transportation sector demand for hydrogen fuel compare within and between regions in 2030? Will individual regions be able to produce enough hydrogen through electrolysis to meet local transportation sector demand?\nWill individual counties in the Southern California region be able to produce enough hydrogen through electrolysis to meet local transportation sector demand?\n\n\n\n\n\nPower lines damaged by Winter Storm Uri in February 2021. Photograph sourced from https://www.crawco.com/blog/energy-companies-deal-with-the-aftermath-of-winter-storms-uri-and-viola\n\n\n\n\n\n\nI used data developed during my year-long Master’s Group Project, which aims to analyze the parity between potential supply of hydrogen through renewable resources and demand from California’s transportation sector, which will be the earliest large-scale adopter of hydrogen fuel.\nI used locational estimates of daily hydrogen production (kilograms/ day) through wind- or solar-based electrolysis that we developed by identifying renewable resource potential across space, excluding area unsuitable for renewable development, and siting electrolyzer (hydrogen production technology) facilities through an optimization model. I also used locational projections of transportation sector demand for hydrogen fuel in 2030 that were developed through a comprehensive technoeconomic study by the UC Davis Institute of Transportation Studies (Fulton et al., 2023). These data include the total quantities of hydrogen fuel (kilogram/ day) that will be demanded at future hydrogen re-fueling stations that will serve light-, medium-, and heavy-duty fuel cell electric vehicles (FCEV)."
  },
  {
    "objectID": "posts/2024-03-06-h2-data-viz/index.html#design-approach",
    "href": "posts/2024-03-06-h2-data-viz/index.html#design-approach",
    "title": "Vizualizing California’s Green Hydrogen Supply and Transportation Sector Demand",
    "section": "Design Approach",
    "text": "Design Approach\n\nGraphic Form\nIt took a lot of exploratory data analysis and many draft figures to decide which graphic forms to use. I decided to display the actual spatial locations through a map and then explored many different aggregations and displays for the other components.\n\n\nCode\n# .........................S&D Map.........................\n\n# plot  ----\np2 &lt;-\n  # plot\n  ggplot() +\n  geom_sf(data = ca_sf) +\n  geom_sf(data = h2_counties_sf, aes(fill = region, shape = \"Demand Center\"), alpha = 0.5, color = \"transparent\") +\n  geom_sf(data = elect_demand, aes(size = demand_capacity_served), col = \"grey40\", alpha = 0.7) +\n\n  # scale\n  scale_size(range = c(2, 5)) +\n  geom_sf(data = elect_renewables2, aes(shape = \"Production Site\"), col = \"black\", size = 2.5, alpha = 0.7) +\n  scale_fill_manual(\n    values = pal_regions,\n    breaks = c(\"Northern CA\", \"Central CA\", \"Coastal CA\", \"Southern CA\"),\n    guide = \"none\"\n  ) +\n  scale_x_continuous(limits = c(-125, -114)) +\n\n  # add labels and annotation\n  labs(\n    size = \"Demand (kg/ day)\",\n    shape = \"Location Type\",\n    title = \"Spatial Distribution of H2 Production and Demand (2030)\",\n  ) +\n  annotate(\n    geom = \"label\",\n    x = c(-123.8, -117, -122.1, -121.2),\n    y = c(39.6, 36.7, 35.9, 33.3),\n    label = c(\"Northern CA\", \"Central CA\", \"Coastal CA\", \"Southern CA\"),\n    color = pal_regions,\n    size = 4,\n    color = \"black\",\n    family = \"mont\",\n    fontface = \"bold\",\n    label.size = 1\n  ) +\n\n  # adjust theme\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 11, family = \"mont\", face = \"bold\", color = \"grey20\"),\n    plot.title.position = \"plot\",\n    plot.subtitle = element_text(size = 10, color = \"grey50\", family = \"mont\"),\n    legend.title = element_text(size = 10, family = \"mont\", face = \"bold\"),\n    legend.text = element_text(size = 9, family = \"mont\"),\n    legend.position = c(0.78, 0.74),\n    legend.background = element_rect(fill = alpha(\"grey97\", 0.5), color = \"grey80\"),\n    axis.title = element_blank(),\n    axis.text = element_blank(),\n    panel.grid = element_blank()\n  ) +\n  guides(\n    size = guide_legend(order = 2),\n    shape = guide_legend(order = 1, override.aes = list(color = c(\"grey50\", \"black\")))\n  )\n\np2\n\n\n\n\n\nI let the story hidden in the data guide the final product, as the most interesting findings were related to whether Southern California could meet its high daily demand. Once I had nailed down the narrative, I experimented with which types of plots and shapes would look best stitched together into one figure.\n\n\nText\nI used various text forms to explicitly state the story since most people are not familiar with the hydrogen space. I found it challenging to decide how much to explain since there is so much jargon related to this niche topic. Ultimately, I decided to include a substantial amount of background paragraphs to give the viewer meaningful context. I also used annotations on subplots to point out the most interesting results. I removed redundant axis labels and included meaningful titles for the subplots.\n\n\nThemes, Colors, & Typography\nI removed or lightened all the plot grid lines and used a white background for the infographic to reduce the amount of unnecessary ink included. I used the Montserrat font family throughout the infographic because my Master’s Group Project used it for our deliverables; that may be silly, but I found it really satisfying to stay consistent and also believe Montserrat invokes positive emotions and friendliness. I also used a slab serif, Alfa Slab One, to make the title stand out.\nI developed a pleasing color palette by using coolors.co and made sure they were accessible for people with visual impairments through the website’s tools. I intentionally used the same 4 distinctive colors to represent the regions of California across all the subplots and increased their transparency at times to improve readability.\n\n\nCode\n## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                             Data visualization                           ----\n## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# .........................Regional Histogram.........................\n# define background text ----\np1_text &lt;- glue::glue((\n  \"\n       Hydrogen fuel (H2) can be produced by combining water and\n       electricity through a process called electrolysis. When\n       renewable electricity (new wind and solar) is used in this\n       process, the resulting H2 is also considered renewable and\n       referred to as green H2. H2 produced through most other\n       processes have a negative environmental impact due to the\n       release of carbon emissions or air pollutants.\n\n       Both production and demand for H2 in California are expected\n       to increase in coming years. The transportation sector is\n       expected to be the earliest adopter of H2 for use in light-,\n       medium- and heavy-duty fuel cell electric vehicles (FCEVs).\n       Transportation demand is concentrated along major highways\n       and within densely populated neighborhoods.\n\n       Green H2 production is constrained to areas with sufficient\n       wind and solar resources, water availability, and favorable\n       permitting and regulation policies. Not all of these viable\n       areas align with transportation demand hotspots, meaning H2\n       distribution (which can be costly) and production through\n       other processes (which are not all renewable) may play an\n       important role in some places.\n\n       In 2030, Northern, Central, Coastal, and Southern CA will\n       all be able to produce far more than enough green H2 to\n       meet demand. Central and Northern CA will be the largest\n       producers of electrolytic hydrogen while Southern CA will\n       be the largest demander of H2 in transportation uses. All\n       the excess green H2 produced can be stored for later use\n       or distributed to areas in need.\n\n\"))\n\n#p1_text\n\n# pivot longer ----\nh2_regions_long &lt;- h2_counties %&gt;%\n  rename(Demand = demand_h2_kg_d, Production = supply_h2_kg_d) %&gt;%\n  pivot_longer(cols = Demand:Production, values_to = \"count\", names_to = \"type\") %&gt;%\n  mutate(\n    x = case_when(\n      type == \"Demand\" ~ 1,\n      type == \"Production\" ~ 2\n    ),\n    region = as.factor(region)\n  ) %&gt;%\n  group_by(region, type) %&gt;%\n  summarize(count = sum(count))\n\n# plot ----\np1 &lt;-\n  # format data\n  h2_regions_long %&gt;%\n  mutate(\n    region = factor(region, levels = (c(\"Central CA\", \"Northern CA\", \"Coastal CA\", \"Southern CA\"))),\n    type = case_when(\n      type == \"Demand\" ~ \"Demand (kg/d)\",\n      type == \"Production\" ~ \"Production (kg/d)\"\n    ),\n    type = factor(type, levels = rev(c(\"Demand (kg/d)\", \"Production (kg/d)\")))\n  ) %&gt;%\n  # plot and scale\n  ggplot(aes(x = region, y = count, pattern = type)) +\n  geom_col_pattern(fill = pal_regions2, position = \"dodge\", alpha = 0.8, color = \"grey0\") +\n  scale_pattern_manual(values = c(\"none\", \"wave\")) +\n  scale_pattern_fill_manual(values = c(\"black\", \"white\")) +\n  scale_y_continuous(expand = c(0.02, 0), labels = scales::label_number(scale_cut = cut_short_scale())) +\n  expand_limits(y = max(7000000)) +\n\n  # add labels and annotation\n  annotate(\"label\", x = 3.4, y = 5500000, label = \"Southern CA will\\ndemand the most\\nH2 fuel\", size = 4, family = \"mont\") +\n  annotate(\n    geom = \"curve\",\n    x = 4.0, xend = 4.25,\n    y = 5400000, yend = 1500000,\n    curvature = -0.5,\n    arrow = arrow(length = unit(0.3, \"cm\")),\n    alpha = 0.5\n  ) +\n  geom_text(aes(label = (c(\n    \"82k\", \"5.9M\",\n    \"35k\", \"1.9M\",\n    \"46k\", \"4.9M\",\n    \"252k\", \"1.8M\"\n  ))), color = \"black\", vjust = -0.9, hjust = 0.5, position = position_dodge(width = 0.9), family = \"mont\", fontface = \"bold\") +\n  labs(\n    y = \"\", x = \"\", subtitle = \"Regional Hydrogen Production and Demand (2030)\", #title = (p1_text)\n  ) +\n\n  # adjust theme\n  theme_minimal() +\n  theme(\n    #plot.title = element_text(size = 10, family = \"mont\", color = \"grey20\", margin = margin(b = 5, l = 20)),\n    #plot.title.position = \"plot\",\n    plot.subtitle = element_text(size = 14, face = \"bold\", family = \"mont\", hjust = 0.5, color = \"grey20\"),\n    legend.title = element_blank(),\n    legend.text = element_text(size = 12, family = \"mont\"), #, margin = margin(r = -3, unit = \"lines\")\n    legend.direction = \"horizontal\", \n    legend.position = \"top\",\n    legend.key.size = unit(0.5, \"cm\"),\n    #axis.title = element_text(size = 14, family = \"mont\"),\n    #axis.title.x = element_blank(),\n    axis.text.x = element_text(size = 14, family = \"mont\"),\n    axis.text.y = element_blank(),\n    panel.grid = element_blank(),\n    aspect.ratio = 0.4\n  ) +\n  guides(fill = guide_legend(reverse = TRUE))\n\np1\n\n\n\n\n\nI would also like to color the region names in the text using those same 4 colors, but will have to do so when I have more time to troubleshoot the tension between my graphic device and element_markdown()displays…\n\n\nGeneral Design\nI designed the overall layout of the canvas to create a visual hierarchy. Since most people read in an “F” or “E” formation, I placed the background text (which I want to be read first) on the very left-hand side and the deep-dive into Southern California counties (which I want to be read later on) on the right-hand side. I also bounded the latter in a bordered call-out box with the same background color as the Southern California region to signal to the viewer that it is an important deep dive. I also tried to limit the overall amount of ink used in plots when possible and isolated the text to a few key locations on the overall canvas.\nWithin the histogram plot, I ordered the regional values from highest to lowest production starting from the left-hand side; this allowed the fact that Southern CA has the lowest regional production to stand out in contrast to the text label that communicated Southern CA has the highest regional demand. In the dumbbell plot, I ordered the counties from largest to smallest difference starting from the top because I wanted to highlight the magnitude of the large differences first.\n\n\nCode\n# .........................SoCal Dumbbell Plot.........................\n# define text ----\np3_text &lt;- glue::glue(\n  \"   In Southern California, most individual counties will be\\n\ndaily net producers of green H2 in 2030. LA coungh is the only\\n\none that cannot produce enough green H2 to meet its local\\n\ntransportation demand. This means distribution of H2 from\\n\nsurrounding counties may play an important role for them. This\\n\ncould also mean that LA will rely on H2 from other production\\n\npathways that have a higher environmental footprint than\\n\nelectrolytic green H2.\n\"\n)\n\n# define function to rewrite values in shorthand format ----\ncustom_number_format &lt;- function(x) {\n  ifelse(x &gt;= 1e6,\n    gsub(\" \", \"\", sprintf(\"%sM\", format(round(x / 1e6, 1), nsmall = 1, big.mark = \"\"))),\n    gsub(\" \", \"\", sprintf(\"%sk\", format(round(x / 1e3), big.mark = \"\")))\n  )\n}\n\n# tidy data labels and values for dotted line in next plot ----\nsocal_counties &lt;- h2_counties %&gt;%\n  filter(county %in% southern_ca_counties) %&gt;%\n  mutate(\n    difference_abbr = (custom_number_format(difference)),\n    # difference_abbr = ifelse(difference_description == \"more supply\", \"+282k\", difference_abbr),\n    difference_description = case_when(\n      (supply_h2_kg_d - demand_h2_kg_d) &gt; 0 ~ \"more supply\",\n      (supply_h2_kg_d - demand_h2_kg_d) &lt; 0 ~ \"more demand\"\n    ),\n    dotted_line = ifelse(difference_description == \"more supply\", demand_h2_kg_d, supply_h2_kg_d)\n  ) %&gt;%\n  filter(county != \"Santa Barbara\")\n\n# plot ----\np3 &lt;-\n  # plot\n  socal_counties %&gt;%\n  ggplot() +\n  geom_segment(aes(x = 0, xend = dotted_line, y = reorder(county, difference), yend = reorder(county, difference)), alpha = 0.4, linetype = \"dotted\", linewidth = 0.5) +\n  geom_segment(aes(x = demand_h2_kg_d, xend = supply_h2_kg_d, y = reorder(county, difference), yend = reorder(county, difference), linetype = \"Difference\"), alpha = 0.4, linewidth = 1.5, color = \"#A05F54\") +\n  geom_point(aes(x = supply_h2_kg_d, y = reorder(county, difference), shape = \"Production\"), size = 2.5, fill = \"#D19180\", color = \"grey20\") +\n  geom_point(aes(x = demand_h2_kg_d, y = reorder(county, difference), shape = \"Demand\"), size = 2.5, fill = \"#9C6F6F\", color = \"grey20\") +\n\n  # scale\n  scale_shape_manual(values = c(\"Production\" = 24, \"Demand\" = 21), breaks = c(\"Production\", \"Demand\")) +\n  scale_x_continuous(labels = scales::label_number(scale_cut = cut_short_scale()), limits = c(0, 900000)) +\n\n  # add labels\n  labs(y = \"\", x = \"Hydrogen (kg/ day)\", subtitle = \"Difference between Daily Hydrogen Production and \\nDemand in Southern California Counties (2030)\"\n       #, title = p3_text\n       ) +\n  geom_text(\n    aes(\n      x = (demand_h2_kg_d + supply_h2_kg_d) / 2,\n      y = reorder(county, difference),\n      label = difference_abbr,\n      family = \"mont\",\n    ),\n    position = position_nudge(y = 0.4),\n    show.legend = FALSE,\n    size = 4\n  ) +\n\n  # adjust theme\n  theme_minimal() +\n  theme(\n    #plot.title = element_text(size = 9, hjust = 0, family = \"mont\", margin = margin(b = 20)),\n    #plot.title.position = \"plot\",\n    plot.subtitle = element_text(size = 12, hjust = 0.5, family = \"mont\", face = \"bold\", margin = margin(b = 10)),\n    legend.title = element_blank(),\n    legend.text = element_text(size = 10, family = \"mont\", face = \"bold\"),\n    legend.direction = \"horizontal\",\n    legend.position = \"top\",\n    legend.justification = c(4, 0),\n    legend.spacing.x = unit(-0, \"lines\"),\n    legend.margin = margin(-5, -5, 0, 0),\n    axis.title = element_text(size = 12, family = \"mont\"),\n    axis.title.x = element_text(family = \"mont\", margin = margin(t = 8)),\n    axis.text = element_text(size = 12, family = \"mont\"),\n    axis.text.y = element_text(size = 12, family = \"mont\"),\n    panel.grid = element_line(color = \"grey97\"),\n    panel.grid.major.y = element_blank(),\n    panel.grid.major.x = element_line(linewidth = 0.3),\n    plot.background = element_rect(colour = \"black\", fill = alpha(\"#9C6F6F\", 0.3), size = 5),\n    plot.margin = margin(t = 30, b = 30, r = 22, l = 22),\n    aspect.ratio = 0.6\n  ) +\n  guides(\n    linetype = guide_legend(order = 1, override.aes = list(x = -5, y = 1)),\n    shape = guide_legend(order = 2, override.aes = list(values = c(), fill = c(\"#D19180\", \"#9C6F6F\"), shape = c(24, 21)))\n  ) +\n\n  # add annotation\n  coord_cartesian(clip = \"off\") +\n  annotate(\"label\", x = 550000, y = 2, label = \"LA is the only county \\nthat cannot produce enough \\ngreen H2 to meet demand\", size = 4, family = \"mont\") +\n  annotate(\n    geom = \"curve\",\n    x = 270000, xend = 85000,\n    y = 1.8, yend = 1.2,\n    curvature = 0.3,\n    arrow = arrow(length = unit(0.3, \"cm\")),\n    alpha = 0.5\n  )\n\np3\n\n\n\n\n\nCode\n# add to blank plot to create white space ----\n# p3 &lt;- ggplot() +\n#   theme_void() +\n#   theme(\n#     plot.margin = margin(t = 20, r = 5, b = 20, l = 5, \"pt\")\n#   ) +\n#   inset_element(p3, left = 0.02, bottom = 0.05, right = 0.95, top = 0.98, align_to = \"full\")\n\n#p3\n\n\n\n\nContextualization and DEI\nI intend for this infographic to be used in a personal blog post about my Master’s Group Project, meaning that it will be combined with additional text that explains the caveats, implications, and findings of that project in greater detail. With this in mind, I only included a moderate amount of contextualization in the figure itself and did not discuss the equity considerations about hydrogen production and use. The purpose of the infographic itself is just to convey the spatial distribution of potential supply and demand to highlight the fact that all of the 2030 transportation sector demand can be met by electrolytic hydrogen alone."
  },
  {
    "objectID": "posts/2024-03-06-h2-data-viz/index.html#footnotes",
    "href": "posts/2024-03-06-h2-data-viz/index.html#footnotes",
    "title": "Vizualizing California’s Green Hydrogen Supply and Transportation Sector Demand",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFulton, L.; Jenn, A.; Yang, C.; Burke, A.; Acharya, T.; Li, X., et al. (2023). California Hydrogen Analysis Project: The Future Role of Hydrogen in a Carbon-Neutral California: Final Synthesis Modeling Report. UC Davis: Hydrogen Pathways Program. Retrieved from https://escholarship.org/uc/item/27m7g841↩︎"
  },
  {
    "objectID": "posts/2024-03-06-h2-data-viz/index.html#code",
    "href": "posts/2024-03-06-h2-data-viz/index.html#code",
    "title": "Vizualizing California’s Green Hydrogen Supply and Transportation Sector Demand in 2030",
    "section": "Code",
    "text": "Code\n\n\nCode\n## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                                    Setup                                 ----\n## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# .........................load libraries.........................\nlibrary(tidyverse)\nlibrary(here)\nlibrary(sf)\nlibrary(ggspatial)\nlibrary(scales)\nlibrary(patchwork)\nlibrary(cowplot)\nlibrary(ggpubr)\nlibrary(ggtext)\nlibrary(ggpattern)\nlibrary(showtext)\nlibrary(magick)\n\n\n# ..........................load fonts...........................\nfont_add_google(name = \"Montserrat\", family = \"mont\")\nfont_add_google(name = \"Alfa Slab One\", family = \"alfa\")\nshowtext_auto()\n\n# .........................define color palettes.........................\n# define color palettes, from coolors\n# n = 4\npal_regions &lt;- c(\n  \"#64865B\", \"#664C5E\", \"#4F7D7A\", \"#D19180\"\n)\n\n# n = 8\npal_regions2 &lt;- c(\n  \"#5B656C\", \"#664C5E\",\n  \"#719B82\", \"#4F7D7A\",\n  \"#86A77D\", \"#64865B\",\n  \"#9C6F6F\", \"#D19180\"\n)\n\n# ..........................import data...........................\n# load CA polygon\nca_sf &lt;- spData::us_states %&gt;%\n  filter(GEOID == \"06\") %&gt;%\n  st_transform(crs = \"EPSG:4326\")\n\n# load ca county outlines\ncounties_sf &lt;- read_sf(here::here(\"data/census/tl_2022_us_county/tl_2022_us_county.shp\")) %&gt;%\n  filter(STATEFP == \"06\") %&gt;% # FIPS code for CA is 06\n  janitor::clean_names() %&gt;%\n  dplyr::select(countyfp, geoid, name, namelsad, geometry) %&gt;%\n  st_transform(crs = \"EPSG:4326\")\n\n# load transportation demand\ndemand_mod &lt;- st_read(here::here(\"data/fueling_demand_2030/fueling_demand_2030.shp\"), quiet = TRUE) %&gt;%\n  st_transform(crs = \"EPSG:4326\")\n\n# load combined demand electrolyzer outputs from script 05\nelect_demand &lt;- st_read(here::here(\"data/electrolyzers_output/electrolyzer_demand_limited_baseline_uncapped.geojson\"), quiet = TRUE) %&gt;%\n  st_transform(crs = \"EPSG:4326\") ## THIS INCLUDES GEOMS OF THE ELECTROLYZER POINTS\n\n# load combined renewables electrolyzer outputs from script 05\nelect_renewables &lt;- st_read(here::here(\"data/electrolyzers_output/electrolyzer_renewables_limited_baseline_uncapped.geojson\"), quiet = TRUE) %&gt;%\n  st_transform(crs = \"EPSG:4326\") ## THIS INCLUDES GEOMS OF THE ELECTROLYZER POINTS\n\n# load combined renewables electrolyzer outputs from script 05\nelect_renewables2 &lt;- st_read(here::here(\"data/electrolyzers_output/electrolyzer_renewables_renewablegeoms_limited_baseline_uncapped.geojson\"), quiet = TRUE) %&gt;%\n  st_transform(crs = \"EPSG:4326\") ## THIS INCLUDES GEOMS OF THE RENEWABLE POINTS\n\n## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                          Data wrangling / cleaning                       ----\n## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# .........................tidy and combine data.........................\n# spatially join county geoms to demand and supply data ----\n# demand_counties &lt;- demand_mod %&gt;% st_join(counties_sf, join = st_intersects)\ndemand_counties &lt;- demand_mod %&gt;% st_join(counties_sf, join = st_nearest_feature)\n\n# check that the total demand match before and after the spatial join\nsum(demand_counties$fueling) == sum(demand_mod$fueling)\n\nsupply_counties &lt;- elect_renewables2 %&gt;% st_join(counties_sf, join = st_nearest_feature) # for supply geoms based on renewables\n\n# sum county-wide hydrogen in each df ----\nh2_demand_counties &lt;- demand_counties %&gt;%\n  st_drop_geometry() %&gt;% # drop geometries\n  group_by(name) %&gt;% # name = county name\n  summarize(demand_h2_kg_d = sum(fueling)) # sum the amount of demand (kg/ d) within each county\n\nh2_supply_counties &lt;- supply_counties %&gt;%\n  st_drop_geometry() %&gt;% # drop geometries\n  group_by(name) %&gt;% # name = county name\n  summarize(supply_h2_kg_d = sum(potential_h2_kg_d)) # sum the amount of supply (kg/ d) within each county\n\n# combine demand and supply data ----\nh2_counties &lt;- h2_demand_counties %&gt;%\n  full_join(h2_supply_counties, by = \"name\") %&gt;% # join by county name\n  mutate(across(everything(), ~ ifelse(is.na(.), 0, .))) %&gt;% # replace NAs\n  rename(\"county\" = \"name\") %&gt;% # rename name column to county\n  mutate(difference = (supply_h2_kg_d - demand_h2_kg_d)) %&gt;% # calculate difference between supply and demand amounts\n  arrange(desc(difference)) # arrange in order based on difference\n\n# id counties that need to be added to df\nadd_counties &lt;- cat(paste(sprintf('\"%s\"', sort(counties_sf$name[!(counties_sf$name %in% h2_counties$county)])), collapse = \", \"))\n\n# add counties with 0 supply and 0 demand ----\nno_h2_counties &lt;- data.frame(\n  county = c(\"Amador\", \"Colusa\", \"Del Norte\", \"Kings\", \"Lake\", \"Lassen\", \"Mendocino\", \"Modoc\", \"Mono\", \"Napa\", \"San Francisco\", \"San Luis Obispo\", \"Santa Barbara\", \"Santa Cruz\", \"Sierra\", \"Sonoma\", \"Stanislaus\", \"Sutter\", \"Trinity\", \"Yolo\", \"Yuba\"),\n  supply_h2_kg_d = 0,\n  demand_h2_kg_d = 0,\n  difference = 0\n)\n\n# add new rows to the existing dataframe using rbind\nh2_counties &lt;- rbind(h2_counties, no_h2_counties)\n\n\n# .........................define state regions.........................\n# define counties within each state region. based on https://ww2.arb.ca.gov/lcti-central-region\ncoastal_ca_counties &lt;- c(\"Alameda\", \"Contra Costa\", \"Marin\", \"Monterey\", \"Napa\", \"San Benito\", \"San Francisco\", \"San Mateo\", \"San Luis Obispo\", \"Santa Clara\", \"Santa Cruz\", \"Sonoma\", \"Solano\")\n\nnorthern_ca_counties &lt;- c(\"Alpine\", \"Amador\", \"Butte\", \"Calaveras\", \"Colusa\", \"Del Norte\", \"El Dorado\", \"Glenn\", \"Humboldt\", \"Lake\", \"Lassen\", \"Mendocino\", \"Modoc\", \"Nevada\", \"Placer\", \"Plumas\", \"Sacramento\", \"Shasta\", \"Sierra\", \"Siskiyou\", \"Sutter\", \"Tehama\", \"Trinity\", \"Yolo\", \"Yuba\")\n\ncentral_ca_counties &lt;- c(\"Fresno\", \"Inyo\", \"Kern\", \"Kings\", \"Madera\", \"Mariposa\", \"Merced\", \"Mono\", \"San Joaquin\", \"Stanislaus\", \"Tulare\", \"Tuolumne\")\n\nsouthern_ca_counties &lt;- c(\"Imperial\", \"Los Angeles\", \"Orange\", \"Riverside\", \"San Bernardino\", \"San Diego\", \"Santa Barbara\", \"Ventura\")\n\n# add regions and difference groupings to df ----\nh2_counties &lt;- h2_counties %&gt;%\n  mutate(\n    region = case_when(\n      county %in% northern_ca_counties ~ \"Northern CA\",\n      county %in% southern_ca_counties ~ \"Southern CA\",\n      county %in% central_ca_counties ~ \"Central CA\",\n      county %in% coastal_ca_counties ~ \"Coastal CA\"\n    ),\n    difference_description = case_when(\n      (supply_h2_kg_d - demand_h2_kg_d) &gt; 0 ~ \"more supply\",\n      (supply_h2_kg_d - demand_h2_kg_d) &lt; 0 ~ \"more demand\"\n    )\n  )\n\n# add county geometries back to full df ---\nh2_counties_sf &lt;- left_join(counties_sf, h2_counties, by = c(\"name\" = \"county\")) %&gt;%\n  mutate(region = ifelse(name == \"San Francisco\", \"Northern CA\", region))\n\n\n## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                             Data visualization                           ----\n## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# .........................Regional Histogram.........................\n# define background text ----\np1_text &lt;- glue::glue((\n  \"\n       Hydrogen fuel (H2) can be produced by combining water and\n       electricity through a process called electrolysis. When\n       renewable electricity (new wind and solar) is used in this\n       process, the resulting H2 is also considered renewable and\n       referred to as green H2. H2 produced through most other\n       processes have a negative environmental impact due to the\n       release of carbon emissions or air pollutants.\n\n       Both production and demand for H2 in California are expected\n       to increase in coming years. The transportation sector is\n       expected to be the earliest adopter of H2 for use in light-,\n       medium- and heavy-duty fuel cell electric vehicles (FCEVs).\n       Transportation demand is concentrated along major highways\n       and within densely populated neighborhoods.\n\n       Green H2 production is constrained to areas with sufficient\n       wind and solar resources, water availability, and favorable\n       permitting and regulation policies. Not all of these viable\n       areas align with transportation demand hotspots, meaning H2\n       distribution (which can be costly) and production through\n       other processes (which are not all renewable) may play an\n       important role in some places.\n\n       In 2030, Northern, Central, Coastal, and Southern CA will\n       all be able to produce far more than enough green H2 to\n       meet demand. Central and Northern CA will be the largest\n       producers of electrolytic hydrogen while Southern CA will\n       be the largest demander of H2 in transportation uses. All\n       the excess green H2 produced can be stored for later use\n       or distributed to areas in need.\n\n\"))\n\np1_text\n\n# pivot longer ----\nh2_regions_long &lt;- h2_counties %&gt;%\n  rename(Demand = demand_h2_kg_d, Production = supply_h2_kg_d) %&gt;%\n  pivot_longer(cols = Demand:Production, values_to = \"count\", names_to = \"type\") %&gt;%\n  mutate(\n    x = case_when(\n      type == \"Demand\" ~ 1,\n      type == \"Production\" ~ 2\n    ),\n    region = as.factor(region)\n  ) %&gt;%\n  group_by(region, type) %&gt;%\n  summarize(count = sum(count))\n\n# plot ----\np1 &lt;-\n  # format data\n  h2_regions_long %&gt;%\n  mutate(\n    region = factor(region, levels = (c(\"Central CA\", \"Northern CA\", \"Coastal CA\", \"Southern CA\"))),\n    type = case_when(\n      type == \"Demand\" ~ \"Demand (kg/d)\",\n      type == \"Production\" ~ \"Production (kg/d)\"\n    ),\n    type = factor(type, levels = rev(c(\"Demand (kg/d)\", \"Production (kg/d)\")))\n  ) %&gt;%\n  # plot and scale\n  ggplot(aes(x = region, y = count, pattern = type)) +\n  geom_col_pattern(fill = pal_regions2, position = \"dodge\", alpha = 0.8, color = \"grey0\") +\n  scale_pattern_manual(values = c(\"none\", \"wave\")) +\n  scale_pattern_fill_manual(values = c(\"black\", \"white\")) +\n  scale_y_continuous(expand = c(0.02, 0), labels = scales::label_number(scale_cut = cut_short_scale())) +\n  expand_limits(y = max(7000000)) +\n\n  # add labels and annotation\n  annotate(\"label\", x = 3.4, y = 5500000, label = \"Southern CA will\\ndemand the most\\nH2 fuel\", size = 3, family = \"mont\") +\n  annotate(\n    geom = \"curve\",\n    x = 4.0, xend = 4.25,\n    y = 5400000, yend = 1500000,\n    curvature = -0.5,\n    arrow = arrow(length = unit(0.3, \"cm\")),\n    alpha = 0.5\n  ) +\n  geom_text(aes(label = (c(\n    \"82k\", \"5.9M\",\n    \"35k\", \"1.9M\",\n    \"46k\", \"4.9M\",\n    \"252k\", \"1.8M\"\n  ))), color = \"black\", vjust = -0.9, hjust = 0.5, position = position_dodge(width = 0.9), family = \"mont\", fontface = \"bold\") +\n  labs(\n    y = \"\", x = \"\", subtitle = \"Regional Hydrogen Production and Demand (2030)\", title = (p1_text)\n  ) +\n\n  # adjust theme\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 10, family = \"mont\", color = \"grey20\", margin = margin(b = 5, l = 20)),\n    plot.title.position = \"plot\",\n    plot.subtitle = element_text(size = 12, face = \"bold\", family = \"mont\", hjust = 0.5, color = \"grey20\"),\n    legend.title = element_blank(),\n    legend.text = element_text(size = 10, family = \"mont\", margin = margin(r = -3, unit = \"lines\")),\n    legend.direction = \"horizontal\",\n    legend.position = \"top\",\n    legend.key.size = unit(0.5, \"cm\"),\n    axis.title = element_text(size = 12, family = \"mont\"),\n    axis.title.x = element_blank(),\n    axis.text.x = element_text(size = 11, family = \"mont\"),\n    axis.text.y = element_blank(),\n    panel.grid = element_blank(),\n    aspect.ratio = 0.4\n  ) +\n  guides(fill = guide_legend(reverse = TRUE))\n\np1\n\n# .........................S&D Map.........................\n\n# plot  ----\np2 &lt;-\n  # plot\n  ggplot() +\n  geom_sf(data = ca_sf) +\n  geom_sf(data = h2_counties_sf, aes(fill = region, shape = \"Demand Center\"), alpha = 0.5, color = \"transparent\") +\n  geom_sf(data = elect_demand, aes(size = demand_capacity_served), col = \"grey40\", alpha = 0.7) +\n\n  # scale\n  scale_size(range = c(2, 5)) +\n  geom_sf(data = elect_renewables2, aes(shape = \"Production Site\"), col = \"black\", size = 2.5, alpha = 0.7) +\n  scale_fill_manual(\n    values = pal_regions,\n    breaks = c(\"Northern CA\", \"Central CA\", \"Coastal CA\", \"Southern CA\"),\n    guide = \"none\"\n  ) +\n  scale_x_continuous(limits = c(-125, -114)) +\n\n  # add labels and annotation\n  labs(\n    size = \"Demand (kg/ day)\",\n    shape = \"Location Type\",\n    title = \"Spatial Distribution of H2 Production and Demand (2030)\",\n  ) +\n  annotate(\n    geom = \"label\",\n    x = c(-123.8, -117, -122.1, -121.2),\n    y = c(39.6, 36.7, 35.9, 33.3),\n    label = c(\"Northern CA\", \"Central CA\", \"Coastal CA\", \"Southern CA\"),\n    color = pal_regions,\n    size = 4,\n    color = \"black\",\n    family = \"mont\",\n    fontface = \"bold\",\n    label.size = 1\n  ) +\n\n  # adjust theme\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 11, family = \"mont\", face = \"bold\", color = \"grey20\"),\n    plot.title.position = \"plot\",\n    plot.subtitle = element_text(size = 10, color = \"grey50\", family = \"mont\"),\n    legend.title = element_text(size = 10, family = \"mont\", face = \"bold\"),\n    legend.text = element_text(size = 9, family = \"mont\"),\n    legend.position = c(0.78, 0.74),\n    legend.background = element_rect(fill = alpha(\"grey97\", 0.5), color = \"grey80\"),\n    axis.title = element_blank(),\n    axis.text = element_blank(),\n    panel.grid = element_blank()\n  ) +\n  guides(\n    size = guide_legend(order = 2),\n    shape = guide_legend(order = 1, override.aes = list(color = c(\"grey50\", \"black\")))\n  )\n\nprint(p2)\n\n# .........................SoCal Dumbbell Plot.........................\n# define text ----\np3_text &lt;- glue::glue(\n  \"   In Southern California, most individual counties will be\\n\ndaily net producers of green H2 in 2030. LA coungh is the only\\n\none that cannot produce enough green H2 to meet its local\\n\ntransportation demand. This means distribution of H2 from\\n\nsurrounding counties may play an important role for them. This\\n\ncould also mean that LA will rely on H2 from other production\\n\npathways that have a higher environmental footprint than\\n\nelectrolytic green H2.\n\"\n)\n\n# define function to rewrite values in shorthand format ----\ncustom_number_format &lt;- function(x) {\n  ifelse(x &gt;= 1e6,\n    gsub(\" \", \"\", sprintf(\"%sM\", format(round(x / 1e6, 1), nsmall = 1, big.mark = \"\"))),\n    gsub(\" \", \"\", sprintf(\"%sk\", format(round(x / 1e3), big.mark = \"\")))\n  )\n}\n\n# tidy data labels and values for dotted line in next plot ----\nsocal_counties &lt;- h2_counties %&gt;%\n  filter(county %in% southern_ca_counties) %&gt;%\n  mutate(\n    difference_abbr = (custom_number_format(difference)),\n    # difference_abbr = ifelse(difference_description == \"more supply\", \"+282k\", difference_abbr),\n    difference_description = case_when(\n      (supply_h2_kg_d - demand_h2_kg_d) &gt; 0 ~ \"more supply\",\n      (supply_h2_kg_d - demand_h2_kg_d) &lt; 0 ~ \"more demand\"\n    ),\n    dotted_line = ifelse(difference_description == \"more supply\", demand_h2_kg_d, supply_h2_kg_d)\n  ) %&gt;%\n  filter(county != \"Santa Barbara\")\n\n# plot ----\np3 &lt;-\n  # plot\n  socal_counties %&gt;%\n  ggplot() +\n  geom_segment(aes(x = 0, xend = dotted_line, y = reorder(county, difference), yend = reorder(county, difference)), alpha = 0.4, linetype = \"dotted\", linewidth = 0.5) +\n  geom_segment(aes(x = demand_h2_kg_d, xend = supply_h2_kg_d, y = reorder(county, difference), yend = reorder(county, difference), linetype = \"Difference\"), alpha = 0.4, linewidth = 1.5, color = \"#A05F54\") +\n  geom_point(aes(x = supply_h2_kg_d, y = reorder(county, difference), shape = \"Production\"), size = 2.5, fill = \"#D19180\", color = \"grey20\") +\n  geom_point(aes(x = demand_h2_kg_d, y = reorder(county, difference), shape = \"Demand\"), size = 2.5, fill = \"#9C6F6F\", color = \"grey20\") +\n\n  # scale\n  scale_shape_manual(values = c(\"Production\" = 24, \"Demand\" = 21), breaks = c(\"Production\", \"Demand\")) +\n  scale_x_continuous(labels = scales::label_number(scale_cut = cut_short_scale()), limits = c(0, 900000)) +\n\n  # add labels\n  labs(y = \"\", x = \"Hydrogen (kg/ day)\", subtitle = \"Difference between Daily Hydrogen Production and \\nDemand in Southern California Counties (2030)\", title = p3_text) +\n  geom_text(\n    aes(\n      x = (demand_h2_kg_d + supply_h2_kg_d) / 2,\n      y = reorder(county, difference),\n      label = difference_abbr,\n      family = \"mont\",\n    ),\n    position = position_nudge(y = 0.4),\n    show.legend = FALSE,\n    size = 3\n  ) +\n\n  # adjust theme\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 9, hjust = 0, family = \"mont\", margin = margin(b = 20)),\n    plot.title.position = \"plot\",\n    plot.subtitle = element_text(size = 10, hjust = 0.5, family = \"mont\", face = \"bold\", margin = margin(b = 10)),\n    legend.title = element_blank(),\n    legend.text = element_text(size = 8, family = \"mont\", face = \"bold\"),\n    legend.direction = \"horizontal\",\n    legend.position = \"top\",\n    legend.justification = c(4, 0),\n    legend.spacing.x = unit(-0, \"lines\"),\n    legend.margin = margin(-5, -5, 0, 0),\n    axis.title = element_text(size = 8, family = \"mont\"),\n    axis.title.x = element_text(family = \"mont\", margin = margin(t = 8)),\n    axis.text = element_text(size = 8, family = \"mont\"),\n    axis.text.y = element_text(size = 8, family = \"mont\"),\n    panel.grid = element_line(color = \"grey97\"),\n    panel.grid.major.y = element_blank(),\n    panel.grid.major.x = element_line(linewidth = 0.3),\n    plot.background = element_rect(colour = \"black\", fill = alpha(\"#9C6F6F\", 0.3), size = 5),\n    plot.margin = margin(t = 20, b = 20, r = 20, l = 20),\n    aspect.ratio = 0.6\n  ) +\n  guides(\n    linetype = guide_legend(order = 1, override.aes = list(x = -5, y = 1)),\n    shape = guide_legend(order = 2, override.aes = list(values = c(), fill = c(\"#D19180\", \"#9C6F6F\"), shape = c(24, 21)))\n  ) +\n\n  # add annotation\n  coord_cartesian(clip = \"off\") +\n  annotate(\"label\", x = 600000, y = 2, label = \"LA is the only county \\nthat cannot produce enough \\ngreen H2 to meet demand\", size = 3, family = \"mont\") +\n  annotate(\n    geom = \"curve\",\n    x = 270000, xend = 85000,\n    y = 1.8, yend = 1.2,\n    curvature = 0.3,\n    arrow = arrow(length = unit(0.3, \"cm\")),\n    alpha = 0.5\n  )\n\np3\n\n# add to blank plot to create white space ----\np3 &lt;- ggplot() +\n  theme_void() +\n  theme(\n    plot.margin = margin(t = 20, r = 5, b = 20, l = 5, \"pt\")\n  ) +\n  inset_element(p3, left = 0.02, bottom = 0.05, right = 0.95, top = 0.98, align_to = \"full\")\n\np3\n\n## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                             Final Infographic                           ----\n## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# load images\nplt1 &lt;- image_read(\"figures/p1.png\") %&gt;%\n  image_ggplot()\n\nplt2 &lt;- image_read(\"figures/p2.png\") %&gt;%\n  image_ggplot()\n\nplt3 &lt;- image_read(\"figures/p3b.png\") %&gt;%\n  image_ggplot()\n\n\n# combine\nplot &lt;- plt1 + plt2 + plt3\n\nplot +\n  plot_annotation(\n    title = \"How much Hydrogen?\",\n    caption = \"\n    Author: Kristin Art\n    Data: Bren H2GO Master's Group Project\n    \"\n  ) &\n  theme(\n    plot.title = element_text(size = 22, hjust = 0.01, family = \"alfa\", colour = \"grey10\"),\n    plot.caption = element_text(size = 8, family = \"mont\")\n  )\n\n# save figure\nggsave(\"figures/infographic_final.png\", bg = \"white\", units = \"in\", width = 12, height = 7)"
  },
  {
    "objectID": "posts/2024-03-22-machine-learning-weather-energy/scripts/index.html#introduction",
    "href": "posts/2024-03-22-machine-learning-weather-energy/scripts/index.html#introduction",
    "title": "Predicting Residential Energy Usage based on Weather",
    "section": "Introduction",
    "text": "Introduction\nIt is important to understand the relationship between weather and energy consumption to inform how we plan and manage energy infrastructure to meet high energy demand events. 1 Understanding this relationship at a fine temporal and spatial scale could allow us to mitigate the extent of weather-related energy strain and impact on people. This is especially relevant in today’s day and age, as climate change alters regional temperatures and increases the frequency of extreme weather events. Improving the efficiency of energy consumption will work in tandem with grid modernization to meet climate and sustainability goals.\nIn this project, I investigate how energy use within residential homes varies based on weather. Past research has shown that there is significant variation in residential energy use within a day. 2 This intuitively makes sense, as most people turn their lights on at night and use more cooking appliances around mealtimes. It’s also well-known that more energy is consumed during the winter months in cold regions due to the generation of heat for warmth. Here, I use high-resolution data from residential apartments to build models that predict hourly energy consumption based on weather patterns.\n\nResearch Question\nWhat weather patterns are good predictors of residential energy usage in Massachussets homes?\n\n\nData\nI used data downloaded from the University of Massachusetts (UMASS) Trace Repository. 3 In particular, I used the Apartment dataset from the 2017 release of the UMASS Smart* Dataset, which is a project that aims to optimize home energy consumption. The Apartment dataset contains minute-level electricity data and local weather conditions from 114 anonymous apartments in Massachussetts collected between 2014-2016.\nThere are 114 individual data files with electricity data corresponding to individual apartments for each year, meaning there are a total of 342 files. There are also 3 data files containing weather data for each year.\n\n\nProject Approach\nBefore I dive right into the project, here is a brief overview of my approach: I began by loading, tidying, and exploring the data. During this phase, I aggregated the per-minute consumption data into hourly averages and randomly sampled a subset of the apartments to reduce the computational power required to perform the analysis. I also removed variables with a large amount of missing observations and performed a casewise deletion for smaller cases of missingness. Then I visually explored the remaining variables to determine whether I needed to make any specific adjustments in my models - I did find that three predictors were highly correlated (&gt; 90%), which informed my decision to reduce them into one principal component before running my models. Next, I split my clean data into training and testing datasets that were stratified on the outcome variable, split the training datasets into 5-folds for cross validation, and specified my model recipe. I then built, tuned, and compared the following model types: Linear Regression, K-Nearest Neighbor, Elastic Net Regression, Random Forest, and Gradient-Boosted Tree. I evaluated the performance of all model iterations based on root mean squared error (RMSE), finalized the workflow for the best model, and fit it to the testing dataset to determine how well it could predict energy consumption based on new data. Now let’s get into the details!"
  },
  {
    "objectID": "posts/2024-03-22-machine-learning-weather-energy/scripts/index.html#load-and-tidy-data",
    "href": "posts/2024-03-22-machine-learning-weather-energy/scripts/index.html#load-and-tidy-data",
    "title": "Predicting Residential Energy Usage based on Weather",
    "section": "Load and Tidy Data",
    "text": "Load and Tidy Data\n\n\nCode\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(janitor)\nlibrary(ggplot2)\nlibrary(kableExtra)\nlibrary(here)\nlibrary(lubridate)\nlibrary(purrr)\nlibrary(naniar)\nlibrary(corrplot)\nlibrary(showtext)\ntidymodels_prefer()\n\n# set seed to reproduce results\nset.seed(2244)\n\n# load fonts\nfont_add_google(name = \"Lato\", family = \"lato\")\nshowtext_auto()\n\n\nSince there were a large amount of individual files that needed to be loaded, aggregated, and cleaned, I began by defining two functions:\n\nThe first function, tidy_weather_data, loads and aggregates data from all files that contain weather information, converts the column headers to lower snake case, and adds columns for time parameters (datetime, date, year, month, and hour) using the tidyverse, janitor, and lubridate packages. I decided to separate out the year, month, and hour columns to be their own predictors because I believe energy usage may vary based on them. I also filtered the dataset to only contain dates from noon on October 14, 2014 onward, as that is the first datetime for which data exists in the apartments dataset.\n\n\n\nCode\n# define list of weather files\nweather_files &lt;- list.files(here::here(\"data/apartment-weather\"), full.names = TRUE) \n\n# define function to process and clean weather data\ntidy_weather_data &lt;- function(file_path) {\n\n  df &lt;- read_csv(file_path) %&gt;% \n    janitor::clean_names() %&gt;% \n    mutate(datetime = lubridate::as_datetime(time, origin = \"1970-01-01\", tz = \"America/New_York\"),  # Convert unix timestamp to Eastern datetime \n           date = lubridate::date(datetime),\n           hour = lubridate::hour(datetime),\n           month = lubridate::month(datetime),\n           year = lubridate::year(datetime)) %&gt;% \n    filter(datetime &gt;= lubridate::ymd_hms(\"2014-10-15 12:00:00\")) # filter data to startdate of apartment data\n  \n  return(df)\n}\n\n# apply function over weather files \nweather_data &lt;- purrr::map_dfr(weather_files, tidy_weather_data)\n\n# inspect result df\nhead(weather_data)\nsummary(weather_data)\n\n\n\nThe second function, tidy_apartment_data, loads and aggregates data from all files that contain electricity data, converts the column headers to lower snake case, and adds columns for time parameters (datetime, date, year, month, and hour) using the tidyverse, janitor, and lubridate packages. I also added a new column to the dataframe containing the unique apartment identification numbers, which were included in the file names. Lastly, I summarized the raw minute-level data into hourly average power use in kiloWatts to reduce the computational power required.\n\n\n\nCode\n# define list of apartment files\napt_files &lt;- list.files(here::here(\"data/apartment\"), pattern = \".csv\", full.names = TRUE, recursive = TRUE) \n\n# define function to process and clean apartment data\ntidy_apartment_data &lt;- function(file_path) {\n  df &lt;- read_csv(file_path, col_names = c(\"datetime\", \"power_kw\"), col_types = cols(datetime = col_datetime(), power_kw = col_double())) %&gt;%\n    janitor::clean_names() %&gt;%\n    mutate(apt_id = as.numeric(stringr::str_extract(basename(file_path), \"Apt(\\\\d+)_\\\\d{4}\") %&gt;% stringr::str_extract(\"(?&lt;=Apt)\\\\d+\")),\n           datetime = lubridate::ymd_hms(datetime),\n           date = lubridate::date(datetime),\n           hour = lubridate::hour(datetime),\n           month = lubridate::month(datetime),\n           year = lubridate::year(datetime)) %&gt;%\n    group_by(date, hour, apt_id) %&gt;%\n    summarize(hourly_average_power_kw = as.numeric(mean(as.numeric(power_kw), na.rm = TRUE))) %&gt;%\n    ungroup()\n  \n  return(df)\n\n}\n\n# apply function over all apartment files \napt_data &lt;- purrr::map_dfr(apt_files, tidy_apartment_data )\n\n# inspect result df\nhead(apt_data)\nsummary(apt_data)\n\n\nAfter loading, aggregating, and cleaning all of the data (good job to my computer), I combined the weather dataset with the apartments dataset by joining them based on the common date and hour columns. I also defined the month, year, and apartment IDs as factors to make them easier to plot later on. Lastly, I randomly sampled 50 out of the 114 apartments in an effort to decrease the computational power and run time required for my machine learning models.\n\n\nCode\n# define random apartment ids to use for models\napt_sample &lt;- sample(1:114, 50, replace = FALSE)\n\n# combine weather and apartment data\nsmart_df &lt;- apt_data %&gt;% \n  full_join(weather_data, by = c(\"date\", \"hour\"), relationship = \"many-to-many\") %&gt;% \n  mutate_at(vars(hour,hourly_average_power_kw, temperature, humidity, visibility, apparent_temperature, pressure, wind_speed, cloud_cover, wind_bearing, precip_intensity, dew_point, precip_probability), as.numeric) %&gt;% \n  mutate_at(vars(month, year, apt_id), as.factor) %&gt;% \n  filter(apt_id %in% apt_sample)\n\n# save combined df\nsave(smart_df, file = \"data/inter_data/smart_df.csv\")"
  },
  {
    "objectID": "posts/2024-03-22-machine-learning-weather-energy/scripts/index.html#exploratory-data-analysis",
    "href": "posts/2024-03-22-machine-learning-weather-energy/scripts/index.html#exploratory-data-analysis",
    "title": "Predicting Residential Energy Usage based on Weather",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nOnce all my data was in one dataframe, my first real step was to figure out how much of it was missing. Here I used the vis_miss() function from the naniar package to visualize any missing values.\n\n\nCode\n# load combined df\nload(here::here(\"posts/2024-03-22-machine-learning-weather-energy/data/inter_data/smart_df.csv\"))\n\n# visualize missing data\nsmart_df %&gt;%\n  naniar::vis_miss(warn_large_data = FALSE) +\n  theme(text = element_text(family = \"lato\"))\n\n\n\n\n\nSurprisingly, the dataset was near-complete! Only 1.2% of it was missing (this might be a new record for me). Nearly all of the missing values were from the cloud_cover column. I wonder why this variable was missing so many observations in an otherwise comprehensive dataset - maybe cloud cover relied on manual human measurement while the other variables were automatically measured by instruments.\nSince the cloud_cover variable itself was missing 13% of observations and was not one of the most impactful predictor variables, I decided to drop the entire variable from the dataset. This way, I avoided losing 13% of the entire dataset like I would if I performed a complete case/ pairwise deletion.\nThe rest of the variables in the dataset were missing between 0-1% of their values. Since this is such a small proportion, I decided to performa complete case/ pairwise deletion across the entire dataset. If the proportion was higher, I would have imputed the missing values.\n\n\nCode\n# remove variables with missing data and non-useful data\nsmart_mod &lt;- smart_df %&gt;% \n  select(-cloud_cover) %&gt;% \n  drop_na()\n\n\nOnce all the missing values were taken care of, I took a peek at the data through descriptive and summary statistics.\nMy final dataframe had 20 variables and 953,578 observations.\n\n\nCode\n# explore data  \nsmart_mod %&gt;% dim()\n\n\n[1] 953578     20\n\n\nThe column names of my dataframe were:\n\n\nCode\nsmart_mod %&gt;% names()\n\n\n [1] \"date\"                    \"hour\"                   \n [3] \"apt_id\"                  \"hourly_average_power_kw\"\n [5] \"temperature\"             \"icon\"                   \n [7] \"humidity\"                \"visibility\"             \n [9] \"summary\"                 \"apparent_temperature\"   \n[11] \"pressure\"                \"wind_speed\"             \n[13] \"time\"                    \"wind_bearing\"           \n[15] \"precip_intensity\"        \"dew_point\"              \n[17] \"precip_probability\"      \"datetime\"               \n[19] \"month\"                   \"year\"                   \n\n\nIt looked like the data type for all the variables are appropriate. Most of the variables were numeric while the summary and icon variables were categorical. The apartment ID, month, and year were all factors because I defined them to be factors above. Lastly, the datetime and date columns are POSIXct and Date objects, respectively.\n\n\nCode\nsmart_mod %&gt;% str()\n\n\ntibble [953,578 × 20] (S3: tbl_df/tbl/data.frame)\n $ date                   : Date[1:953578], format: \"2014-10-15\" \"2014-10-15\" ...\n $ hour                   : num [1:953578] 12 13 14 15 16 17 18 19 20 21 ...\n $ apt_id                 : Factor w/ 114 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ hourly_average_power_kw: num [1:953578] 0 0 0 0 0 0 0 0 0 0 ...\n $ temperature            : num [1:953578] 74.2 75.3 75.9 75.9 74.7 ...\n $ icon                   : chr [1:953578] \"partly-cloudy-day\" \"partly-cloudy-day\" \"cloudy\" \"clear-day\" ...\n $ humidity               : num [1:953578] 0.73 0.71 0.68 0.67 0.7 0.72 0.76 0.79 0.81 0.83 ...\n $ visibility             : num [1:953578] 9.56 9.29 10 10 9.79 10 9.95 9.95 9.91 8.86 ...\n $ summary                : chr [1:953578] \"Partly Cloudy\" \"Partly Cloudy\" \"Overcast\" \"Clear\" ...\n $ apparent_temperature   : num [1:953578] 74.2 75.3 75.9 75.9 74.7 ...\n $ pressure               : num [1:953578] 1019 1018 1017 1017 1016 ...\n $ wind_speed             : num [1:953578] 10.07 10.19 10.45 11.05 9.19 ...\n $ time                   : num [1:953578] 1.41e+09 1.41e+09 1.41e+09 1.41e+09 1.41e+09 ...\n $ wind_bearing           : num [1:953578] 183 178 179 174 178 162 152 154 147 146 ...\n $ precip_intensity       : num [1:953578] 0.0023 0.0011 0.0022 0 0.0011 0 0 0 0 0.001 ...\n $ dew_point              : num [1:953578] 65 65.2 64.8 64.2 64.3 ...\n $ precip_probability     : num [1:953578] 0.06 0.01 0.06 0 0.01 0 0 0 0 0.01 ...\n $ datetime               : POSIXct[1:953578], format: \"2014-10-15 12:00:00\" \"2014-10-15 13:00:00\" ...\n $ month                  : Factor w/ 12 levels \"1\",\"2\",\"3\",\"4\",..: 10 10 10 10 10 10 10 10 10 10 ...\n $ year                   : Factor w/ 3 levels \"2014\",\"2015\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\nThe summary statistics for all the variables are shown in the table below:\n\n\nCode\nsmart_mod %&gt;% summary() %&gt;% \n  kbl(caption = \"Summary Statistics for all Variables in the Smart* Dataset\") %&gt;% \n  kable_styling(full_width = F, font = \"lato\") %&gt;% \n  scroll_box(width = \"100%\", height = \"200px\")\n\n\n\n\nSummary Statistics for all Variables in the Smart* Dataset\n\n\n\ndate\nhour\napt_id\nhourly_average_power_kw\ntemperature\nicon\nhumidity\nvisibility\nsummary\napparent_temperature\npressure\nwind_speed\ntime\nwind_bearing\nprecip_intensity\ndew_point\nprecip_probability\ndatetime\nmonth\nyear\n\n\n\n\n\nMin. :2014-10-15\nMin. : 0.0\n92 : 19379\nMin. : 0.0000\nMin. :-13.05\nLength:953578\nMin. :0.130\nMin. : 0.290\nLength:953578\nMin. :-32.99\nMin. : 986.1\nMin. : 0.020\nMin. :1.413e+09\nMin. : 0.0\nMin. :0.000000\nMin. :-27.69\nMin. :0.00000\nMin. :2014-10-15 08:00:00.00\n11 :108150\n2014: 93127\n\n\n\n1st Qu.:2015-05-01\n1st Qu.: 6.0\n36 : 19068\n1st Qu.: 0.2112\n1st Qu.: 34.49\nClass :character\n1st Qu.:0.530\n1st Qu.: 9.190\nClass :character\n1st Qu.: 29.49\n1st Qu.:1012.0\n1st Qu.: 3.630\n1st Qu.:1.430e+09\n1st Qu.:158.0\n1st Qu.:0.000000\n1st Qu.: 24.25\n1st Qu.:0.00000\n1st Qu.:2015-05-01 02:00:00.00\n10 : 94277\n2015:439100\n\n\n\nMedian :2015-11-15\nMedian :12.0\n38 : 19068\nMedian : 0.9015\nMedian : 48.62\nMode :character\nMedian :0.700\nMedian :10.000\nMode :character\nMedian : 46.43\nMedian :1017.2\nMedian : 5.860\nMedian :1.448e+09\nMedian :220.0\nMedian :0.000000\nMedian : 37.82\nMedian :0.00000\nMedian :2015-11-15 18:00:00.00\n12 : 92501\n2016:421351\n\n\n\nMean :2015-11-14\nMean :11.5\n40 : 19068\nMean : 1.1568\nMean : 48.90\nNA\nMean :0.676\nMean : 9.103\nNA\nMean : 46.20\nMean :1017.2\nMean : 6.539\nMean :1.448e+09\nMean :207.4\nMean :0.003222\nMean : 37.48\nMean :0.06097\nMean :2015-11-15 09:39:56.80\n1 : 76800\nNA\n\n\n\n3rd Qu.:2016-05-31\n3rd Qu.:17.0\n53 : 19068\n3rd Qu.: 1.7686\n3rd Qu.: 64.76\nNA\n3rd Qu.:0.850\n3rd Qu.:10.000\nNA\n3rd Qu.: 64.76\n3rd Qu.:1022.3\n3rd Qu.: 8.650\n3rd Qu.:1.465e+09\n3rd Qu.:294.0\n3rd Qu.:0.000000\n3rd Qu.: 54.31\n3rd Qu.:0.00000\n3rd Qu.:2016-05-31 12:00:00.00\n5 : 74400\nNA\n\n\n\nMax. :2016-12-28\nMax. :23.0\n109 : 19068\nMax. :79.1174\nMax. : 93.78\nNA\nMax. :0.980\nMax. :10.000\nNA\nMax. : 98.45\nMax. :1044.5\nMax. :24.940\nMax. :1.483e+09\nMax. :359.0\nMax. :0.426900\nMax. : 75.29\nMax. :0.90000\nMax. :2016-12-28 22:00:00.00\n7 : 74400\nNA\n\n\n\nNA\nNA\n(Other):838859\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n(Other):433050\nNA\n\n\n\n\n\n\n\nLastly, here’s a look at the first few rows of the data in case you want to get a feel for it:\n\n\nCode\nsmart_mod %&gt;% head() %&gt;% \n  kbl(caption = \"First 6 Rows of the Smart* Dataset\") %&gt;% \n  kable_styling(full_width = F, font = \"lato\") %&gt;% \n  scroll_box(width = \"100%\", height = \"200px\")\n\n\n\n\nFirst 6 Rows of the Smart* Dataset\n\n\ndate\nhour\napt_id\nhourly_average_power_kw\ntemperature\nicon\nhumidity\nvisibility\nsummary\napparent_temperature\npressure\nwind_speed\ntime\nwind_bearing\nprecip_intensity\ndew_point\nprecip_probability\ndatetime\nmonth\nyear\n\n\n\n\n2014-10-15\n12\n1\n0\n74.20\npartly-cloudy-day\n0.73\n9.56\nPartly Cloudy\n74.20\n1018.98\n10.07\n1413388800\n183\n0.0023\n65.00\n0.06\n2014-10-15 12:00:00\n10\n2014\n\n\n2014-10-15\n13\n1\n0\n75.32\npartly-cloudy-day\n0.71\n9.29\nPartly Cloudy\n75.32\n1017.95\n10.19\n1413392400\n178\n0.0011\n65.21\n0.01\n2014-10-15 13:00:00\n10\n2014\n\n\n2014-10-15\n14\n1\n0\n75.91\ncloudy\n0.68\n10.00\nOvercast\n75.91\n1017.11\n10.45\n1413396000\n179\n0.0022\n64.76\n0.06\n2014-10-15 14:00:00\n10\n2014\n\n\n2014-10-15\n15\n1\n0\n75.86\nclear-day\n0.67\n10.00\nClear\n75.86\n1016.71\n11.05\n1413399600\n174\n0.0000\n64.21\n0.00\n2014-10-15 15:00:00\n10\n2014\n\n\n2014-10-15\n16\n1\n0\n74.74\nclear-day\n0.70\n9.79\nClear\n74.74\n1016.49\n9.19\n1413403200\n178\n0.0011\n64.34\n0.01\n2014-10-15 16:00:00\n10\n2014\n\n\n2014-10-15\n17\n1\n0\n74.14\npartly-cloudy-day\n0.72\n10.00\nPartly Cloudy\n74.14\n1016.22\n9.86\n1413406800\n162\n0.0000\n64.48\n0.00\n2014-10-15 17:00:00\n10\n2014"
  },
  {
    "objectID": "posts/2024-03-22-machine-learning-weather-energy/scripts/index.html#visual-exploratory-data-analysis",
    "href": "posts/2024-03-22-machine-learning-weather-energy/scripts/index.html#visual-exploratory-data-analysis",
    "title": "Predicting Residential Energy Usage based on Weather",
    "section": "Visual Exploratory Data Analysis",
    "text": "Visual Exploratory Data Analysis\nNext, I began my favorite type of exploratory analysis - visualization!\n\nCorrelation Plot\nFirst off, I explored how correlated all of the numeric variables were by using the corrplot() function from the corrplot package to visualize a correlation matrix. It showed me that temperature, apparent_temperature, and dew point were highly positively correlated, which makes sense since they are physically related; since they were so highly correlated (&gt; 90%), I decide to use a principal components analysis (PCA) in my model recipe below to collapse them into 1 feature instead of 3. Interestingly, these three variables were negatively correlated with hourly_average_power_kw, which is the outcome variable of interest. Another interesting finding is that visibility was negatively correlated to humidity, precipitation intensity, and precipitation probability - this makes sense since it is hard to see far while it’s raining. The other correlations were also logical since weather variables are typically all related.\n\n\nCode\n# correlation plot of all variables\nsmart_mod %&gt;%\n  select(where(is.numeric)) %&gt;%\n  cor() %&gt;%\n  corrplot(method = 'number', order = 'FPC', type = 'lower', family = \"lato\", number.cex=0.6, bg = \"grey80\") \n\n\n\n\n\n\n\n\n\n\n\nPower Usage Distribution\nNext, I explored the distribution of hourly_average_power_kw, which is the outcome variable of interest. The outcome variable was highly positively skewed, as the vast majority of observations (order of \\(10^5\\) - \\(10^6\\)) for each bin were between 0 - 10 kiloWatts. There were only a handful of observations (order of \\(10^1\\)) for each bin between 15 - 232 kiloWatts.\n\n\nCode\n# histogram of energy usage\nggplot(data = smart_mod, aes(x = hourly_average_power_kw))+\n  geom_histogram(fill = \"#DAA49A\", col = \"#875053\", bins = 150)+\n  labs(x = \"Power Usage (kW)\", y = \"Count\")+\n  theme_minimal()+\n  theme(text = element_text(family = \"lato\"))\n\n\n\n\n\n\n\n\n\n\n\nPower Usage by Month\nI visualized the relationship between power usage and month by making a box-and-whisket plot. As expected, power usage is lowest during the warm months (June - September) and highest during the cold months (November - February); this makes sense since most people crank up the heat in the winter months to stay warm. Interestingly, there were quite a few outliers for all of the months, which could mean some apartments use more energy in general.\n\n\nCode\n# boxplot of energy usage against month\nggplot(data = smart_mod, aes(x = factor(month, labels = month.name), y = hourly_average_power_kw, group = month))+\n  geom_boxplot(fill = \"#DAA49A\", col = \"#875053\")+\n  scale_y_continuous(limits = c(0,12))+\n  #geom_jitter(alpha = 0.4, col = \"#DAA49A\")+\n  labs(x = \"Month\", y = \"Power (kW)\")+\n  theme_minimal()+\n  theme(text = element_text(family = \"lato\"))\n\n\n\n\n\n\n\n\n\n\n\nTemperature Distribution\nNext, I explored the distribution of temperature, which I expect would have a significant impact on energy use. Temprature had a normal distribution with a slight left tail, indicating a small negative skew.\n\n\nCode\n# histogram of temperature\nggplot(data = smart_mod, aes(x = temperature))+\n  # geom_histogram(aes(y = ..density..), bins = 50, fill = \"#DAA49A\", col = \"#875053\")+\n  # geom_density(linewidth = 1.5)+\n  geom_histogram(bins = 50, fill = \"#DAA49A\", col = \"#875053\")+\n  labs(x = \"Temperature (deg F)\", y = \"Count\")+\n  theme_minimal()+\n  theme(text = element_text(family = \"lato\"))"
  },
  {
    "objectID": "posts/2024-03-22-machine-learning-weather-energy/scripts/index.html#model-set-up",
    "href": "posts/2024-03-22-machine-learning-weather-energy/scripts/index.html#model-set-up",
    "title": "Predicting Residential Energy Usage based on Weather",
    "section": "Model Set-Up",
    "text": "Model Set-Up\nNext, I prepared my data for the machine learning models. Here, I randomly split the data into training and testing datasets, split the training dataset into 5 folds for k-fold cross validation, and specified a model recipe.\n\nSplit Training and Testing Data\nFirst off, I split the full dataset into training and testing datasets. Like the names imply, the training dataset will be used to train the models while the testing dataset will be used to test the predictive power of the models at the very end. I split the data using the initial_split() function from the rsample package. The split is stratified on the outcome variable, hourly_average_power_kw, to ensure that both the training and the testing datasets have roughly the same distribution of hourly_average_power_kw. I split the full dataset so that 3/4 of it becomes the training dataset and the remaining 1/4 becomes the testing dataset. This was to ensure there is a good amount of data for training while still retaining enough for substantial testing.\n\n\nCode\n# split data\nsmart_split &lt;- rsample::initial_split(smart_mod, \n                                        prop = 0.75,\n                                        strata = hourly_average_power_kw)\n\n# assign splits to train and test objects\nsmart_train &lt;- rsample::training(smart_split)\nsmart_test &lt;- rsample::testing(smart_split)\n\n\n\n\nK-Fold Cross Validation\nI also performed a k-fold cross validation on my entire training set with k = 5. This splits the entire training set into 5 folds that each consist of a mini-training, or analysis set, and a mini-testing, or assessment set. Each of my models will be trained on the analysis sets and tested on the assessment sets of each fold and the mean performance metric across all folds will be reported. I used the vfold function from rsample to split the training dataset into 5 folds and stratified on the outcome variable once again to ensure that each subset of the data has the same distribution of hourly_average_power_kw.\n\n\nCode\n# split data for k-fold CV\nsmart_folds &lt;- rsample::vfold_cv(smart_train, v = 5, strata = hourly_average_power_kw)\n\n\n\n\nBuild Recipe\nThen I specified a recipe for the models to use. I used the recipes package to do things like define the model recipe, dummy code the categorical variables, center all predictors, scale all predictors, and reduce the dimensions of those highly correlated predictors I noticed during the EDA (temperature, apparent_temperature, and dew_point).\nMy recipe tells my models to predict hourly_average_power_kw as a function of temperature + humidity + visibility + summary + apparent_temperature + pressure + wind_speed + wind_bearing + precip_intensity + dew_point + precip_probability + year + month + hour + time. The results from the pre-processing steps I specified are shown below.\n\n\nCode\n# define recipe\nsmart_recipe &lt;- recipes::recipe(hourly_average_power_kw ~ temperature + humidity + visibility + summary + apparent_temperature + pressure + wind_speed + wind_bearing + precip_intensity + dew_point + precip_probability + year + month + hour + time, data = smart_train) %&gt;% \n  recipes::step_dummy(all_nominal_predictors()) %&gt;% # dummy code categorical variables\n  recipes::step_normalize(all_numeric_predictors(), -all_nominal_predictors()) %&gt;% # center and scale numeric predictors only\n  recipes::step_pca(c(\"temperature\", \"apparent_temperature\", \"dew_point\"), num_comp = 1) # convert highly correlated variables (&gt;90) into 1 principal component\n\n#apply/view recipe\nsmart_recipe %&gt;% \n  recipes::prep() %&gt;% \n  recipes::bake(new_data = smart_train) %&gt;% \n  head() %&gt;% \n  kable() %&gt;% \n  kable_styling(full_width = F, font = \"lato\") %&gt;% \n  scroll_box(width = \"100%\", height = \"200px\")\n\n\n\n\n\n\nhumidity\nvisibility\npressure\nwind_speed\nwind_bearing\nprecip_intensity\nprecip_probability\nhour\ntime\nhourly_average_power_kw\nsummary_Breezy.and.Foggy\nsummary_Breezy.and.Mostly.Cloudy\nsummary_Breezy.and.Overcast\nsummary_Breezy.and.Partly.Cloudy\nsummary_Clear\nsummary_Drizzle\nsummary_Drizzle.and.Breezy\nsummary_Dry\nsummary_Flurries\nsummary_Foggy\nsummary_Heavy.Rain\nsummary_Heavy.Snow\nsummary_Humid.and.Overcast\nsummary_Humid.and.Partly.Cloudy\nsummary_Light.Rain\nsummary_Light.Rain.and.Breezy\nsummary_Light.Snow\nsummary_Mostly.Cloudy\nsummary_Overcast\nsummary_Partly.Cloudy\nsummary_Rain\nsummary_Rain.and.Breezy\nsummary_Snow\nyear_X2015\nyear_X2016\nmonth_X2\nmonth_X3\nmonth_X4\nmonth_X5\nmonth_X6\nmonth_X7\nmonth_X8\nmonth_X9\nmonth_X10\nmonth_X11\nmonth_X12\nPC1\n\n\n\n\n0.2820793\n0.2571194\n0.2295259\n0.9074681\n-0.2347835\n-0.0560576\n-0.0067788\n0.0723027\n-1.727829\n0\n-0.0101725\n-0.007095\n-0.007095\n-0.0378107\n-1.727704\n-0.1460816\n-0.0073847\n-0.0073847\n-0.0608575\n-0.0465443\n-0.029337\n-0.0203483\n-0.0074788\n-0.0068951\n-0.2423889\n-0.0126264\n-0.1116211\n-0.0788855\n-0.1086493\n2.7684555\n-0.1128537\n-0.0123463\n-0.0618335\n-0.9242423\n-0.8889373\n-0.278143\n-0.2909624\n-0.2860174\n-0.2909087\n-0.2860689\n-0.2906768\n-0.2912252\n-0.2853847\n3.018419\n-0.3573269\n-0.3275356\n-2.267436\n\n\n0.1779755\n0.1048700\n0.0987204\n0.9383314\n-0.2826580\n-0.1275158\n-0.2912672\n0.2167757\n-1.727647\n0\n-0.0101725\n-0.007095\n-0.007095\n-0.0378107\n-1.727704\n-0.1460816\n-0.0073847\n-0.0073847\n-0.0608575\n-0.0465443\n-0.029337\n-0.0203483\n-0.0074788\n-0.0068951\n-0.2423889\n-0.0126264\n-0.1116211\n-0.0788855\n-0.1086493\n2.7684555\n-0.1128537\n-0.0123463\n-0.0618335\n-0.9242423\n-0.8889373\n-0.278143\n-0.2909624\n-0.2860174\n-0.2909087\n-0.2860689\n-0.2906768\n-0.2912252\n-0.2853847\n3.018419\n-0.3573269\n-0.3275356\n-2.335619\n\n\n0.0218198\n0.5052295\n-0.0079559\n1.0052019\n-0.2730831\n-0.0620124\n-0.0067788\n0.3612487\n-1.727465\n0\n-0.0101725\n-0.007095\n-0.007095\n-0.0378107\n-1.727704\n-0.1460816\n-0.0073847\n-0.0073847\n-0.0608575\n-0.0465443\n-0.029337\n-0.0203483\n-0.0074788\n-0.0068951\n-0.2423889\n-0.0126264\n-0.1116211\n-0.0788855\n9.2039128\n-0.3612117\n-0.1128537\n-0.0123463\n-0.0618335\n-0.9242423\n-0.8889373\n-0.278143\n-0.2909624\n-0.2860174\n-0.2909087\n-0.2860689\n-0.2906768\n-0.2912252\n-0.2853847\n3.018419\n-0.3573269\n-0.3275356\n-2.355355\n\n\n0.2300274\n0.5052295\n-0.1209819\n0.8534573\n-0.4358565\n-0.1930191\n-0.3481648\n0.7946676\n-1.726919\n0\n-0.0101725\n-0.007095\n-0.007095\n-0.0378107\n-1.727704\n-0.1460816\n-0.0073847\n-0.0073847\n-0.0608575\n-0.0465443\n-0.029337\n-0.0203483\n-0.0074788\n-0.0068951\n-0.2423889\n-0.0126264\n-0.1116211\n-0.0788855\n-0.1086493\n2.7684555\n-0.1128537\n-0.0123463\n-0.0618335\n-0.9242423\n-0.8889373\n-0.278143\n-0.2909624\n-0.2860174\n-0.2909087\n-0.2860689\n-0.2906768\n-0.2912252\n-0.2853847\n3.018419\n-0.3573269\n-0.3275356\n-2.249099\n\n\n0.5943907\n0.4770352\n-0.0981227\n0.1924680\n-0.5124558\n-0.1930191\n-0.3481648\n1.0836135\n-1.726555\n0\n-0.0101725\n-0.007095\n-0.007095\n-0.0378107\n-1.727704\n-0.1460816\n-0.0073847\n-0.0073847\n-0.0608575\n-0.0465443\n-0.029337\n-0.0203483\n-0.0074788\n-0.0068951\n-0.2423889\n-0.0126264\n-0.1116211\n-0.0788855\n-0.1086493\n2.7684555\n-0.1128537\n-0.0123463\n-0.0618335\n-0.9242423\n-0.8889373\n-0.278143\n-0.2909624\n-0.2860174\n-0.2909087\n-0.2860689\n-0.2906768\n-0.2912252\n-0.2853847\n3.018419\n-0.3573269\n-0.3275356\n-2.089715\n\n\n0.6984945\n0.4544797\n-0.1044725\n0.0381514\n-0.5794801\n-0.1930191\n-0.3481648\n1.2280865\n-1.726373\n0\n-0.0101725\n-0.007095\n-0.007095\n-0.0378107\n-1.727704\n-0.1460816\n-0.0073847\n-0.0073847\n-0.0608575\n-0.0465443\n-0.029337\n-0.0203483\n-0.0074788\n-0.0068951\n-0.2423889\n-0.0126264\n-0.1116211\n-0.0788855\n-0.1086493\n2.7684555\n-0.1128537\n-0.0123463\n-0.0618335\n-0.9242423\n-0.8889373\n-0.278143\n-0.2909624\n-0.2860174\n-0.2909087\n-0.2860689\n-0.2906768\n-0.2912252\n-0.2853847\n3.018419\n-0.3573269\n-0.3275356\n-2.058875"
  },
  {
    "objectID": "posts/2024-03-22-machine-learning-weather-energy/scripts/index.html#model-building",
    "href": "posts/2024-03-22-machine-learning-weather-energy/scripts/index.html#model-building",
    "title": "Predicting Residential Energy Usage based on Weather",
    "section": "Model Building",
    "text": "Model Building\nNow it was finally time to build the predictive models! I used 5 different machine learning algorithms to do so: Linear Regression, K-Nearest Neighbor, Elastic Net Regression, Random Forest, and Gradient-Boosted Trees. All of the model types aside from the Linear Regression had specific hyperparameters that I could tune; hyperparameters are parameters external to the actual modeled data that control the learning process and performance of a model.\nThe general steps for building a model using tidymodels are:\n\nSpecify the model type, computational engine, model mode, and hyperparameters to tune (if applicable). Since my problem involves predicting a continuous outcome variable, I always set the mode to regression during this step.\nSet up a workflow by combining the model specifications from step 1 with the model recipe.\nCreate a tuning grid with a range of values for each hyperparameter of the model. Then train and evaluate versions of the model that use different combinations of the hyperparameters using the training data set. Since I have a lot of data and this step can take a long time to run, I saved the model tuning results as .Rds files to avoid re-running the model tuning.\nCompare the performance metric across all model versions and select the best one. Finalize the workflow with the best model and its specific hyperparameters.\nFit the final model to the testing dataset to evaluate its predictive performance on new data.\n\nSteps 1-3 were included in the code chunks below for each model type I explored. Step 4 was included in the next section, Model Results, and step 5 was included in the Best Model Results section below.\n\nLinear Regression\n\n\nCode\n# define model engine and mode\nlm_mod &lt;- parsnip::linear_reg() %&gt;% \n  parsnip::set_engine(\"lm\")\n\n# set up workflow\nlm_wkflow &lt;- workflows::workflow() %&gt;% \n  workflows::add_model(lm_mod) %&gt;% \n  workflows::add_recipe(smart_recipe)\n\n# fit single lm model across all folds of training set\nlm_res &lt;- tune::fit_resamples(lm_wkflow, resamples = smart_folds)\n\n# save en results\nsave(lm_res, file = \"data/inter_data/lm_res.rda\")\n\n\n\n\nK-Nearest Neighbor\n\n\nCode\n# define model engine and mode\nknn_mod &lt;- parsnip::nearest_neighbor(neighbors = tune()) %&gt;% \n  parsnip::set_engine(\"kknn\") %&gt;% \n  parsnip::set_mode(\"regression\")\n\n# set up workflow\nknn_wkflow &lt;- workflow() %&gt;% \n  workflows::add_model(knn_mod) %&gt;% \n  workflows::add_recipe(smart_recipe)\n\n# set up grid to tune neighbors\nknn_grid &lt;- grid_regular(neighbors(range = c(1, 10)), levels = 5)\n\n\n\n\nCode\n# tune neighbors for knn\nknn_res &lt;- tune::tune_grid(knn_wkflow, grid = knn_grid, resamples = smart_folds,\n                  control = control_grid(verbose = TRUE))\n\n# save en results\nsave(knn_res, file = \"data/inter_data/knn_res.rda\")\n\n\n\n\nElastic Net Regression\n\n\nCode\n# set up model\nen_mod &lt;- parsnip::linear_reg(penalty = tune(), mixture = tune()) %&gt;%\n  parsnip::set_engine(\"glmnet\") %&gt;%\n  parsnip::set_mode(\"regression\")\n\n# set up workflow\nen_wkflow &lt;- workflows::workflow() %&gt;%\n  workflows::add_model(en_mod) %&gt;%\n  workflows::add_recipe(smart_recipe)\n\n# create a regular grid for tuning penalty and mixture \nen_grid &lt;- dials::grid_regular(penalty(range = c(0.01,3), trans = identity_trans()), mixture(range = c(0, 1)), levels = 10)\n\n\n\n\nCode\n# tune hyperparameters for en\nen_res &lt;- tune::tune_grid(en_wkflow, grid = en_grid, resamples = smart_folds, \n                          control = control_grid(verbose = TRUE))\n\n# save en results\nsave(en_res, file = \"data/inter_data/en_res.rda\")\n\n\n\n\nRandom Forest\n\n\nCode\n# set up model\nrf_mod &lt;- parsnip::rand_forest(mtry = tune(), \n                               trees = tune(),\n                               min_n = tune()) %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;% \n  set_mode(\"regression\")\n\n# set up workflow\nrf_wkflow &lt;- workflow() %&gt;% \n  add_model(rf_mod) %&gt;% \n  add_recipe(smart_recipe)\n\n# create a regular grid for tuning mtry, trees, and min_n\nrf_grid &lt;- dials::grid_regular(mtry(range = c(1, 15)), # n predictors that will be randomly sampled at each split when creating tree models\n                               trees(range = c(200, 400)), # n of trees contained \n                               min_n(range = c(10, 30)) # min n of data points in a node required for the node to be split further\n                               )\n\n\n\n\nCode\n# tune hyperparameters for rf\nrf_res &lt;- tune::tune_grid(rf_wkflow, resamples = smart_folds, grid = rf_grid,\n                  control = control_grid(verbose = TRUE))\n\n# save rf results\nsave(rf_res, file = \"data/inter_data/rf_res.rda\")\n\n\n\n\nGradient-Boosted Tree\n\n\nCode\n# set up model\nbt_mod &lt;- boost_tree(mtry = tune(),\n                     trees = tune(), \n                     learn_rate = tune()) %&gt;%\n  set_engine(\"xgboost\") %&gt;% \n  set_mode(\"regression\")\n\n# set up workflow\nbt_wkflow &lt;- workflow() %&gt;% \n  add_model(bt_mod) %&gt;% \n  add_recipe(smart_recipe)\n\n# create a regular grid for tuning mtry, trees, and learning rate\nbt_grid &lt;- grid_regular(mtry(range = c(1, 15)), \n                        trees(range = c(200, 400)),\n                        learn_rate(range = c(-10, -1)),\n                        levels = 5)\n\n\n\n\nCode\n# tune hyperparameters for bt\nbt_res &lt;- tune_grid(bt_wkflow, resamples = smart_folds, grid = bt_grid, control = control_grid(verbose = TRUE))\n\n# save rf results\nsave(bt_res, file = \"data/inter_data/bt_res.rda\")"
  },
  {
    "objectID": "posts/2024-03-22-machine-learning-weather-energy/scripts/index.html#model-results",
    "href": "posts/2024-03-22-machine-learning-weather-energy/scripts/index.html#model-results",
    "title": "Predicting Residential Energy Usage based on Weather",
    "section": "Model Results",
    "text": "Model Results\nAfter all of the models were built, I compared their performance by evaluating the root mean squared error (RMSE) values of each. The RMSE (root mean squared error) measures the magnitude of error between predicted and actual values - lower RMSE values therefore reflect better model performance. Some of the autoplots also displayed the \\(R^2\\) values of the models, which explains the variance of the actual observed values where 1 is a perfect fit - higher \\(R^2\\) values therefore reflect better model performance.\nI use the autoplot function to view each model’s RMSE and the show_best function from the tune package to determine the best-performing models from those that I tuned.\n\nLinear Regression\nThere are no hyperparameters to tune in linear regression models, so I only developed one version of this model type. The mean RMSE across all 5 folds of the training data was 0.787 with a standard error of 0.00552.\n\n\nCode\n# load lm results\nload(here::here(\"posts/2024-03-22-machine-learning-weather-energy/data/inter_data/lm_res.rda\"))\n\n# show best model\nlm_res %&gt;% tune::show_best(metric = \"rmse\")\n\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   0.787     5 0.00552 Preprocessor1_Model1\n\n\nCode\n# save best model results\nlm_best &lt;- lm_res %&gt;% tune::show_best(metric = \"rmse\") %&gt;% slice(1)\n\n\n\n\nK-Nearest Neighbor\nIn K-Nearest Neighbor models, we can tune the K hyperparameter, which specified the number of neighbors that should be considered when evaluating an observation’s expected value. I tuned my K-Nearest Model for 5 values of K between 1 to 10.\nBased on the autoplot, RMSE decreases and \\(R^2\\) increases as the value of K increases. The best performing model version had K = 10 with a mean RMSE across all 5 folds of 1.06 and a standard error of 0.00465.\n\n\nCode\n# load knn results\nload(here::here(\"posts/2024-03-22-machine-learning-weather-energy/data/inter_data/knn_res.rda\"))\n\n# plot\nknn_res %&gt;%\n  autoplot() +\n  theme_minimal() +\n  theme(text = element_text(family = \"lato\"))\n\n\n\n\n\n\n\n\n\nCode\n# show best models\nknn_res %&gt;% tune::show_best(metric = \"rmse\")\n\n\n# A tibble: 5 × 7\n  neighbors .metric .estimator  mean     n std_err .config             \n      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1        10 rmse    standard    1.06     5 0.00465 Preprocessor1_Model5\n2         7 rmse    standard    1.10     5 0.00456 Preprocessor1_Model4\n3         5 rmse    standard    1.13     5 0.00439 Preprocessor1_Model3\n4         3 rmse    standard    1.16     5 0.00424 Preprocessor1_Model2\n5         1 rmse    standard    1.19     5 0.00395 Preprocessor1_Model1\n\n\nCode\n# save best model results\nknn_best &lt;- knn_res %&gt;% tune::show_best(metric = \"rmse\") %&gt;% slice(1)\n\n\n\n\nElastic Net Regression\nIn Elastic Net Regression models, we can tune the penalty and mixture hyperparameters, which specify the …, respectively. I tuned my Elastic Net Regression models for 10 levels of penalty between 0.01 to 3 and 10 levels of mixture between 0 to 1. When mixture = 0, the model is actually performing a ridge regression and when mixture = 1, the model is performing a lasso regression.\nBased on the autoplot, RMSE decreases and \\(R^2\\) increases as the values of mixture, or the proportion of lasso penalty, and penalty, or the amount of regularization, both decrease. The best performing model version had mixture = 0.111, which means it is much closer to being a ridge regression. The best model also had penalty = 0.01, a mean RMSE across all folds of 0.788, and a standard error of 0.00552.\n\n\nCode\n# load en results\nload(here::here(\"posts/2024-03-22-machine-learning-weather-energy/data/inter_data/en_res.rda\"))\n\n# plot\nen_res %&gt;%\n  autoplot() +\n  theme_minimal() +\n  theme(text = element_text(family = \"lato\"))\n\n\n\n\n\n\n\n\n\nCode\n# show best models\nen_res %&gt;% tune::show_best(metric = \"rmse\")\n\n\n# A tibble: 5 × 8\n  penalty mixture .metric .estimator  mean     n std_err .config               \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n1    0.01   0.111 rmse    standard   0.788     5 0.00552 Preprocessor1_Model011\n2    0.01   0.222 rmse    standard   0.788     5 0.00552 Preprocessor1_Model021\n3    0.01   0.333 rmse    standard   0.789     5 0.00552 Preprocessor1_Model031\n4    0.01   0.444 rmse    standard   0.789     5 0.00552 Preprocessor1_Model041\n5    0.01   0     rmse    standard   0.789     5 0.00547 Preprocessor1_Model001\n\n\nCode\n# save best model results\nen_best &lt;- en_res %&gt;% tune::show_best(metric = \"rmse\") %&gt;% slice(1)\n\n\n\n\nRandom Forest\nIn random forest models, we can tune the mtry, trees, and min_n hyperparameters. mtry represents the number of parameters that can be randomly chosen from at each split of the tree. When mtry is less than 1, that means the tree will have no parameters to choose from. When mtry = 15, the tree has access to all of the predictors at each split, which is the same as bagging. I cannot use a mtry greater than 15 because my model recipe only includes 15 predictors, so I used 3 values between 1 to 15. trees represents the total number of trees to include in the forest ensemble. I used 3 values of trees between 200 to 400. min_n represents the minimum number of observations that need to be in a node in order for it to be split further. I used min_n values between 10 to 30 during the tuning process.**\nBased on the autoplot the number of trees does not make much of a difference in model performance, as all the colored lines are virtually on top of each other. The minimal node size did not appear to make much of a difference either, although it looks like min_n = 30 had a slightly lower RMSE than the lower values. The models that had access to 8 parameters at every split consistently performed the best. The best model had mtry = 8, trees = 300, and min_n = 30 with a mean RMSE across all folds of 0.760 and a standard error of 0.00563.\n\n\nCode\n# load rf results\nload(here::here(\"posts/2024-03-22-machine-learning-weather-energy/data/inter_data/rf_res.rda\"))\n\n# plot\nrf_res %&gt;% autoplot() + \n  theme_minimal() +\n  theme(text = element_text(family = \"lato\"))\n\n\n\n\n\n\n\n\n\nCode\n# show best rf models\nrf_res %&gt;% tune::show_best(metric = \"rmse\") \n\n\n# A tibble: 5 × 9\n   mtry trees min_n .metric .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1     8   300    30 rmse    standard   0.760     5 0.00563 Preprocessor1_Model23\n2     8   400    20 rmse    standard   0.760     5 0.00562 Preprocessor1_Model17\n3     8   400    30 rmse    standard   0.760     5 0.00562 Preprocessor1_Model26\n4     8   400    10 rmse    standard   0.760     5 0.00562 Preprocessor1_Model08\n5     8   200    30 rmse    standard   0.760     5 0.00562 Preprocessor1_Model20\n\n\nCode\n# save best model results\nrf_best &lt;- rf_res %&gt;% tune::show_best(metric = \"rmse\") %&gt;% slice(1)\n\n\n\n\nGradient-Boosted Tree\nIn Gradient-Boosted Decision Tree models, we can tune the mtry, trees, min_n, and learn_rate hyperparameters. The first three were described above in the Random Forest model results and the last one, learn_rate, represents how fast the boosted tree changes with each iteration. When learn_rate = 0, the tree doesn’t learn at all and at small values of learn_rate, the tree learns very slowly. I tuned my Gradient-Boosted Tree for 5 values of learn_rate between \\(10^{-10}\\) to \\(10^{-1}\\). Since this hyperparameter is the most impactful for gradient-boosted trees, I opted not to tune an alternative hyperparameter, min_n to reduce the computational power required. I did try out 5 values of mtry between 1 to 15 and 5 values of trees between 200 to 400 as well though.\nBased on the autoplot, models with larger learning rates had the best performance. The number of trees did not make as significant a difference and the number of parameters that were available at every split did not make as much of a difference in most models, although models with mtry &lt; 4 had slightly worse performance. The best performing model had mtry = 15, trees = 200, and learn_rate = 0.1 with a mean RMSE across all folds of 0.755 and a standard error of 0.00568.\n\n\nCode\n# load bt results\nload(here::here(\"posts/2024-03-22-machine-learning-weather-energy/data/inter_data/bt_res.rda\"))\n\n# plot\nbt_res %&gt;% autoplot() + \n  theme_minimal() +\n  theme(text = element_text(family = \"lato\"))\n\n\n\n\n\n\n\n\n\nCode\n# show best rf models\nbt_res %&gt;% tune::show_best(metric = \"rmse\") \n\n\n# A tibble: 5 × 9\n   mtry trees learn_rate .metric .estimator  mean     n std_err .config         \n  &lt;int&gt; &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;           \n1    15   200        0.1 rmse    standard   0.755     5 0.00568 Preprocessor1_M…\n2    11   200        0.1 rmse    standard   0.755     5 0.00561 Preprocessor1_M…\n3    15   250        0.1 rmse    standard   0.755     5 0.00570 Preprocessor1_M…\n4    11   250        0.1 rmse    standard   0.755     5 0.00562 Preprocessor1_M…\n5     8   250        0.1 rmse    standard   0.755     5 0.00563 Preprocessor1_M…\n\n\nCode\n# save best model results\nbt_best &lt;- bt_res %&gt;% tune::show_best(metric = \"rmse\") %&gt;% slice(1)"
  },
  {
    "objectID": "posts/2024-03-22-machine-learning-weather-energy/scripts/index.html#best-model-results",
    "href": "posts/2024-03-22-machine-learning-weather-energy/scripts/index.html#best-model-results",
    "title": "Predicting Residential Energy Usage based on Weather",
    "section": "Best Model Results",
    "text": "Best Model Results\nOnce I combined the lowest RMSE from each model type, it was clear that a Gradient-Boosted Tree is the winner! It has a slightly lower RMSE than the runner-ups and predicted hourly_average_power_kw values that deviate from the actual observed values by approximately 0.755 kw on average. As mentioned above, the best performing model had mtry = 15, trees = 200, and learn_rate = 0.1 with a mean RMSE across all folds of 0.755 and a standard error of 0.00568.\n\n\nCode\n# combine best performance results from each model type\nall_rmse &lt;- tibble(Model = c(\"Linear Regression\", \"K-Nearest Neighbor\", \"Elastic Net Regression\", \"Random Forest\", \"Gradient-Boosted Trees\"), RMSE = c(lm_best$mean, knn_best$mean, en_best$mean, rf_best$mean, bt_best$mean)) %&gt;% \n  mutate(RMSE = round(RMSE, 3)) %&gt;% \n  arrange(RMSE)\n\nall_rmse %&gt;% \n  kable() %&gt;% \n  kable_styling(full_width = F, font = \"lato\") \n\n\n\n\n\nModel\nRMSE\n\n\n\n\nGradient-Boosted Trees\n0.755\n\n\nRandom Forest\n0.760\n\n\nLinear Regression\n0.787\n\n\nElastic Net Regression\n0.788\n\n\nK-Nearest Neighbor\n1.060\n\n\n\n\n\n\n\nNext I finalized the workflow using the hyperparameters from the best Gradient-Boosted Tree model and fit it to the entire training dataset.\n\n\nCode\n# save best model \nbest_mod &lt;- bt_res %&gt;% \n  tune::select_best(metric = \"rmse\", mtries, trees, learn_rate)\n\n# finalize workflow with best model\nfinal_wkflow &lt;- tune::finalize_workflow(bt_wkflow, best_mod)\n\n# fit model to training data\nfinal_fit_mod &lt;- parsnip::fit(final_wkflow, smart_train)\n\n\nOne cool thing about tree-based models is we can visualize which predictors were the most significant drivers of the outcome through a variable importance plot (VIP). Based on the VIP for the best model, the PC1 feature was the most important predictor variable by far; remember, the PC1 feature was extracted from the original temperature, apparent_temperature, and dew_point parameters, which were highly correlated (&gt; 90%) to each other. The time parameter was the second most important variable, followed by hour and months. This result makes sense but surprised me! I was expecting more weather-related parameters to show up in the top 10.\n\n\nCode\n# create variable importance plot using training data\nfinal_fit_mod %&gt;% \n  workflowsets::extract_fit_parsnip() %&gt;% \n  vip::vip(aesthetics = list(fill = \"#DAA49A\", color = \"#875053\")) +\n  theme_minimal() +\n  theme(text = element_text(family = \"lato\"))\n\n\n\n\n\n\n\n\n\nNext, it was finally time to introduce the best model to data its never seen before! I ran the model on the testing dataset to see how well it could predict values it was not trained on. The model’s RMSE on the testing dataset was 0.835, which is only slightly worse than the mean RMSE from the training process. This indicates that the training RMSE across 5-folds was a pretty good indicator of the model’s overall performance.\n\n\nCode\n# assess model performance on entire testing set\nfinal_mod_test &lt;- augment(final_fit_mod, smart_test) %&gt;% \n  rmse(truth = hourly_average_power_kw, estimate = .pred) %&gt;% \n  print()\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.835\n\n\nWhen I plotted the predicted values of power usage against the actual observed values, it was clear that the model does not predict high values well at all; in fact, the model did not predict any power usage values higher than 3.75 kW. This is due to that strong positive skew in the outcome variable, which means even our best model was only trained with a handful of observations for the higher power usage values. Only the values that fall directly on the diagonal line in the plot below were accurately predicted by the model.\n\n\nCode\n# plot predicted vs. actual values from testing data\naugment(final_fit_mod, smart_test) %&gt;% \n  ggplot(aes(x = .pred, y = hourly_average_power_kw)) +\n  geom_point(color = \"#DAA49A\", alpha = 0.2) +\n  geom_abline(lty = 2) +\n  coord_obs_pred() +\n  labs(title = \"Predicted vs. Actual Values of Hourly Average Power Usage (kW)\",\n       y = \"Actual Hourly Average Power Usage (kW)\",\n       y = \"Predicted Hourly Average Power Usage (kW)\") +\n  theme_minimal()+\n  theme(text = element_text(family = \"lato\"),\n        plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Update this with fake or untested data for blog post when have mroe time. \n# For fun, I used the model to predict residential power usage during the warmest and coolest observations in smart df. \n# filter to observations with highest and lowest temperature\nextreme_temps &lt;- smart_mod %&gt;%\n  filter(row_number() == which.max(temperature) | row_number() == which.min(temperature))\n\nfinal_predictions &lt;- augment(final_fit_mod, extreme_temps) %&gt;% \n  select(.pred, hourly_average_power_kw, temperature, everything())\n\nfinal_predictions"
  },
  {
    "objectID": "posts/2024-03-22-machine-learning-weather-energy/scripts/index.html#conclusion",
    "href": "posts/2024-03-22-machine-learning-weather-energy/scripts/index.html#conclusion",
    "title": "Predicting Residential Energy Usage based on Weather",
    "section": "Conclusion",
    "text": "Conclusion\nThe best model for predicting residential energy usage based on weather and time was a Gradient-Boosted Tree model. A Random Forest model did nearly as well, with only a small difference in RMSE separating the two. This is no surprise since decision tree-based models tend to work really well on many types of data. This is because tree-based models are highly flexible and non-parametric, meaning they do not assume any parametric constraints on the outcome variable. Interestingly, the Elastic Net Regression and Linear Regression models had a pretty similar performance to these top tree-based models, indicating that they also do a decent job of predicting residential energy usage; these model types could be more useful to a user who is willing to accept a little higher error in order to use models that are much less computationally expensive. The K-Nearest Neighbor model had the worst predictive power, which makes sense since KNN models tend to do poorly on data with too many predictors, or high dimensions.\nOverall, the best model did an okay job at predicting residential energy usage. The actual values of the outcome variable, hourly_average_power_kw, ranged between 0 to 11.5 kW. On average, the best model’s predicted values were about 0.835 kW off from the actual observed values based on the testing RMSE; this means the model’s average error was about 7% of the entire range. The model could be improved by adding in more data values that have high energy usage values. By normalizing the distribution of the outcome variable in this way, I could improve the model’s learning and potentially improve its performance.\nIt was interesting to see that the principal component of temperature, apparent_temperature, and dew_point is the most important predictor of residential energy usage. Since apparent temperature itself is also a factor of relative humidity and wind speed, this principal component feature may represent all the key weather predictors due to their interrelatedness. Therefore, it makes sense that time variables (hour, time, month) would be the next most important predictors in the VIP rather than some of the general weather predictors that remained (descriptive weather summary, wind-bearing, etc.). This makes intuitive sense and it was fun to see it explained quantitatively. I learned a lot through this project and am looking forward to exploring more energy data with machine learning algorithms!"
  },
  {
    "objectID": "posts/2024-03-22-machine-learning-weather-energy/scripts/index.html#references",
    "href": "posts/2024-03-22-machine-learning-weather-energy/scripts/index.html#references",
    "title": "Predicting Residential Energy Usage based on Weather",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "posts/2024-03-22-machine-learning-weather-energy/scripts/index.html#footnotes",
    "href": "posts/2024-03-22-machine-learning-weather-energy/scripts/index.html#footnotes",
    "title": "Predicting Residential Energy Usage based on Weather",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThorve et al., 2023. https://www.nature.com/articles/s41597-022-01914-1#Tab1↩︎\nFikru and Gautier, 2015. https://www.sciencedirect.com/science/article/pii/S030626191500046X#ab005↩︎\nBarker, S. UMass Smart* Dataset - 2017 release. UMassTraceRepository https://traces.cs.umass.edu/index.php/smart/smart (2017).↩︎"
  },
  {
    "objectID": "posts/2024-03-22-machine-learning-weather-energy/index.html#introduction",
    "href": "posts/2024-03-22-machine-learning-weather-energy/index.html#introduction",
    "title": "Predicting Residential Energy Usage based on Weather",
    "section": "Introduction",
    "text": "Introduction\nIt is important to understand the relationship between weather and energy consumption to inform how we plan and manage energy infrastructure to meet high energy demand events. 1 Understanding this relationship at a fine temporal and spatial scale could allow us to mitigate the extent of weather-related energy strain and impact on people. This is especially relevant in today’s day and age, as climate change alters regional temperatures and increases the frequency of extreme weather events. Improving the efficiency of energy consumption will work in tandem with grid modernization to meet climate and sustainability goals.\nIn this project, I investigate how energy use within residential homes varies based on weather. Past research has shown that there is significant variation in residential energy use within a day. 2 This intuitively makes sense, as most people turn their lights on at night and use more cooking appliances around mealtimes. It’s also well-known that more energy is consumed during the winter months in cold regions due to the generation of heat for warmth. Here, I use high-resolution data from residential apartments to build models that predict hourly energy consumption based on weather patterns.\n\nResearch Question\nWhat weather patterns are good predictors of residential energy usage in Massachussets homes?\n\n\nData\nI used data downloaded from the University of Massachusetts (UMASS) Trace Repository. 3 In particular, I used the Apartment dataset from the 2017 release of the UMASS Smart* Dataset, which is a project that aims to optimize home energy consumption. The Apartment dataset contains minute-level electricity data and local weather conditions from 114 anonymous apartments in Massachussetts collected between 2014-2016.\nThere are 114 individual data files with electricity data corresponding to individual apartments for each year, meaning there are a total of 342 files. There are also 3 data files containing weather data for each year.\n\n\nProject Approach\nBefore I dive right into the project, here is a brief overview of my approach: I began by loading, tidying, and exploring the data. During this phase, I aggregated the per-minute consumption data into hourly averages and randomly sampled a subset of the apartments to reduce the computational power required to perform the analysis. I also removed variables with a large amount of missing observations and performed a casewise deletion for smaller cases of missingness. Then I visually explored the remaining variables to determine whether I needed to make any specific adjustments in my models - I did find that three predictors were highly correlated (&gt; 90%), which informed my decision to reduce them into one principal component before running my models. Next, I split my clean data into training and testing datasets that were stratified on the outcome variable, split the training datasets into 5-folds for cross validation, and specified my model recipe. I then built, tuned, and compared the following model types: Linear Regression, K-Nearest Neighbor, Elastic Net Regression, Random Forest, and Gradient-Boosted Tree. I evaluated the performance of all model iterations based on root mean squared error (RMSE), finalized the workflow for the best model, and fit it to the testing dataset to determine how well it could predict energy consumption based on new data. Now let’s get into the details!\n\nEDAModel Set-UpModel BuildingModel ResultsBest Model Results\n\n\n\nLoad and Tidy Data\n\n\nCode\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(janitor)\nlibrary(ggplot2)\nlibrary(kableExtra)\nlibrary(here)\nlibrary(lubridate)\nlibrary(purrr)\nlibrary(naniar)\nlibrary(corrplot)\nlibrary(showtext)\nlibrary(googledrive)\ntidymodels_prefer()\n\n# set seed to reproduce results\nset.seed(2244)\n\n# load fonts\nfont_add_google(name = \"Lato\", family = \"lato\")\nshowtext_auto()\n\n\nSince there were a large amount of individual files that needed to be loaded, aggregated, and cleaned, I began by defining two functions:\n\nThe first function, tidy_weather_data, loads and aggregates data from all files that contain weather information, converts the column headers to lower snake case, and adds columns for time parameters (datetime, date, year, month, and hour) using the tidyverse, janitor, and lubridate packages. I decided to separate out the year, month, and hour columns to be their own predictors because I believe energy usage may vary based on them. I also filtered the dataset to only contain dates from noon on October 14, 2014 onward, as that is the first datetime for which data exists in the apartments dataset.\n\n\n\nCode\n# define list of weather files\nweather_files &lt;- list.files(here::here(\"data/apartment-weather\"), full.names = TRUE) \n\n# define function to process and clean weather data\ntidy_weather_data &lt;- function(file_path) {\n\n  df &lt;- read_csv(file_path) %&gt;% \n    janitor::clean_names() %&gt;% \n    mutate(datetime = lubridate::as_datetime(time, origin = \"1970-01-01\", tz = \"America/New_York\"),  # Convert unix timestamp to Eastern datetime \n           date = lubridate::date(datetime),\n           hour = lubridate::hour(datetime),\n           month = lubridate::month(datetime),\n           year = lubridate::year(datetime)) %&gt;% \n    filter(datetime &gt;= lubridate::ymd_hms(\"2014-10-15 12:00:00\")) # filter data to startdate of apartment data\n  \n  return(df)\n}\n\n# apply function over weather files \nweather_data &lt;- purrr::map_dfr(weather_files, tidy_weather_data)\n\n# inspect result df\nhead(weather_data)\nsummary(weather_data)\n\n\n\nThe second function, tidy_apartment_data, loads and aggregates data from all files that contain electricity data, converts the column headers to lower snake case, and adds columns for time parameters (datetime, date, year, month, and hour) using the tidyverse, janitor, and lubridate packages. I also added a new column to the dataframe containing the unique apartment identification numbers, which were included in the file names. Lastly, I summarized the raw minute-level data into hourly average power use in kiloWatts to reduce the computational power required.\n\n\n\nCode\n# define list of apartment files\napt_files &lt;- list.files(here::here(\"data/apartment\"), pattern = \".csv\", full.names = TRUE, recursive = TRUE) \n\n# define function to process and clean apartment data\ntidy_apartment_data &lt;- function(file_path) {\n  df &lt;- read_csv(file_path, col_names = c(\"datetime\", \"power_kw\"), col_types = cols(datetime = col_datetime(), power_kw = col_double())) %&gt;%\n    janitor::clean_names() %&gt;%\n    mutate(apt_id = as.numeric(stringr::str_extract(basename(file_path), \"Apt(\\\\d+)_\\\\d{4}\") %&gt;% stringr::str_extract(\"(?&lt;=Apt)\\\\d+\")),\n           datetime = lubridate::ymd_hms(datetime),\n           date = lubridate::date(datetime),\n           hour = lubridate::hour(datetime),\n           month = lubridate::month(datetime),\n           year = lubridate::year(datetime)) %&gt;%\n    group_by(date, hour, apt_id) %&gt;%\n    summarize(hourly_average_power_kw = as.numeric(mean(as.numeric(power_kw), na.rm = TRUE))) %&gt;%\n    ungroup()\n  \n  return(df)\n\n}\n\n# apply function over all apartment files \napt_data &lt;- purrr::map_dfr(apt_files, tidy_apartment_data )\n\n# inspect result df\nhead(apt_data)\nsummary(apt_data)\n\n\nAfter loading, aggregating, and cleaning all of the data (good job to my computer), I combined the weather dataset with the apartments dataset by joining them based on the common date and hour columns. I also defined the month, year, and apartment IDs as factors to make them easier to plot later on. Lastly, I randomly sampled 50 out of the 114 apartments in an effort to decrease the computational power and run time required for my machine learning models.\n\n\nCode\n# define random apartment ids to use for models\napt_sample &lt;- sample(1:114, 50, replace = FALSE)\n\n# combine weather and apartment data\nsmart_df &lt;- apt_data %&gt;% \n  full_join(weather_data, by = c(\"date\", \"hour\"), relationship = \"many-to-many\") %&gt;% \n  mutate_at(vars(hour,hourly_average_power_kw, temperature, humidity, visibility, apparent_temperature, pressure, wind_speed, cloud_cover, wind_bearing, precip_intensity, dew_point, precip_probability), as.numeric) %&gt;% \n  mutate_at(vars(month, year, apt_id), as.factor) %&gt;% \n  filter(apt_id %in% apt_sample)\n\n# save combined df\nsave(smart_df, file = \"data/inter_data/smart_df.csv\")\n\n\n\n\nExploratory Data Analysis\nOnce all my data was in one dataframe, my first real step was to figure out how much of it was missing. Here I used the vis_miss() function from the naniar package to visualize any missing values.\n\n\nCode\n# load combined df\nload(here::here(\"posts/2024-03-22-machine-learning-weather-energy/data/inter_data/smart_df.csv\"))\n\n# visualize missing data\nsmart_df %&gt;%\n  naniar::vis_miss(warn_large_data = FALSE) +\n  theme(text = element_text(family = \"lato\"))\n\n\n\n\n\n\n\n\n\nSurprisingly, the dataset was near-complete! Only 1.2% of it was missing (this might be a new record for me). Nearly all of the missing values were from the cloud_cover column. I wonder why this variable was missing so many observations in an otherwise comprehensive dataset - maybe cloud cover relied on manual human measurement while the other variables were automatically measured by instruments.\nSince the cloud_cover variable itself was missing 13% of observations and was not one of the most impactful predictor variables, I decided to drop the entire variable from the dataset. This way, I avoided losing 13% of the entire dataset like I would if I performed a complete case/ pairwise deletion.\nThe rest of the variables in the dataset were missing between 0-1% of their values. Since this is such a small proportion, I decided to performa complete case/ pairwise deletion across the entire dataset. If the proportion was higher, I would have imputed the missing values.\n\n\nCode\n# remove variables with missing data and non-useful data\nsmart_mod &lt;- smart_df %&gt;% \n  select(-cloud_cover) %&gt;% \n  drop_na()\n\n\nOnce all the missing values were taken care of, I took a peek at the data through descriptive and summary statistics.\nMy final dataframe had 20 variables and 953,578 observations.\n\n\nCode\n# explore data  \nsmart_mod %&gt;% dim()\n\n\n[1] 953578     20\n\n\nThe column names of my dataframe were:\n\n\nCode\nsmart_mod %&gt;% names()\n\n\n [1] \"date\"                    \"hour\"                   \n [3] \"apt_id\"                  \"hourly_average_power_kw\"\n [5] \"temperature\"             \"icon\"                   \n [7] \"humidity\"                \"visibility\"             \n [9] \"summary\"                 \"apparent_temperature\"   \n[11] \"pressure\"                \"wind_speed\"             \n[13] \"time\"                    \"wind_bearing\"           \n[15] \"precip_intensity\"        \"dew_point\"              \n[17] \"precip_probability\"      \"datetime\"               \n[19] \"month\"                   \"year\"                   \n\n\nIt looked like the data type for all the variables are appropriate. Most of the variables were numeric while the summary and icon variables were categorical. The apartment ID, month, and year were all factors because I defined them to be factors above. Lastly, the datetime and date columns are POSIXct and Date objects, respectively.\n\n\nCode\nsmart_mod %&gt;% str()\n\n\ntibble [953,578 × 20] (S3: tbl_df/tbl/data.frame)\n $ date                   : Date[1:953578], format: \"2014-10-15\" \"2014-10-15\" ...\n $ hour                   : num [1:953578] 12 13 14 15 16 17 18 19 20 21 ...\n $ apt_id                 : Factor w/ 114 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ hourly_average_power_kw: num [1:953578] 0 0 0 0 0 0 0 0 0 0 ...\n $ temperature            : num [1:953578] 74.2 75.3 75.9 75.9 74.7 ...\n $ icon                   : chr [1:953578] \"partly-cloudy-day\" \"partly-cloudy-day\" \"cloudy\" \"clear-day\" ...\n $ humidity               : num [1:953578] 0.73 0.71 0.68 0.67 0.7 0.72 0.76 0.79 0.81 0.83 ...\n $ visibility             : num [1:953578] 9.56 9.29 10 10 9.79 10 9.95 9.95 9.91 8.86 ...\n $ summary                : chr [1:953578] \"Partly Cloudy\" \"Partly Cloudy\" \"Overcast\" \"Clear\" ...\n $ apparent_temperature   : num [1:953578] 74.2 75.3 75.9 75.9 74.7 ...\n $ pressure               : num [1:953578] 1019 1018 1017 1017 1016 ...\n $ wind_speed             : num [1:953578] 10.07 10.19 10.45 11.05 9.19 ...\n $ time                   : num [1:953578] 1.41e+09 1.41e+09 1.41e+09 1.41e+09 1.41e+09 ...\n $ wind_bearing           : num [1:953578] 183 178 179 174 178 162 152 154 147 146 ...\n $ precip_intensity       : num [1:953578] 0.0023 0.0011 0.0022 0 0.0011 0 0 0 0 0.001 ...\n $ dew_point              : num [1:953578] 65 65.2 64.8 64.2 64.3 ...\n $ precip_probability     : num [1:953578] 0.06 0.01 0.06 0 0.01 0 0 0 0 0.01 ...\n $ datetime               : POSIXct[1:953578], format: \"2014-10-15 12:00:00\" \"2014-10-15 13:00:00\" ...\n $ month                  : Factor w/ 12 levels \"1\",\"2\",\"3\",\"4\",..: 10 10 10 10 10 10 10 10 10 10 ...\n $ year                   : Factor w/ 3 levels \"2014\",\"2015\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\nThe summary statistics for all the variables are shown in the table below:\n\n\nCode\nsmart_mod %&gt;% summary() %&gt;% \n  kbl(caption = \"Summary Statistics for all Variables in the Smart* Dataset\") %&gt;% \n  kable_styling(full_width = F, font = \"lato\") %&gt;% \n  scroll_box(width = \"100%\", height = \"200px\")\n\n\n\n\nSummary Statistics for all Variables in the Smart* Dataset\n\n\n\ndate\nhour\napt_id\nhourly_average_power_kw\ntemperature\nicon\nhumidity\nvisibility\nsummary\napparent_temperature\npressure\nwind_speed\ntime\nwind_bearing\nprecip_intensity\ndew_point\nprecip_probability\ndatetime\nmonth\nyear\n\n\n\n\n\nMin. :2014-10-15\nMin. : 0.0\n92 : 19379\nMin. : 0.0000\nMin. :-13.05\nLength:953578\nMin. :0.130\nMin. : 0.290\nLength:953578\nMin. :-32.99\nMin. : 986.1\nMin. : 0.020\nMin. :1.413e+09\nMin. : 0.0\nMin. :0.000000\nMin. :-27.69\nMin. :0.00000\nMin. :2014-10-15 08:00:00.00\n11 :108150\n2014: 93127\n\n\n\n1st Qu.:2015-05-01\n1st Qu.: 6.0\n36 : 19068\n1st Qu.: 0.2112\n1st Qu.: 34.49\nClass :character\n1st Qu.:0.530\n1st Qu.: 9.190\nClass :character\n1st Qu.: 29.49\n1st Qu.:1012.0\n1st Qu.: 3.630\n1st Qu.:1.430e+09\n1st Qu.:158.0\n1st Qu.:0.000000\n1st Qu.: 24.25\n1st Qu.:0.00000\n1st Qu.:2015-05-01 02:00:00.00\n10 : 94277\n2015:439100\n\n\n\nMedian :2015-11-15\nMedian :12.0\n38 : 19068\nMedian : 0.9015\nMedian : 48.62\nMode :character\nMedian :0.700\nMedian :10.000\nMode :character\nMedian : 46.43\nMedian :1017.2\nMedian : 5.860\nMedian :1.448e+09\nMedian :220.0\nMedian :0.000000\nMedian : 37.82\nMedian :0.00000\nMedian :2015-11-15 18:00:00.00\n12 : 92501\n2016:421351\n\n\n\nMean :2015-11-14\nMean :11.5\n40 : 19068\nMean : 1.1568\nMean : 48.90\nNA\nMean :0.676\nMean : 9.103\nNA\nMean : 46.20\nMean :1017.2\nMean : 6.539\nMean :1.448e+09\nMean :207.4\nMean :0.003222\nMean : 37.48\nMean :0.06097\nMean :2015-11-15 09:39:56.80\n1 : 76800\nNA\n\n\n\n3rd Qu.:2016-05-31\n3rd Qu.:17.0\n53 : 19068\n3rd Qu.: 1.7686\n3rd Qu.: 64.76\nNA\n3rd Qu.:0.850\n3rd Qu.:10.000\nNA\n3rd Qu.: 64.76\n3rd Qu.:1022.3\n3rd Qu.: 8.650\n3rd Qu.:1.465e+09\n3rd Qu.:294.0\n3rd Qu.:0.000000\n3rd Qu.: 54.31\n3rd Qu.:0.00000\n3rd Qu.:2016-05-31 12:00:00.00\n5 : 74400\nNA\n\n\n\nMax. :2016-12-28\nMax. :23.0\n109 : 19068\nMax. :79.1174\nMax. : 93.78\nNA\nMax. :0.980\nMax. :10.000\nNA\nMax. : 98.45\nMax. :1044.5\nMax. :24.940\nMax. :1.483e+09\nMax. :359.0\nMax. :0.426900\nMax. : 75.29\nMax. :0.90000\nMax. :2016-12-28 22:00:00.00\n7 : 74400\nNA\n\n\n\nNA\nNA\n(Other):838859\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n(Other):433050\nNA\n\n\n\n\n\n\n\nLastly, here’s a look at the first few rows of the data in case you want to get a feel for it:\n\n\nCode\nsmart_mod %&gt;% head() %&gt;% \n  kbl(caption = \"First 6 Rows of the Smart* Dataset\") %&gt;% \n  kable_styling(full_width = F, font = \"lato\") %&gt;% \n  scroll_box(width = \"100%\", height = \"200px\")\n\n\n\n\nFirst 6 Rows of the Smart* Dataset\n\n\ndate\nhour\napt_id\nhourly_average_power_kw\ntemperature\nicon\nhumidity\nvisibility\nsummary\napparent_temperature\npressure\nwind_speed\ntime\nwind_bearing\nprecip_intensity\ndew_point\nprecip_probability\ndatetime\nmonth\nyear\n\n\n\n\n2014-10-15\n12\n1\n0\n74.20\npartly-cloudy-day\n0.73\n9.56\nPartly Cloudy\n74.20\n1018.98\n10.07\n1413388800\n183\n0.0023\n65.00\n0.06\n2014-10-15 12:00:00\n10\n2014\n\n\n2014-10-15\n13\n1\n0\n75.32\npartly-cloudy-day\n0.71\n9.29\nPartly Cloudy\n75.32\n1017.95\n10.19\n1413392400\n178\n0.0011\n65.21\n0.01\n2014-10-15 13:00:00\n10\n2014\n\n\n2014-10-15\n14\n1\n0\n75.91\ncloudy\n0.68\n10.00\nOvercast\n75.91\n1017.11\n10.45\n1413396000\n179\n0.0022\n64.76\n0.06\n2014-10-15 14:00:00\n10\n2014\n\n\n2014-10-15\n15\n1\n0\n75.86\nclear-day\n0.67\n10.00\nClear\n75.86\n1016.71\n11.05\n1413399600\n174\n0.0000\n64.21\n0.00\n2014-10-15 15:00:00\n10\n2014\n\n\n2014-10-15\n16\n1\n0\n74.74\nclear-day\n0.70\n9.79\nClear\n74.74\n1016.49\n9.19\n1413403200\n178\n0.0011\n64.34\n0.01\n2014-10-15 16:00:00\n10\n2014\n\n\n2014-10-15\n17\n1\n0\n74.14\npartly-cloudy-day\n0.72\n10.00\nPartly Cloudy\n74.14\n1016.22\n9.86\n1413406800\n162\n0.0000\n64.48\n0.00\n2014-10-15 17:00:00\n10\n2014\n\n\n\n\n\n\n\n\n\nVisual Exploratory Data Analysis\nNext, I began my favorite type of exploratory analysis - visualization!\n\nCorrelation Plot\nFirst off, I explored how correlated all of the numeric variables were by using the corrplot() function from the corrplot package to visualize a correlation matrix. It showed me that temperature, apparent_temperature, and dew point were highly positively correlated, which makes sense since they are physically related; since they were so highly correlated (&gt; 90%), I decide to use a principal components analysis (PCA) in my model recipe below to collapse them into 1 feature instead of 3. Interestingly, these three variables were negatively correlated with hourly_average_power_kw, which is the outcome variable of interest. Another interesting finding is that visibility was negatively correlated to humidity, precipitation intensity, and precipitation probability - this makes sense since it is hard to see far while it’s raining. The other correlations were also logical since weather variables are typically all related.\n\n\nCode\n# correlation plot of all variables\nsmart_mod %&gt;%\n  select(where(is.numeric)) %&gt;%\n  cor() %&gt;%\n  corrplot(method = 'number', order = 'FPC', type = 'lower', family = \"lato\", number.cex=0.6, bg = \"grey80\") \n\n\n\n\n\n\n\n\n\n\n\nPower Usage Distribution\nNext, I explored the distribution of hourly_average_power_kw, which is the outcome variable of interest. The outcome variable was highly positively skewed, as the vast majority of observations (order of \\(10^5\\) - \\(10^6\\)) for each bin were between 0 - 10 kiloWatts. There were only a handful of observations (order of \\(10^1\\)) for each bin between 15 - 232 kiloWatts.\n\n\nCode\n# histogram of energy usage\nggplot(data = smart_mod, aes(x = hourly_average_power_kw))+\n  geom_histogram(fill = \"#DAA49A\", col = \"#875053\", bins = 150)+\n  labs(x = \"Power Usage (kW)\", y = \"Count\")+\n  theme_minimal()+\n  theme(text = element_text(family = \"lato\"))\n\n\n\n\n\n\n\n\n\n\n\nPower Usage by Month\nI visualized the relationship between power usage and month by making a box-and-whisket plot. As expected, power usage is lowest during the warm months (June - September) and highest during the cold months (November - February); this makes sense since most people crank up the heat in the winter months to stay warm. Interestingly, there were quite a few outliers for all of the months, which could mean some apartments use more energy in general.\n\n\nCode\n# boxplot of energy usage against month\nggplot(data = smart_mod, aes(x = factor(month, labels = month.name), y = hourly_average_power_kw, group = month))+\n  geom_boxplot(fill = \"#DAA49A\", col = \"#875053\")+\n  scale_y_continuous(limits = c(0,12))+\n  #geom_jitter(alpha = 0.4, col = \"#DAA49A\")+\n  labs(x = \"Month\", y = \"Power (kW)\")+\n  theme_minimal()+\n  theme(text = element_text(family = \"lato\"))\n\n\n\n\n\n\n\n\n\n\n\nTemperature Distribution\nNext, I explored the distribution of temperature, which I expect would have a significant impact on energy use. Temprature had a normal distribution with a slight left tail, indicating a small negative skew.\n\n\nCode\n# histogram of temperature\nggplot(data = smart_mod, aes(x = temperature))+\n  # geom_histogram(aes(y = ..density..), bins = 50, fill = \"#DAA49A\", col = \"#875053\")+\n  # geom_density(linewidth = 1.5)+\n  geom_histogram(bins = 50, fill = \"#DAA49A\", col = \"#875053\")+\n  labs(x = \"Temperature (deg F)\", y = \"Count\")+\n  theme_minimal()+\n  theme(text = element_text(family = \"lato\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\nNext, I prepared my data for the machine learning models. Here, I randomly split the data into training and testing datasets, split the training dataset into 5 folds for k-fold cross validation, and specified a model recipe.\n\nSplit Training and Testing Data\nFirst off, I split the full dataset into training and testing datasets. Like the names imply, the training dataset will be used to train the models while the testing dataset will be used to test the predictive power of the models at the very end. I split the data using the initial_split() function from the rsample package. The split is stratified on the outcome variable, hourly_average_power_kw, to ensure that both the training and the testing datasets have roughly the same distribution of hourly_average_power_kw. I split the full dataset so that 3/4 of it becomes the training dataset and the remaining 1/4 becomes the testing dataset. This was to ensure there is a good amount of data for training while still retaining enough for substantial testing.\n\n\nCode\n# split data\nsmart_split &lt;- rsample::initial_split(smart_mod, \n                                        prop = 0.75,\n                                        strata = hourly_average_power_kw)\n\n# assign splits to train and test objects\nsmart_train &lt;- rsample::training(smart_split)\nsmart_test &lt;- rsample::testing(smart_split)\n\n\n\n\nK-Fold Cross Validation\nI also performed a k-fold cross validation on my entire training set with k = 5. This splits the entire training set into 5 folds that each consist of a mini-training, or analysis set, and a mini-testing, or assessment set. Each of my models will be trained on the analysis sets and tested on the assessment sets of each fold and the mean performance metric across all folds will be reported. I used the vfold function from rsample to split the training dataset into 5 folds and stratified on the outcome variable once again to ensure that each subset of the data has the same distribution of hourly_average_power_kw.\n\n\nCode\n# split data for k-fold CV\nsmart_folds &lt;- rsample::vfold_cv(smart_train, v = 5, strata = hourly_average_power_kw)\n\n\n\n\nBuild Recipe\nThen I specified a recipe for the models to use. I used the recipes package to do things like define the model recipe, dummy code the categorical variables, center all predictors, scale all predictors, and reduce the dimensions of those highly correlated predictors I noticed during the EDA (temperature, apparent_temperature, and dew_point).\nMy recipe tells my models to predict hourly_average_power_kw as a function of temperature + humidity + visibility + summary + apparent_temperature + pressure + wind_speed + wind_bearing + precip_intensity + dew_point + precip_probability + year + month + hour + time. The results from the pre-processing steps I specified are shown below.\n\n\nCode\n# define recipe\nsmart_recipe &lt;- recipes::recipe(hourly_average_power_kw ~ temperature + humidity + visibility + summary + apparent_temperature + pressure + wind_speed + wind_bearing + precip_intensity + dew_point + precip_probability + year + month + hour + time, data = smart_train) %&gt;% \n  recipes::step_dummy(all_nominal_predictors()) %&gt;% # dummy code categorical variables\n  recipes::step_normalize(all_numeric_predictors(), -all_nominal_predictors()) %&gt;% # center and scale numeric predictors only\n  recipes::step_pca(c(\"temperature\", \"apparent_temperature\", \"dew_point\"), num_comp = 1) # convert highly correlated variables (&gt;90) into 1 principal component\n\n#apply/view recipe\nsmart_recipe %&gt;% \n  recipes::prep() %&gt;% \n  recipes::bake(new_data = smart_train) %&gt;% \n  head() %&gt;% \n  kable() %&gt;% \n  kable_styling(full_width = F, font = \"lato\") %&gt;% \n  scroll_box(width = \"100%\", height = \"200px\")\n\n\n\n\n\n\nhumidity\nvisibility\npressure\nwind_speed\nwind_bearing\nprecip_intensity\nprecip_probability\nhour\ntime\nhourly_average_power_kw\nsummary_Breezy.and.Foggy\nsummary_Breezy.and.Mostly.Cloudy\nsummary_Breezy.and.Overcast\nsummary_Breezy.and.Partly.Cloudy\nsummary_Clear\nsummary_Drizzle\nsummary_Drizzle.and.Breezy\nsummary_Dry\nsummary_Flurries\nsummary_Foggy\nsummary_Heavy.Rain\nsummary_Heavy.Snow\nsummary_Humid.and.Overcast\nsummary_Humid.and.Partly.Cloudy\nsummary_Light.Rain\nsummary_Light.Rain.and.Breezy\nsummary_Light.Snow\nsummary_Mostly.Cloudy\nsummary_Overcast\nsummary_Partly.Cloudy\nsummary_Rain\nsummary_Rain.and.Breezy\nsummary_Snow\nyear_X2015\nyear_X2016\nmonth_X2\nmonth_X3\nmonth_X4\nmonth_X5\nmonth_X6\nmonth_X7\nmonth_X8\nmonth_X9\nmonth_X10\nmonth_X11\nmonth_X12\nPC1\n\n\n\n\n0.2820793\n0.2571194\n0.2295259\n0.9074681\n-0.2347835\n-0.0560576\n-0.0067788\n0.0723027\n-1.727829\n0\n-0.0101725\n-0.007095\n-0.007095\n-0.0378107\n-1.727704\n-0.1460816\n-0.0073847\n-0.0073847\n-0.0608575\n-0.0465443\n-0.029337\n-0.0203483\n-0.0074788\n-0.0068951\n-0.2423889\n-0.0126264\n-0.1116211\n-0.0788855\n-0.1086493\n2.7684555\n-0.1128537\n-0.0123463\n-0.0618335\n-0.9242423\n-0.8889373\n-0.278143\n-0.2909624\n-0.2860174\n-0.2909087\n-0.2860689\n-0.2906768\n-0.2912252\n-0.2853847\n3.018419\n-0.3573269\n-0.3275356\n-2.267436\n\n\n0.1779755\n0.1048700\n0.0987204\n0.9383314\n-0.2826580\n-0.1275158\n-0.2912672\n0.2167757\n-1.727647\n0\n-0.0101725\n-0.007095\n-0.007095\n-0.0378107\n-1.727704\n-0.1460816\n-0.0073847\n-0.0073847\n-0.0608575\n-0.0465443\n-0.029337\n-0.0203483\n-0.0074788\n-0.0068951\n-0.2423889\n-0.0126264\n-0.1116211\n-0.0788855\n-0.1086493\n2.7684555\n-0.1128537\n-0.0123463\n-0.0618335\n-0.9242423\n-0.8889373\n-0.278143\n-0.2909624\n-0.2860174\n-0.2909087\n-0.2860689\n-0.2906768\n-0.2912252\n-0.2853847\n3.018419\n-0.3573269\n-0.3275356\n-2.335619\n\n\n0.0218198\n0.5052295\n-0.0079559\n1.0052019\n-0.2730831\n-0.0620124\n-0.0067788\n0.3612487\n-1.727465\n0\n-0.0101725\n-0.007095\n-0.007095\n-0.0378107\n-1.727704\n-0.1460816\n-0.0073847\n-0.0073847\n-0.0608575\n-0.0465443\n-0.029337\n-0.0203483\n-0.0074788\n-0.0068951\n-0.2423889\n-0.0126264\n-0.1116211\n-0.0788855\n9.2039128\n-0.3612117\n-0.1128537\n-0.0123463\n-0.0618335\n-0.9242423\n-0.8889373\n-0.278143\n-0.2909624\n-0.2860174\n-0.2909087\n-0.2860689\n-0.2906768\n-0.2912252\n-0.2853847\n3.018419\n-0.3573269\n-0.3275356\n-2.355355\n\n\n0.2300274\n0.5052295\n-0.1209819\n0.8534573\n-0.4358565\n-0.1930191\n-0.3481648\n0.7946676\n-1.726919\n0\n-0.0101725\n-0.007095\n-0.007095\n-0.0378107\n-1.727704\n-0.1460816\n-0.0073847\n-0.0073847\n-0.0608575\n-0.0465443\n-0.029337\n-0.0203483\n-0.0074788\n-0.0068951\n-0.2423889\n-0.0126264\n-0.1116211\n-0.0788855\n-0.1086493\n2.7684555\n-0.1128537\n-0.0123463\n-0.0618335\n-0.9242423\n-0.8889373\n-0.278143\n-0.2909624\n-0.2860174\n-0.2909087\n-0.2860689\n-0.2906768\n-0.2912252\n-0.2853847\n3.018419\n-0.3573269\n-0.3275356\n-2.249099\n\n\n0.5943907\n0.4770352\n-0.0981227\n0.1924680\n-0.5124558\n-0.1930191\n-0.3481648\n1.0836135\n-1.726555\n0\n-0.0101725\n-0.007095\n-0.007095\n-0.0378107\n-1.727704\n-0.1460816\n-0.0073847\n-0.0073847\n-0.0608575\n-0.0465443\n-0.029337\n-0.0203483\n-0.0074788\n-0.0068951\n-0.2423889\n-0.0126264\n-0.1116211\n-0.0788855\n-0.1086493\n2.7684555\n-0.1128537\n-0.0123463\n-0.0618335\n-0.9242423\n-0.8889373\n-0.278143\n-0.2909624\n-0.2860174\n-0.2909087\n-0.2860689\n-0.2906768\n-0.2912252\n-0.2853847\n3.018419\n-0.3573269\n-0.3275356\n-2.089715\n\n\n0.6984945\n0.4544797\n-0.1044725\n0.0381514\n-0.5794801\n-0.1930191\n-0.3481648\n1.2280865\n-1.726373\n0\n-0.0101725\n-0.007095\n-0.007095\n-0.0378107\n-1.727704\n-0.1460816\n-0.0073847\n-0.0073847\n-0.0608575\n-0.0465443\n-0.029337\n-0.0203483\n-0.0074788\n-0.0068951\n-0.2423889\n-0.0126264\n-0.1116211\n-0.0788855\n-0.1086493\n2.7684555\n-0.1128537\n-0.0123463\n-0.0618335\n-0.9242423\n-0.8889373\n-0.278143\n-0.2909624\n-0.2860174\n-0.2909087\n-0.2860689\n-0.2906768\n-0.2912252\n-0.2853847\n3.018419\n-0.3573269\n-0.3275356\n-2.058875\n\n\n\n\n\n\n\n\n\n\nNow it was finally time to build the predictive models! I used 5 different machine learning algorithms to do so: Linear Regression, K-Nearest Neighbor, Elastic Net Regression, Random Forest, and Gradient-Boosted Trees. All of the model types aside from the Linear Regression had specific hyperparameters that I could tune; hyperparameters are parameters external to the actual modeled data that control the learning process and performance of a model.\nThe general steps for building a model using tidymodels are:\n\nSpecify the model type, computational engine, model mode, and hyperparameters to tune (if applicable). Since my problem involves predicting a continuous outcome variable, I always set the mode to regression during this step.\nSet up a workflow by combining the model specifications from step 1 with the model recipe.\nCreate a tuning grid with a range of values for each hyperparameter of the model. Then train and evaluate versions of the model that use different combinations of the hyperparameters using the training data set. Since I have a lot of data and this step can take a long time to run, I saved the model tuning results as .Rds files to avoid re-running the model tuning.\nCompare the performance metric across all model versions and select the best one. Finalize the workflow with the best model and its specific hyperparameters.\nFit the final model to the testing dataset to evaluate its predictive performance on new data.\n\nSteps 1-3 were included in the code chunks below for each model type I explored. Step 4 was included in the next section, Model Results, and step 5 was included in the Best Model Results section below.\n\nLinear Regression\n\n\nCode\n# define model engine and mode\nlm_mod &lt;- parsnip::linear_reg() %&gt;% \n  parsnip::set_engine(\"lm\")\n\n# set up workflow\nlm_wkflow &lt;- workflows::workflow() %&gt;% \n  workflows::add_model(lm_mod) %&gt;% \n  workflows::add_recipe(smart_recipe)\n\n# fit single lm model across all folds of training set\nlm_res &lt;- tune::fit_resamples(lm_wkflow, resamples = smart_folds)\n\n# save en results\nsave(lm_res, file = \"data/inter_data/lm_res.rda\")\n\n\n\n\nK-Nearest Neighbor\n\n\nCode\n# define model engine and mode\nknn_mod &lt;- parsnip::nearest_neighbor(neighbors = tune()) %&gt;% \n  parsnip::set_engine(\"kknn\") %&gt;% \n  parsnip::set_mode(\"regression\")\n\n# set up workflow\nknn_wkflow &lt;- workflow() %&gt;% \n  workflows::add_model(knn_mod) %&gt;% \n  workflows::add_recipe(smart_recipe)\n\n# set up grid to tune neighbors\nknn_grid &lt;- grid_regular(neighbors(range = c(1, 10)), levels = 5)\n\n\n\n\nCode\n# tune neighbors for knn\nknn_res &lt;- tune::tune_grid(knn_wkflow, grid = knn_grid, resamples = smart_folds,\n                  control = control_grid(verbose = TRUE))\n\n# save en results\nsave(knn_res, file = \"data/inter_data/knn_res.rda\")\n\n\n\n\nElastic Net Regression\n\n\nCode\n# set up model\nen_mod &lt;- parsnip::linear_reg(penalty = tune(), mixture = tune()) %&gt;%\n  parsnip::set_engine(\"glmnet\") %&gt;%\n  parsnip::set_mode(\"regression\")\n\n# set up workflow\nen_wkflow &lt;- workflows::workflow() %&gt;%\n  workflows::add_model(en_mod) %&gt;%\n  workflows::add_recipe(smart_recipe)\n\n# create a regular grid for tuning penalty and mixture \nen_grid &lt;- dials::grid_regular(penalty(range = c(0.01,3), trans = identity_trans()), mixture(range = c(0, 1)), levels = 10)\n\n\n\n\nCode\n# tune hyperparameters for en\nen_res &lt;- tune::tune_grid(en_wkflow, grid = en_grid, resamples = smart_folds, \n                          control = control_grid(verbose = TRUE))\n\n# save en results\nsave(en_res, file = \"data/inter_data/en_res.rda\")\n\n\n\n\nRandom Forest\n\n\nCode\n# set up model\nrf_mod &lt;- parsnip::rand_forest(mtry = tune(), \n                               trees = tune(),\n                               min_n = tune()) %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;% \n  set_mode(\"regression\")\n\n# set up workflow\nrf_wkflow &lt;- workflow() %&gt;% \n  add_model(rf_mod) %&gt;% \n  add_recipe(smart_recipe)\n\n# create a regular grid for tuning mtry, trees, and min_n\nrf_grid &lt;- dials::grid_regular(mtry(range = c(1, 15)), # n predictors that will be randomly sampled at each split when creating tree models\n                               trees(range = c(200, 400)), # n of trees contained \n                               min_n(range = c(10, 30)) # min n of data points in a node required for the node to be split further\n                               )\n\n\n\n\nCode\n# tune hyperparameters for rf\nrf_res &lt;- tune::tune_grid(rf_wkflow, resamples = smart_folds, grid = rf_grid,\n                  control = control_grid(verbose = TRUE))\n\n# save rf results\nsave(rf_res, file = \"data/inter_data/rf_res.rda\")\n\n\n\n\nGradient-Boosted Tree\n\n\nCode\n# set up model\nbt_mod &lt;- boost_tree(mtry = tune(),\n                     trees = tune(), \n                     learn_rate = tune()) %&gt;%\n  set_engine(\"xgboost\") %&gt;% \n  set_mode(\"regression\")\n\n# set up workflow\nbt_wkflow &lt;- workflow() %&gt;% \n  add_model(bt_mod) %&gt;% \n  add_recipe(smart_recipe)\n\n# create a regular grid for tuning mtry, trees, and learning rate\nbt_grid &lt;- grid_regular(mtry(range = c(1, 15)), \n                        trees(range = c(200, 400)),\n                        learn_rate(range = c(-10, -1)),\n                        levels = 5)\n\n\n\n\nCode\n# tune hyperparameters for bt\nbt_res &lt;- tune_grid(bt_wkflow, resamples = smart_folds, grid = bt_grid, control = control_grid(verbose = TRUE))\n\n# save rf results\nsave(bt_res, file = \"data/inter_data/bt_res.rda\")\n\n\n\n\n\nAfter all of the models were built, I compared their performance by evaluating the root mean squared error (RMSE) values of each. The RMSE (root mean squared error) measures the magnitude of error between predicted and actual values - lower RMSE values therefore reflect better model performance. Some of the autoplots also displayed the \\(R^2\\) values of the models, which explains the variance of the actual observed values where 1 is a perfect fit - higher \\(R^2\\) values therefore reflect better model performance.\nI use the autoplot function to view each model’s RMSE and the show_best function from the tune package to determine the best-performing models from those that I tuned.\n\nLinear Regression\nThere are no hyperparameters to tune in linear regression models, so I only developed one version of this model type. The mean RMSE across all 5 folds of the training data was 0.787 with a standard error of 0.00552.\n\n\nCode\n# load lm results\n#load(here::here(\"posts/2024-03-22-machine-learning-weather-energy/data/inter_data/lm_res.rda\"))\n\n# https://drive.google.com/file/d/1hcnd63aVF3jjjY6kEjwye84XoU4wrwt_/view?usp=sharing\n# define file id from google drive link\nfile_id &lt;- \"1hcnd63aVF3jjjY6kEjwye84XoU4wrwt_\"\n\n# download file and load into R\ndrive_download(as_id(file_id), overwrite = TRUE)\n\n\n! Using an auto-discovered, cached token.\n\n\n  To suppress this message, modify your code or options to clearly consent to\n  the use of a cached token.\n\n\n  See gargle's \"Non-interactive auth\" vignette for more details:\n\n\n  &lt;https://gargle.r-lib.org/articles/non-interactive-auth.html&gt;\n\n\nℹ The googledrive package is using a cached token for 'kristinart21@gmail.com'.\n\n\nAuto-refreshing stale OAuth token.\n\n\nFile downloaded:\n\n\n• 'lm_res.rda' &lt;id: 1hcnd63aVF3jjjY6kEjwye84XoU4wrwt_&gt;\n\n\nSaved locally as:\n\n\n• 'lm_res.rda'\n\n\nCode\n# load into R\nload(\"lm_res.rda\")\n\n# show best model\nlm_res %&gt;% tune::show_best(metric = \"rmse\")\n\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   0.787     5 0.00552 Preprocessor1_Model1\n\n\nCode\n# save best model results\nlm_best &lt;- lm_res %&gt;% tune::show_best(metric = \"rmse\") %&gt;% slice(1)\n\n\n\n\nK-Nearest Neighbor\nIn K-Nearest Neighbor models, we can tune the K hyperparameter, which specified the number of neighbors that should be considered when evaluating an observation’s expected value. I tuned my K-Nearest Model for 5 values of K between 1 to 10.\nBased on the autoplot, RMSE decreases and \\(R^2\\) increases as the value of K increases. The best performing model version had K = 10 with a mean RMSE across all 5 folds of 1.06 and a standard error of 0.00465.\n\n\nCode\n# load knn results\n#load(here::here(\"posts/2024-03-22-machine-learning-weather-energy/data/inter_data/knn_res.rda\"))\n\n# https://drive.google.com/file/d/1sTymXM0bfOHraxbZoZcTHd2a5sLTs_Ca/view?usp=drive_link\n# define file id from google drive link\nfile_id &lt;- \"1sTymXM0bfOHraxbZoZcTHd2a5sLTs_Ca\"\n\n# download file and load into R\ndrive_download(as_id(file_id), overwrite = TRUE)\n\n\nFile downloaded:\n\n\n• 'knn_res.rda' &lt;id: 1sTymXM0bfOHraxbZoZcTHd2a5sLTs_Ca&gt;\n\n\nSaved locally as:\n\n\n• 'knn_res.rda'\n\n\nCode\n# load into R\nload(\"knn_res.rda\")\n\n# plot\nknn_res %&gt;%\n  autoplot() +\n  theme_minimal() +\n  theme(text = element_text(family = \"lato\"))\n\n\n\n\n\n\n\n\n\nCode\n# show best models\nknn_res %&gt;% tune::show_best(metric = \"rmse\")\n\n\n# A tibble: 5 × 7\n  neighbors .metric .estimator  mean     n std_err .config             \n      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1        10 rmse    standard    1.06     5 0.00465 Preprocessor1_Model5\n2         7 rmse    standard    1.10     5 0.00456 Preprocessor1_Model4\n3         5 rmse    standard    1.13     5 0.00439 Preprocessor1_Model3\n4         3 rmse    standard    1.16     5 0.00424 Preprocessor1_Model2\n5         1 rmse    standard    1.19     5 0.00395 Preprocessor1_Model1\n\n\nCode\n# save best model results\nknn_best &lt;- knn_res %&gt;% tune::show_best(metric = \"rmse\") %&gt;% slice(1)\n\n\n\n\nElastic Net Regression\nIn Elastic Net Regression models, we can tune the penalty and mixture hyperparameters, which specify the …, respectively. I tuned my Elastic Net Regression models for 10 levels of penalty between 0.01 to 3 and 10 levels of mixture between 0 to 1. When mixture = 0, the model is actually performing a ridge regression and when mixture = 1, the model is performing a lasso regression.\nBased on the autoplot, RMSE decreases and \\(R^2\\) increases as the values of mixture, or the proportion of lasso penalty, and penalty, or the amount of regularization, both decrease. The best performing model version had mixture = 0.111, which means it is much closer to being a ridge regression. The best model also had penalty = 0.01, a mean RMSE across all folds of 0.788, and a standard error of 0.00552.\n\n\nCode\n# load en results\n#load(here::here(\"posts/2024-03-22-machine-learning-weather-energy/data/inter_data/en_res.rda\"))\n\n#https://drive.google.com/file/d/1SG3bzqmu669tUFdJleBxV8DIpbM34pxN/view?usp=drive_link\n# define file id from google drive link\nfile_id &lt;- \"1SG3bzqmu669tUFdJleBxV8DIpbM34pxN\"\n\n# download file and load into R\ndrive_download(as_id(file_id), overwrite = TRUE)\n\n\nFile downloaded:\n\n\n• 'en_res.rda' &lt;id: 1SG3bzqmu669tUFdJleBxV8DIpbM34pxN&gt;\n\n\nSaved locally as:\n\n\n• 'en_res.rda'\n\n\nCode\n# load into R\nload(\"en_res.rda\")\n\n# plot\nen_res %&gt;%\n  autoplot() +\n  theme_minimal() +\n  theme(text = element_text(family = \"lato\"))\n\n\n\n\n\n\n\n\n\nCode\n# show best models\nen_res %&gt;% tune::show_best(metric = \"rmse\")\n\n\n# A tibble: 5 × 8\n  penalty mixture .metric .estimator  mean     n std_err .config               \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n1    0.01   0.111 rmse    standard   0.788     5 0.00552 Preprocessor1_Model011\n2    0.01   0.222 rmse    standard   0.788     5 0.00552 Preprocessor1_Model021\n3    0.01   0.333 rmse    standard   0.789     5 0.00552 Preprocessor1_Model031\n4    0.01   0.444 rmse    standard   0.789     5 0.00552 Preprocessor1_Model041\n5    0.01   0     rmse    standard   0.789     5 0.00547 Preprocessor1_Model001\n\n\nCode\n# save best model results\nen_best &lt;- en_res %&gt;% tune::show_best(metric = \"rmse\") %&gt;% slice(1)\n\n\n\n\nRandom Forest\nIn random forest models, we can tune the mtry, trees, and min_n hyperparameters. mtry represents the number of parameters that can be randomly chosen from at each split of the tree. When mtry is less than 1, that means the tree will have no parameters to choose from. When mtry = 15, the tree has access to all of the predictors at each split, which is the same as bagging. I cannot use a mtry greater than 15 because my model recipe only includes 15 predictors, so I used 3 values between 1 to 15. trees represents the total number of trees to include in the forest ensemble. I used 3 values of trees between 200 to 400. min_n represents the minimum number of observations that need to be in a node in order for it to be split further. I used min_n values between 10 to 30 during the tuning process.**\nBased on the autoplot the number of trees does not make much of a difference in model performance, as all the colored lines are virtually on top of each other. The minimal node size did not appear to make much of a difference either, although it looks like min_n = 30 had a slightly lower RMSE than the lower values. The models that had access to 8 parameters at every split consistently performed the best. The best model had mtry = 8, trees = 300, and min_n = 30 with a mean RMSE across all folds of 0.760 and a standard error of 0.00563.\n\n\nCode\n# load rf results\n#load(here::here(\"posts/2024-03-22-machine-learning-weather-energy/data/inter_data/rf_res.rda\"))\n\n# https://drive.google.com/file/d/1D35V6XB1rZNdxx0WrcwS4uVeOSZKeeE6/view?usp=drive_link\n# define file id from google drive link\nfile_id &lt;- \"1D35V6XB1rZNdxx0WrcwS4uVeOSZKeeE6\"\n\n# download file and load into R\ndrive_download(as_id(file_id), overwrite = TRUE)\n\n\nFile downloaded:\n\n\n• 'rf_res.rda' &lt;id: 1D35V6XB1rZNdxx0WrcwS4uVeOSZKeeE6&gt;\n\n\nSaved locally as:\n\n\n• 'rf_res.rda'\n\n\nCode\n# load into R\nload(\"rf_res.rda\")\n\n# plot\nrf_res %&gt;% autoplot() + \n  theme_minimal() +\n  theme(text = element_text(family = \"lato\"))\n\n\n\n\n\n\n\n\n\nCode\n# show best rf models\nrf_res %&gt;% tune::show_best(metric = \"rmse\") \n\n\n# A tibble: 5 × 9\n   mtry trees min_n .metric .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1     8   300    30 rmse    standard   0.760     5 0.00563 Preprocessor1_Model23\n2     8   400    20 rmse    standard   0.760     5 0.00562 Preprocessor1_Model17\n3     8   400    30 rmse    standard   0.760     5 0.00562 Preprocessor1_Model26\n4     8   400    10 rmse    standard   0.760     5 0.00562 Preprocessor1_Model08\n5     8   200    30 rmse    standard   0.760     5 0.00562 Preprocessor1_Model20\n\n\nCode\n# save best model results\nrf_best &lt;- rf_res %&gt;% tune::show_best(metric = \"rmse\") %&gt;% slice(1)\n\n\n\n\nGradient-Boosted Tree\nIn Gradient-Boosted Decision Tree models, we can tune the mtry, trees, min_n, and learn_rate hyperparameters. The first three were described above in the Random Forest model results and the last one, learn_rate, represents how fast the boosted tree changes with each iteration. When learn_rate = 0, the tree doesn’t learn at all and at small values of learn_rate, the tree learns very slowly. I tuned my Gradient-Boosted Tree for 5 values of learn_rate between \\(10^{-10}\\) to \\(10^{-1}\\). Since this hyperparameter is the most impactful for gradient-boosted trees, I opted not to tune an alternative hyperparameter, min_n to reduce the computational power required. I did try out 5 values of mtry between 1 to 15 and 5 values of trees between 200 to 400 as well though.\nBased on the autoplot, models with larger learning rates had the best performance. The number of trees did not make as significant a difference and the number of parameters that were available at every split did not make as much of a difference in most models, although models with mtry &lt; 4 had slightly worse performance. The best performing model had mtry = 15, trees = 200, and learn_rate = 0.1 with a mean RMSE across all folds of 0.755 and a standard error of 0.00568.\n\n\nCode\n# load bt results\n#load(here::here(\"posts/2024-03-22-machine-learning-weather-energy/data/inter_data/bt_res.rda\"))\n\n# https://drive.google.com/file/d/1btCSIyH50_q2OVKrinDxviv1QEN_nk_o/view?usp=drive_link\n# define file id from google drive link\nfile_id &lt;- \"1btCSIyH50_q2OVKrinDxviv1QEN_nk_o\"\n\n# download file and load into R\ndrive_download(as_id(file_id), overwrite = TRUE)\n\n\nFile downloaded:\n\n\n• 'bt_res.rda' &lt;id: 1btCSIyH50_q2OVKrinDxviv1QEN_nk_o&gt;\n\n\nSaved locally as:\n\n\n• 'bt_res.rda'\n\n\nCode\n# load into R\nload(\"bt_res.rda\")\n\n# plot\nbt_res %&gt;% autoplot() + \n  theme_minimal() +\n  theme(text = element_text(family = \"lato\"))\n\n\n\n\n\n\n\n\n\nCode\n# show best rf models\nbt_res %&gt;% tune::show_best(metric = \"rmse\") \n\n\n# A tibble: 5 × 9\n   mtry trees learn_rate .metric .estimator  mean     n std_err .config         \n  &lt;int&gt; &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;           \n1    15   200        0.1 rmse    standard   0.755     5 0.00568 Preprocessor1_M…\n2    11   200        0.1 rmse    standard   0.755     5 0.00561 Preprocessor1_M…\n3    15   250        0.1 rmse    standard   0.755     5 0.00570 Preprocessor1_M…\n4    11   250        0.1 rmse    standard   0.755     5 0.00562 Preprocessor1_M…\n5     8   250        0.1 rmse    standard   0.755     5 0.00563 Preprocessor1_M…\n\n\nCode\n# save best model results\nbt_best &lt;- bt_res %&gt;% tune::show_best(metric = \"rmse\") %&gt;% slice(1)\n\n\n\n\n\nOnce I combined the lowest RMSE from each model type, it was clear that a Gradient-Boosted Tree is the winner! It has a slightly lower RMSE than the runner-ups and predicted hourly_average_power_kw values that deviate from the actual observed values by approximately 0.755 kw on average. As mentioned above, the best performing model had mtry = 15, trees = 200, and learn_rate = 0.1 with a mean RMSE across all folds of 0.755 and a standard error of 0.00568.\n\n\nCode\n# combine best performance results from each model type\nall_rmse &lt;- tibble(Model = c(\"Linear Regression\", \"K-Nearest Neighbor\", \"Elastic Net Regression\", \"Random Forest\", \"Gradient-Boosted Trees\"), RMSE = c(lm_best$mean, knn_best$mean, en_best$mean, rf_best$mean, bt_best$mean)) %&gt;% \n  mutate(RMSE = round(RMSE, 3)) %&gt;% \n  arrange(RMSE)\n\nall_rmse %&gt;% \n  kable() %&gt;% \n  kable_styling(full_width = F, font = \"lato\") \n\n\n\n\n\nModel\nRMSE\n\n\n\n\nGradient-Boosted Trees\n0.755\n\n\nRandom Forest\n0.760\n\n\nLinear Regression\n0.787\n\n\nElastic Net Regression\n0.788\n\n\nK-Nearest Neighbor\n1.060\n\n\n\n\n\n\n\nNext I finalized the workflow using the hyperparameters from the best Gradient-Boosted Tree model and fit it to the entire training dataset.\n\n\nCode\n# save best model \nbest_mod &lt;- bt_res %&gt;% \n  tune::select_best(metric = \"rmse\", mtries, trees, learn_rate)\n\n# finalize workflow with best model\nfinal_wkflow &lt;- tune::finalize_workflow(bt_wkflow, best_mod)\n\n# fit model to training data\nfinal_fit_mod &lt;- parsnip::fit(final_wkflow, smart_train)\n\n\nOne cool thing about tree-based models is we can visualize which predictors were the most significant drivers of the outcome through a variable importance plot (VIP). Based on the VIP for the best model, the PC1 feature was the most important predictor variable by far; remember, the PC1 feature was extracted from the original temperature, apparent_temperature, and dew_point parameters, which were highly correlated (&gt; 90%) to each other. The time parameter was the second most important variable, followed by hour and months. This result makes sense but surprised me! I was expecting more weather-related parameters to show up in the top 10.\n\n\nCode\n# create variable importance plot using training data\nfinal_fit_mod %&gt;% \n  workflowsets::extract_fit_parsnip() %&gt;% \n  vip::vip(aesthetics = list(fill = \"#DAA49A\", color = \"#875053\")) +\n  theme_minimal() +\n  theme(text = element_text(family = \"lato\"))\n\n\n\n\n\n\n\n\n\nNext, it was finally time to introduce the best model to data its never seen before! I ran the model on the testing dataset to see how well it could predict values it was not trained on. The model’s RMSE on the testing dataset was 0.835, which is only slightly worse than the mean RMSE from the training process. This indicates that the training RMSE across 5-folds was a pretty good indicator of the model’s overall performance.\n\n\nCode\n# assess model performance on entire testing set\nfinal_mod_test &lt;- augment(final_fit_mod, smart_test) %&gt;% \n  rmse(truth = hourly_average_power_kw, estimate = .pred) %&gt;% \n  print()\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.835\n\n\nWhen I plotted the predicted values of power usage against the actual observed values, it was clear that the model does not predict high values well at all; in fact, the model did not predict any power usage values higher than 3.75 kW. This is due to that strong positive skew in the outcome variable, which means even our best model was only trained with a handful of observations for the higher power usage values. Only the values that fall directly on the diagonal line in the plot below were accurately predicted by the model.\n\n\nCode\n# plot predicted vs. actual values from testing data\naugment(final_fit_mod, smart_test) %&gt;% \n  ggplot(aes(x = .pred, y = hourly_average_power_kw)) +\n  geom_point(color = \"#DAA49A\", alpha = 0.2) +\n  geom_abline(lty = 2) +\n  coord_obs_pred() +\n  labs(title = \"Predicted vs. Actual Values of Hourly Average Power Usage (kW)\",\n       y = \"Actual Hourly Average Power Usage (kW)\",\n       y = \"Predicted Hourly Average Power Usage (kW)\") +\n  theme_minimal()+\n  theme(text = element_text(family = \"lato\"),\n        plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Update this with fake or untested data for blog post when have mroe time. \n# For fun, I used the model to predict residential power usage during the warmest and coolest observations in smart df. \n# filter to observations with highest and lowest temperature\nextreme_temps &lt;- smart_mod %&gt;%\n  filter(row_number() == which.max(temperature) | row_number() == which.min(temperature))\n\nfinal_predictions &lt;- augment(final_fit_mod, extreme_temps) %&gt;% \n  select(.pred, hourly_average_power_kw, temperature, everything())\n\nfinal_predictions \n\n\n\nConclusion\nThe best model for predicting residential energy usage based on weather and time was a Gradient-Boosted Tree model. A Random Forest model did nearly as well, with only a small difference in RMSE separating the two. This is no surprise since decision tree-based models tend to work really well on many types of data. This is because tree-based models are highly flexible and non-parametric, meaning they do not assume any parametric constraints on the outcome variable. Interestingly, the Elastic Net Regression and Linear Regression models had a pretty similar performance to these top tree-based models, indicating that they also do a decent job of predicting residential energy usage; these model types could be more useful to a user who is willing to accept a little higher error in order to use models that are much less computationally expensive. The K-Nearest Neighbor model had the worst predictive power, which makes sense since KNN models tend to do poorly on data with too many predictors, or high dimensions.\nOverall, the best model did an okay job at predicting residential energy usage. The actual values of the outcome variable, hourly_average_power_kw, ranged between 0 to 11.5 kW. On average, the best model’s predicted values were about 0.835 kW off from the actual observed values based on the testing RMSE; this means the model’s average error was about 7% of the entire range. The model could be improved by adding in more data values that have high energy usage values. By normalizing the distribution of the outcome variable in this way, I could improve the model’s learning and potentially improve its performance.\nIt was interesting to see that the principal component of temperature, apparent_temperature, and dew_point is the most important predictor of residential energy usage. Since apparent temperature itself is also a factor of relative humidity and wind speed, this principal component feature may represent all the key weather predictors due to their interrelatedness. Therefore, it makes sense that time variables (hour, time, month) would be the next most important predictors in the VIP rather than some of the general weather predictors that remained (descriptive weather summary, wind-bearing, etc.). This makes intuitive sense and it was fun to see it explained quantitatively. I learned a lot through this project and am looking forward to exploring more energy data with machine learning algorithms!"
  },
  {
    "objectID": "posts/2024-03-22-machine-learning-weather-energy/index.html#load-and-tidy-data",
    "href": "posts/2024-03-22-machine-learning-weather-energy/index.html#load-and-tidy-data",
    "title": "Predicting Residential Energy Usage based on Weather",
    "section": "Load and Tidy Data",
    "text": "Load and Tidy Data\n\n\nCode\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(janitor)\nlibrary(ggplot2)\nlibrary(kableExtra)\nlibrary(here)\nlibrary(lubridate)\nlibrary(purrr)\nlibrary(naniar)\nlibrary(corrplot)\nlibrary(showtext)\nlibrary(googledrive)\ntidymodels_prefer()\noptions(googledrive_quiet = TRUE) \n\n# set seed to reproduce results\nset.seed(2244)\n\n# load fonts\nfont_add_google(name = \"Lato\", family = \"lato\")\nshowtext_auto()\n\n\nSince there were a large amount of individual files that needed to be loaded, aggregated, and cleaned, I began by defining two functions:\n\nThe first function, tidy_weather_data, loads and aggregates data from all files that contain weather information, converts the column headers to lower snake case, and adds columns for time parameters (datetime, date, year, month, and hour) using the tidyverse, janitor, and lubridate packages. I decided to separate out the year, month, and hour columns to be their own predictors because I believe energy usage may vary based on them. I also filtered the dataset to only contain dates from noon on October 14, 2014 onward, as that is the first datetime for which data exists in the apartments dataset.\n\n\n\nCode\n# define list of weather files\nweather_files &lt;- list.files(here::here(\"data/apartment-weather\"), full.names = TRUE) \n\n# define function to process and clean weather data\ntidy_weather_data &lt;- function(file_path) {\n\n  df &lt;- read_csv(file_path) %&gt;% \n    janitor::clean_names() %&gt;% \n    mutate(datetime = lubridate::as_datetime(time, origin = \"1970-01-01\", tz = \"America/New_York\"),  # Convert unix timestamp to Eastern datetime \n           date = lubridate::date(datetime),\n           hour = lubridate::hour(datetime),\n           month = lubridate::month(datetime),\n           year = lubridate::year(datetime)) %&gt;% \n    filter(datetime &gt;= lubridate::ymd_hms(\"2014-10-15 12:00:00\")) # filter data to startdate of apartment data\n  \n  return(df)\n}\n\n# apply function over weather files \nweather_data &lt;- purrr::map_dfr(weather_files, tidy_weather_data)\n\n# inspect result df\nhead(weather_data)\nsummary(weather_data)\n\n\n\nThe second function, tidy_apartment_data, loads and aggregates data from all files that contain electricity data, converts the column headers to lower snake case, and adds columns for time parameters (datetime, date, year, month, and hour) using the tidyverse, janitor, and lubridate packages. I also added a new column to the dataframe containing the unique apartment identification numbers, which were included in the file names. Lastly, I summarized the raw minute-level data into hourly average power use in kiloWatts to reduce the computational power required.\n\n\n\nCode\n# define list of apartment files\napt_files &lt;- list.files(here::here(\"data/apartment\"), pattern = \".csv\", full.names = TRUE, recursive = TRUE) \n\n# define function to process and clean apartment data\ntidy_apartment_data &lt;- function(file_path) {\n  df &lt;- read_csv(file_path, col_names = c(\"datetime\", \"power_kw\"), col_types = cols(datetime = col_datetime(), power_kw = col_double())) %&gt;%\n    janitor::clean_names() %&gt;%\n    mutate(apt_id = as.numeric(stringr::str_extract(basename(file_path), \"Apt(\\\\d+)_\\\\d{4}\") %&gt;% stringr::str_extract(\"(?&lt;=Apt)\\\\d+\")),\n           datetime = lubridate::ymd_hms(datetime),\n           date = lubridate::date(datetime),\n           hour = lubridate::hour(datetime),\n           month = lubridate::month(datetime),\n           year = lubridate::year(datetime)) %&gt;%\n    group_by(date, hour, apt_id) %&gt;%\n    summarize(hourly_average_power_kw = as.numeric(mean(as.numeric(power_kw), na.rm = TRUE))) %&gt;%\n    ungroup()\n  \n  return(df)\n\n}\n\n# apply function over all apartment files \napt_data &lt;- purrr::map_dfr(apt_files, tidy_apartment_data )\n\n# inspect result df\nhead(apt_data)\nsummary(apt_data)\n\n\nAfter loading, aggregating, and cleaning all of the data (good job to my computer), I combined the weather dataset with the apartments dataset by joining them based on the common date and hour columns. I also defined the month, year, and apartment IDs as factors to make them easier to plot later on. Lastly, I randomly sampled 50 out of the 114 apartments in an effort to decrease the computational power and run time required for my machine learning models.\n\n\nCode\n# define random apartment ids to use for models\napt_sample &lt;- sample(1:114, 50, replace = FALSE)\n\n# combine weather and apartment data\nsmart_df &lt;- apt_data %&gt;% \n  full_join(weather_data, by = c(\"date\", \"hour\"), relationship = \"many-to-many\") %&gt;% \n  mutate_at(vars(hour,hourly_average_power_kw, temperature, humidity, visibility, apparent_temperature, pressure, wind_speed, cloud_cover, wind_bearing, precip_intensity, dew_point, precip_probability), as.numeric) %&gt;% \n  mutate_at(vars(month, year, apt_id), as.factor) %&gt;% \n  filter(apt_id %in% apt_sample)\n\n# save combined df\nsave(smart_df, file = \"data/inter_data/smart_df.csv\")"
  },
  {
    "objectID": "posts/2024-03-22-machine-learning-weather-energy/index.html#exploratory-data-analysis",
    "href": "posts/2024-03-22-machine-learning-weather-energy/index.html#exploratory-data-analysis",
    "title": "Predicting Residential Energy Usage based on Weather",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nOnce all my data was in one dataframe, my first real step was to figure out how much of it was missing. Here I used the vis_miss() function from the naniar package to visualize any missing values.\n\n\nCode\n# load combined df\nload(here::here(\"posts/2024-03-22-machine-learning-weather-energy/data/inter_data/smart_df.csv\"))\n\n# visualize missing data\nsmart_df %&gt;%\n  naniar::vis_miss(warn_large_data = FALSE) +\n  theme(text = element_text(family = \"lato\"))\n\n\n\n\n\n\n\n\n\nSurprisingly, the dataset was near-complete! Only 1.2% of it was missing (this might be a new record for me). Nearly all of the missing values were from the cloud_cover column. I wonder why this variable was missing so many observations in an otherwise comprehensive dataset - maybe cloud cover relied on manual human measurement while the other variables were automatically measured by instruments.\nSince the cloud_cover variable itself was missing 13% of observations and was not one of the most impactful predictor variables, I decided to drop the entire variable from the dataset. This way, I avoided losing 13% of the entire dataset like I would if I performed a complete case/ pairwise deletion.\nThe rest of the variables in the dataset were missing between 0-1% of their values. Since this is such a small proportion, I decided to performa complete case/ pairwise deletion across the entire dataset. If the proportion was higher, I would have imputed the missing values.\n\n\nCode\n# remove variables with missing data and non-useful data\nsmart_mod &lt;- smart_df %&gt;% \n  select(-cloud_cover) %&gt;% \n  drop_na()\n\n\nOnce all the missing values were taken care of, I took a peek at the data through descriptive and summary statistics.\nMy final dataframe had 20 variables and 953,578 observations.\n\n\nCode\n# explore data  \nsmart_mod %&gt;% dim()\n\n\n[1] 953578     20\n\n\nThe column names of my dataframe were:\n\n\nCode\nsmart_mod %&gt;% names()\n\n\n [1] \"date\"                    \"hour\"                   \n [3] \"apt_id\"                  \"hourly_average_power_kw\"\n [5] \"temperature\"             \"icon\"                   \n [7] \"humidity\"                \"visibility\"             \n [9] \"summary\"                 \"apparent_temperature\"   \n[11] \"pressure\"                \"wind_speed\"             \n[13] \"time\"                    \"wind_bearing\"           \n[15] \"precip_intensity\"        \"dew_point\"              \n[17] \"precip_probability\"      \"datetime\"               \n[19] \"month\"                   \"year\"                   \n\n\nIt looked like the data type for all the variables are appropriate. Most of the variables were numeric while the summary and icon variables were categorical. The apartment ID, month, and year were all factors because I defined them to be factors above. Lastly, the datetime and date columns are POSIXct and Date objects, respectively.\n\n\nCode\nsmart_mod %&gt;% str()\n\n\ntibble [953,578 × 20] (S3: tbl_df/tbl/data.frame)\n $ date                   : Date[1:953578], format: \"2014-10-15\" \"2014-10-15\" ...\n $ hour                   : num [1:953578] 12 13 14 15 16 17 18 19 20 21 ...\n $ apt_id                 : Factor w/ 114 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ hourly_average_power_kw: num [1:953578] 0 0 0 0 0 0 0 0 0 0 ...\n $ temperature            : num [1:953578] 74.2 75.3 75.9 75.9 74.7 ...\n $ icon                   : chr [1:953578] \"partly-cloudy-day\" \"partly-cloudy-day\" \"cloudy\" \"clear-day\" ...\n $ humidity               : num [1:953578] 0.73 0.71 0.68 0.67 0.7 0.72 0.76 0.79 0.81 0.83 ...\n $ visibility             : num [1:953578] 9.56 9.29 10 10 9.79 10 9.95 9.95 9.91 8.86 ...\n $ summary                : chr [1:953578] \"Partly Cloudy\" \"Partly Cloudy\" \"Overcast\" \"Clear\" ...\n $ apparent_temperature   : num [1:953578] 74.2 75.3 75.9 75.9 74.7 ...\n $ pressure               : num [1:953578] 1019 1018 1017 1017 1016 ...\n $ wind_speed             : num [1:953578] 10.07 10.19 10.45 11.05 9.19 ...\n $ time                   : num [1:953578] 1.41e+09 1.41e+09 1.41e+09 1.41e+09 1.41e+09 ...\n $ wind_bearing           : num [1:953578] 183 178 179 174 178 162 152 154 147 146 ...\n $ precip_intensity       : num [1:953578] 0.0023 0.0011 0.0022 0 0.0011 0 0 0 0 0.001 ...\n $ dew_point              : num [1:953578] 65 65.2 64.8 64.2 64.3 ...\n $ precip_probability     : num [1:953578] 0.06 0.01 0.06 0 0.01 0 0 0 0 0.01 ...\n $ datetime               : POSIXct[1:953578], format: \"2014-10-15 12:00:00\" \"2014-10-15 13:00:00\" ...\n $ month                  : Factor w/ 12 levels \"1\",\"2\",\"3\",\"4\",..: 10 10 10 10 10 10 10 10 10 10 ...\n $ year                   : Factor w/ 3 levels \"2014\",\"2015\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\nThe summary statistics for all the variables are shown in the table below:\n\n\nCode\nsmart_mod %&gt;% summary() %&gt;% \n  kbl(caption = \"Summary Statistics for all Variables in the Smart* Dataset\") %&gt;% \n  kable_styling(full_width = F, font = \"lato\") %&gt;% \n  scroll_box(width = \"100%\", height = \"200px\")\n\n\n\n\nSummary Statistics for all Variables in the Smart* Dataset\n\n\n\ndate\nhour\napt_id\nhourly_average_power_kw\ntemperature\nicon\nhumidity\nvisibility\nsummary\napparent_temperature\npressure\nwind_speed\ntime\nwind_bearing\nprecip_intensity\ndew_point\nprecip_probability\ndatetime\nmonth\nyear\n\n\n\n\n\nMin. :2014-10-15\nMin. : 0.0\n92 : 19379\nMin. : 0.0000\nMin. :-13.05\nLength:953578\nMin. :0.130\nMin. : 0.290\nLength:953578\nMin. :-32.99\nMin. : 986.1\nMin. : 0.020\nMin. :1.413e+09\nMin. : 0.0\nMin. :0.000000\nMin. :-27.69\nMin. :0.00000\nMin. :2014-10-15 08:00:00.00\n11 :108150\n2014: 93127\n\n\n\n1st Qu.:2015-05-01\n1st Qu.: 6.0\n36 : 19068\n1st Qu.: 0.2112\n1st Qu.: 34.49\nClass :character\n1st Qu.:0.530\n1st Qu.: 9.190\nClass :character\n1st Qu.: 29.49\n1st Qu.:1012.0\n1st Qu.: 3.630\n1st Qu.:1.430e+09\n1st Qu.:158.0\n1st Qu.:0.000000\n1st Qu.: 24.25\n1st Qu.:0.00000\n1st Qu.:2015-05-01 02:00:00.00\n10 : 94277\n2015:439100\n\n\n\nMedian :2015-11-15\nMedian :12.0\n38 : 19068\nMedian : 0.9015\nMedian : 48.62\nMode :character\nMedian :0.700\nMedian :10.000\nMode :character\nMedian : 46.43\nMedian :1017.2\nMedian : 5.860\nMedian :1.448e+09\nMedian :220.0\nMedian :0.000000\nMedian : 37.82\nMedian :0.00000\nMedian :2015-11-15 18:00:00.00\n12 : 92501\n2016:421351\n\n\n\nMean :2015-11-14\nMean :11.5\n40 : 19068\nMean : 1.1568\nMean : 48.90\nNA\nMean :0.676\nMean : 9.103\nNA\nMean : 46.20\nMean :1017.2\nMean : 6.539\nMean :1.448e+09\nMean :207.4\nMean :0.003222\nMean : 37.48\nMean :0.06097\nMean :2015-11-15 09:39:56.80\n1 : 76800\nNA\n\n\n\n3rd Qu.:2016-05-31\n3rd Qu.:17.0\n53 : 19068\n3rd Qu.: 1.7686\n3rd Qu.: 64.76\nNA\n3rd Qu.:0.850\n3rd Qu.:10.000\nNA\n3rd Qu.: 64.76\n3rd Qu.:1022.3\n3rd Qu.: 8.650\n3rd Qu.:1.465e+09\n3rd Qu.:294.0\n3rd Qu.:0.000000\n3rd Qu.: 54.31\n3rd Qu.:0.00000\n3rd Qu.:2016-05-31 12:00:00.00\n5 : 74400\nNA\n\n\n\nMax. :2016-12-28\nMax. :23.0\n109 : 19068\nMax. :79.1174\nMax. : 93.78\nNA\nMax. :0.980\nMax. :10.000\nNA\nMax. : 98.45\nMax. :1044.5\nMax. :24.940\nMax. :1.483e+09\nMax. :359.0\nMax. :0.426900\nMax. : 75.29\nMax. :0.90000\nMax. :2016-12-28 22:00:00.00\n7 : 74400\nNA\n\n\n\nNA\nNA\n(Other):838859\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n(Other):433050\nNA\n\n\n\n\n\n\n\nLastly, here’s a look at the first few rows of the data in case you want to get a feel for it:\n\n\nCode\nsmart_mod %&gt;% head() %&gt;% \n  kbl(caption = \"First 6 Rows of the Smart* Dataset\") %&gt;% \n  kable_styling(full_width = F, font = \"lato\") %&gt;% \n  scroll_box(width = \"100%\", height = \"200px\")\n\n\n\n\nFirst 6 Rows of the Smart* Dataset\n\n\ndate\nhour\napt_id\nhourly_average_power_kw\ntemperature\nicon\nhumidity\nvisibility\nsummary\napparent_temperature\npressure\nwind_speed\ntime\nwind_bearing\nprecip_intensity\ndew_point\nprecip_probability\ndatetime\nmonth\nyear\n\n\n\n\n2014-10-15\n12\n1\n0\n74.20\npartly-cloudy-day\n0.73\n9.56\nPartly Cloudy\n74.20\n1018.98\n10.07\n1413388800\n183\n0.0023\n65.00\n0.06\n2014-10-15 12:00:00\n10\n2014\n\n\n2014-10-15\n13\n1\n0\n75.32\npartly-cloudy-day\n0.71\n9.29\nPartly Cloudy\n75.32\n1017.95\n10.19\n1413392400\n178\n0.0011\n65.21\n0.01\n2014-10-15 13:00:00\n10\n2014\n\n\n2014-10-15\n14\n1\n0\n75.91\ncloudy\n0.68\n10.00\nOvercast\n75.91\n1017.11\n10.45\n1413396000\n179\n0.0022\n64.76\n0.06\n2014-10-15 14:00:00\n10\n2014\n\n\n2014-10-15\n15\n1\n0\n75.86\nclear-day\n0.67\n10.00\nClear\n75.86\n1016.71\n11.05\n1413399600\n174\n0.0000\n64.21\n0.00\n2014-10-15 15:00:00\n10\n2014\n\n\n2014-10-15\n16\n1\n0\n74.74\nclear-day\n0.70\n9.79\nClear\n74.74\n1016.49\n9.19\n1413403200\n178\n0.0011\n64.34\n0.01\n2014-10-15 16:00:00\n10\n2014\n\n\n2014-10-15\n17\n1\n0\n74.14\npartly-cloudy-day\n0.72\n10.00\nPartly Cloudy\n74.14\n1016.22\n9.86\n1413406800\n162\n0.0000\n64.48\n0.00\n2014-10-15 17:00:00\n10\n2014"
  },
  {
    "objectID": "posts/2024-03-22-machine-learning-weather-energy/index.html#visual-exploratory-data-analysis",
    "href": "posts/2024-03-22-machine-learning-weather-energy/index.html#visual-exploratory-data-analysis",
    "title": "Predicting Residential Energy Usage based on Weather",
    "section": "Visual Exploratory Data Analysis",
    "text": "Visual Exploratory Data Analysis\nNext, I began my favorite type of exploratory analysis - visualization!\n\nCorrelation Plot\nFirst off, I explored how correlated all of the numeric variables were by using the corrplot() function from the corrplot package to visualize a correlation matrix. It showed me that temperature, apparent_temperature, and dew point were highly positively correlated, which makes sense since they are physically related; since they were so highly correlated (&gt; 90%), I decide to use a principal components analysis (PCA) in my model recipe below to collapse them into 1 feature instead of 3. Interestingly, these three variables were negatively correlated with hourly_average_power_kw, which is the outcome variable of interest. Another interesting finding is that visibility was negatively correlated to humidity, precipitation intensity, and precipitation probability - this makes sense since it is hard to see far while it’s raining. The other correlations were also logical since weather variables are typically all related.\n\n\nCode\n# correlation plot of all variables\nsmart_mod %&gt;%\n  select(where(is.numeric)) %&gt;%\n  cor() %&gt;%\n  corrplot(method = 'number', order = 'FPC', type = 'lower', family = \"lato\", number.cex=0.6, bg = \"grey80\") \n\n\n\n\n\n\n\n\n\n\n\nPower Usage Distribution\nNext, I explored the distribution of hourly_average_power_kw, which is the outcome variable of interest. The outcome variable was highly positively skewed, as the vast majority of observations (order of \\(10^5\\) - \\(10^6\\)) for each bin were between 0 - 10 kiloWatts. There were only a handful of observations (order of \\(10^1\\)) for each bin between 15 - 232 kiloWatts.\n\n\nCode\n# histogram of energy usage\nggplot(data = smart_mod, aes(x = hourly_average_power_kw))+\n  geom_histogram(fill = \"#DAA49A\", col = \"#875053\", bins = 150)+\n  labs(x = \"Power Usage (kW)\", y = \"Count\")+\n  theme_minimal()+\n  theme(text = element_text(family = \"lato\"))\n\n\n\n\n\n\n\n\n\n\n\nPower Usage by Month\nI visualized the relationship between power usage and month by making a box-and-whisket plot. As expected, power usage is lowest during the warm months (June - September) and highest during the cold months (November - February); this makes sense since most people crank up the heat in the winter months to stay warm. Interestingly, there were quite a few outliers for all of the months, which could mean some apartments use more energy in general.\n\n\nCode\n# boxplot of energy usage against month\nggplot(data = smart_mod, aes(x = factor(month, labels = month.name), y = hourly_average_power_kw, group = month))+\n  geom_boxplot(fill = \"#DAA49A\", col = \"#875053\")+\n  scale_y_continuous(limits = c(0,12))+\n  #geom_jitter(alpha = 0.4, col = \"#DAA49A\")+\n  labs(x = \"Month\", y = \"Power (kW)\")+\n  theme_minimal()+\n  theme(text = element_text(family = \"lato\"))\n\n\n\n\n\n\n\n\n\n\n\nTemperature Distribution\nNext, I explored the distribution of temperature, which I expect would have a significant impact on energy use. Temprature had a normal distribution with a slight left tail, indicating a small negative skew.\n\n\nCode\n# histogram of temperature\nggplot(data = smart_mod, aes(x = temperature))+\n  # geom_histogram(aes(y = ..density..), bins = 50, fill = \"#DAA49A\", col = \"#875053\")+\n  # geom_density(linewidth = 1.5)+\n  geom_histogram(bins = 50, fill = \"#DAA49A\", col = \"#875053\")+\n  labs(x = \"Temperature (deg F)\", y = \"Count\")+\n  theme_minimal()+\n  theme(text = element_text(family = \"lato\"))"
  },
  {
    "objectID": "posts/2024-03-22-machine-learning-weather-energy/index.html#model-set-up",
    "href": "posts/2024-03-22-machine-learning-weather-energy/index.html#model-set-up",
    "title": "Predicting Residential Energy Usage based on Weather",
    "section": "Model Set-Up",
    "text": "Model Set-Up\nNext, I prepared my data for the machine learning models. Here, I randomly split the data into training and testing datasets, split the training dataset into 5 folds for k-fold cross validation, and specified a model recipe.\n\nSplit Training and Testing Data\nFirst off, I split the full dataset into training and testing datasets. Like the names imply, the training dataset will be used to train the models while the testing dataset will be used to test the predictive power of the models at the very end. I split the data using the initial_split() function from the rsample package. The split is stratified on the outcome variable, hourly_average_power_kw, to ensure that both the training and the testing datasets have roughly the same distribution of hourly_average_power_kw. I split the full dataset so that 3/4 of it becomes the training dataset and the remaining 1/4 becomes the testing dataset. This was to ensure there is a good amount of data for training while still retaining enough for substantial testing.\n\n\nCode\n# split data\nsmart_split &lt;- rsample::initial_split(smart_mod, \n                                        prop = 0.75,\n                                        strata = hourly_average_power_kw)\n\n# assign splits to train and test objects\nsmart_train &lt;- rsample::training(smart_split)\nsmart_test &lt;- rsample::testing(smart_split)\n\n\n\n\nK-Fold Cross Validation\nI also performed a k-fold cross validation on my entire training set with k = 5. This splits the entire training set into 5 folds that each consist of a mini-training, or analysis set, and a mini-testing, or assessment set. Each of my models will be trained on the analysis sets and tested on the assessment sets of each fold and the mean performance metric across all folds will be reported. I used the vfold function from rsample to split the training dataset into 5 folds and stratified on the outcome variable once again to ensure that each subset of the data has the same distribution of hourly_average_power_kw.\n\n\nCode\n# split data for k-fold CV\nsmart_folds &lt;- rsample::vfold_cv(smart_train, v = 5, strata = hourly_average_power_kw)\n\n\n\n\nBuild Recipe\nThen I specified a recipe for the models to use. I used the recipes package to do things like define the model recipe, dummy code the categorical variables, center all predictors, scale all predictors, and reduce the dimensions of those highly correlated predictors I noticed during the EDA (temperature, apparent_temperature, and dew_point).\nMy recipe tells my models to predict hourly_average_power_kw as a function of temperature + humidity + visibility + summary + apparent_temperature + pressure + wind_speed + wind_bearing + precip_intensity + dew_point + precip_probability + year + month + hour + time. The results from the pre-processing steps I specified are shown below.\n\n\nCode\n# define recipe\nsmart_recipe &lt;- recipes::recipe(hourly_average_power_kw ~ temperature + humidity + visibility + summary + apparent_temperature + pressure + wind_speed + wind_bearing + precip_intensity + dew_point + precip_probability + year + month + hour + time, data = smart_train) %&gt;% \n  recipes::step_dummy(all_nominal_predictors()) %&gt;% # dummy code categorical variables\n  recipes::step_normalize(all_numeric_predictors(), -all_nominal_predictors()) %&gt;% # center and scale numeric predictors only\n  recipes::step_pca(c(\"temperature\", \"apparent_temperature\", \"dew_point\"), num_comp = 1) # convert highly correlated variables (&gt;90) into 1 principal component\n\n#apply/view recipe\nsmart_recipe %&gt;% \n  recipes::prep() %&gt;% \n  recipes::bake(new_data = smart_train) %&gt;% \n  head() %&gt;% \n  kable() %&gt;% \n  kable_styling(full_width = F, font = \"lato\") %&gt;% \n  scroll_box(width = \"100%\", height = \"200px\")\n\n\n\n\n\n\nhumidity\nvisibility\npressure\nwind_speed\nwind_bearing\nprecip_intensity\nprecip_probability\nhour\ntime\nhourly_average_power_kw\nsummary_Breezy.and.Foggy\nsummary_Breezy.and.Mostly.Cloudy\nsummary_Breezy.and.Overcast\nsummary_Breezy.and.Partly.Cloudy\nsummary_Clear\nsummary_Drizzle\nsummary_Drizzle.and.Breezy\nsummary_Dry\nsummary_Flurries\nsummary_Foggy\nsummary_Heavy.Rain\nsummary_Heavy.Snow\nsummary_Humid.and.Overcast\nsummary_Humid.and.Partly.Cloudy\nsummary_Light.Rain\nsummary_Light.Rain.and.Breezy\nsummary_Light.Snow\nsummary_Mostly.Cloudy\nsummary_Overcast\nsummary_Partly.Cloudy\nsummary_Rain\nsummary_Rain.and.Breezy\nsummary_Snow\nyear_X2015\nyear_X2016\nmonth_X2\nmonth_X3\nmonth_X4\nmonth_X5\nmonth_X6\nmonth_X7\nmonth_X8\nmonth_X9\nmonth_X10\nmonth_X11\nmonth_X12\nPC1\n\n\n\n\n0.2820793\n0.2571194\n0.2295259\n0.9074681\n-0.2347835\n-0.0560576\n-0.0067788\n0.0723027\n-1.727829\n0\n-0.0101725\n-0.007095\n-0.007095\n-0.0378107\n-1.727704\n-0.1460816\n-0.0073847\n-0.0073847\n-0.0608575\n-0.0465443\n-0.029337\n-0.0203483\n-0.0074788\n-0.0068951\n-0.2423889\n-0.0126264\n-0.1116211\n-0.0788855\n-0.1086493\n2.7684555\n-0.1128537\n-0.0123463\n-0.0618335\n-0.9242423\n-0.8889373\n-0.278143\n-0.2909624\n-0.2860174\n-0.2909087\n-0.2860689\n-0.2906768\n-0.2912252\n-0.2853847\n3.018419\n-0.3573269\n-0.3275356\n-2.267436\n\n\n0.1779755\n0.1048700\n0.0987204\n0.9383314\n-0.2826580\n-0.1275158\n-0.2912672\n0.2167757\n-1.727647\n0\n-0.0101725\n-0.007095\n-0.007095\n-0.0378107\n-1.727704\n-0.1460816\n-0.0073847\n-0.0073847\n-0.0608575\n-0.0465443\n-0.029337\n-0.0203483\n-0.0074788\n-0.0068951\n-0.2423889\n-0.0126264\n-0.1116211\n-0.0788855\n-0.1086493\n2.7684555\n-0.1128537\n-0.0123463\n-0.0618335\n-0.9242423\n-0.8889373\n-0.278143\n-0.2909624\n-0.2860174\n-0.2909087\n-0.2860689\n-0.2906768\n-0.2912252\n-0.2853847\n3.018419\n-0.3573269\n-0.3275356\n-2.335619\n\n\n0.0218198\n0.5052295\n-0.0079559\n1.0052019\n-0.2730831\n-0.0620124\n-0.0067788\n0.3612487\n-1.727465\n0\n-0.0101725\n-0.007095\n-0.007095\n-0.0378107\n-1.727704\n-0.1460816\n-0.0073847\n-0.0073847\n-0.0608575\n-0.0465443\n-0.029337\n-0.0203483\n-0.0074788\n-0.0068951\n-0.2423889\n-0.0126264\n-0.1116211\n-0.0788855\n9.2039128\n-0.3612117\n-0.1128537\n-0.0123463\n-0.0618335\n-0.9242423\n-0.8889373\n-0.278143\n-0.2909624\n-0.2860174\n-0.2909087\n-0.2860689\n-0.2906768\n-0.2912252\n-0.2853847\n3.018419\n-0.3573269\n-0.3275356\n-2.355355\n\n\n0.2300274\n0.5052295\n-0.1209819\n0.8534573\n-0.4358565\n-0.1930191\n-0.3481648\n0.7946676\n-1.726919\n0\n-0.0101725\n-0.007095\n-0.007095\n-0.0378107\n-1.727704\n-0.1460816\n-0.0073847\n-0.0073847\n-0.0608575\n-0.0465443\n-0.029337\n-0.0203483\n-0.0074788\n-0.0068951\n-0.2423889\n-0.0126264\n-0.1116211\n-0.0788855\n-0.1086493\n2.7684555\n-0.1128537\n-0.0123463\n-0.0618335\n-0.9242423\n-0.8889373\n-0.278143\n-0.2909624\n-0.2860174\n-0.2909087\n-0.2860689\n-0.2906768\n-0.2912252\n-0.2853847\n3.018419\n-0.3573269\n-0.3275356\n-2.249099\n\n\n0.5943907\n0.4770352\n-0.0981227\n0.1924680\n-0.5124558\n-0.1930191\n-0.3481648\n1.0836135\n-1.726555\n0\n-0.0101725\n-0.007095\n-0.007095\n-0.0378107\n-1.727704\n-0.1460816\n-0.0073847\n-0.0073847\n-0.0608575\n-0.0465443\n-0.029337\n-0.0203483\n-0.0074788\n-0.0068951\n-0.2423889\n-0.0126264\n-0.1116211\n-0.0788855\n-0.1086493\n2.7684555\n-0.1128537\n-0.0123463\n-0.0618335\n-0.9242423\n-0.8889373\n-0.278143\n-0.2909624\n-0.2860174\n-0.2909087\n-0.2860689\n-0.2906768\n-0.2912252\n-0.2853847\n3.018419\n-0.3573269\n-0.3275356\n-2.089715\n\n\n0.6984945\n0.4544797\n-0.1044725\n0.0381514\n-0.5794801\n-0.1930191\n-0.3481648\n1.2280865\n-1.726373\n0\n-0.0101725\n-0.007095\n-0.007095\n-0.0378107\n-1.727704\n-0.1460816\n-0.0073847\n-0.0073847\n-0.0608575\n-0.0465443\n-0.029337\n-0.0203483\n-0.0074788\n-0.0068951\n-0.2423889\n-0.0126264\n-0.1116211\n-0.0788855\n-0.1086493\n2.7684555\n-0.1128537\n-0.0123463\n-0.0618335\n-0.9242423\n-0.8889373\n-0.278143\n-0.2909624\n-0.2860174\n-0.2909087\n-0.2860689\n-0.2906768\n-0.2912252\n-0.2853847\n3.018419\n-0.3573269\n-0.3275356\n-2.058875"
  },
  {
    "objectID": "posts/2024-03-22-machine-learning-weather-energy/index.html#model-building",
    "href": "posts/2024-03-22-machine-learning-weather-energy/index.html#model-building",
    "title": "Predicting Residential Energy Usage based on Weather",
    "section": "Model Building",
    "text": "Model Building\nNow it was finally time to build the predictive models! I used 5 different machine learning algorithms to do so: Linear Regression, K-Nearest Neighbor, Elastic Net Regression, Random Forest, and Gradient-Boosted Trees. All of the model types aside from the Linear Regression had specific hyperparameters that I could tune; hyperparameters are parameters external to the actual modeled data that control the learning process and performance of a model.\nThe general steps for building a model using tidymodels are:\n\nSpecify the model type, computational engine, model mode, and hyperparameters to tune (if applicable). Since my problem involves predicting a continuous outcome variable, I always set the mode to regression during this step.\nSet up a workflow by combining the model specifications from step 1 with the model recipe.\nCreate a tuning grid with a range of values for each hyperparameter of the model. Then train and evaluate versions of the model that use different combinations of the hyperparameters using the training data set. Since I have a lot of data and this step can take a long time to run, I saved the model tuning results as .Rds files to avoid re-running the model tuning.\nCompare the performance metric across all model versions and select the best one. Finalize the workflow with the best model and its specific hyperparameters.\nFit the final model to the testing dataset to evaluate its predictive performance on new data.\n\nSteps 1-3 were included in the code chunks below for each model type I explored. Step 4 was included in the next section, Model Results, and step 5 was included in the Best Model Results section below.\n\nLinear Regression\n\n\nCode\n# define model engine and mode\nlm_mod &lt;- parsnip::linear_reg() %&gt;% \n  parsnip::set_engine(\"lm\")\n\n# set up workflow\nlm_wkflow &lt;- workflows::workflow() %&gt;% \n  workflows::add_model(lm_mod) %&gt;% \n  workflows::add_recipe(smart_recipe)\n\n# fit single lm model across all folds of training set\nlm_res &lt;- tune::fit_resamples(lm_wkflow, resamples = smart_folds)\n\n# save en results\nsave(lm_res, file = \"data/inter_data/lm_res.rda\")\n\n\n\n\nK-Nearest Neighbor\n\n\nCode\n# define model engine and mode\nknn_mod &lt;- parsnip::nearest_neighbor(neighbors = tune()) %&gt;% \n  parsnip::set_engine(\"kknn\") %&gt;% \n  parsnip::set_mode(\"regression\")\n\n# set up workflow\nknn_wkflow &lt;- workflow() %&gt;% \n  workflows::add_model(knn_mod) %&gt;% \n  workflows::add_recipe(smart_recipe)\n\n# set up grid to tune neighbors\nknn_grid &lt;- grid_regular(neighbors(range = c(1, 10)), levels = 5)\n\n\n\n\nCode\n# tune neighbors for knn\nknn_res &lt;- tune::tune_grid(knn_wkflow, grid = knn_grid, resamples = smart_folds,\n                  control = control_grid(verbose = TRUE))\n\n# save en results\nsave(knn_res, file = \"data/inter_data/knn_res.rda\")\n\n\n\n\nElastic Net Regression\n\n\nCode\n# set up model\nen_mod &lt;- parsnip::linear_reg(penalty = tune(), mixture = tune()) %&gt;%\n  parsnip::set_engine(\"glmnet\") %&gt;%\n  parsnip::set_mode(\"regression\")\n\n# set up workflow\nen_wkflow &lt;- workflows::workflow() %&gt;%\n  workflows::add_model(en_mod) %&gt;%\n  workflows::add_recipe(smart_recipe)\n\n# create a regular grid for tuning penalty and mixture \nen_grid &lt;- dials::grid_regular(penalty(range = c(0.01,3), trans = identity_trans()), mixture(range = c(0, 1)), levels = 10)\n\n\n\n\nCode\n# tune hyperparameters for en\nen_res &lt;- tune::tune_grid(en_wkflow, grid = en_grid, resamples = smart_folds, \n                          control = control_grid(verbose = TRUE))\n\n# save en results\nsave(en_res, file = \"data/inter_data/en_res.rda\")\n\n\n\n\nRandom Forest\n\n\nCode\n# set up model\nrf_mod &lt;- parsnip::rand_forest(mtry = tune(), \n                               trees = tune(),\n                               min_n = tune()) %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;% \n  set_mode(\"regression\")\n\n# set up workflow\nrf_wkflow &lt;- workflow() %&gt;% \n  add_model(rf_mod) %&gt;% \n  add_recipe(smart_recipe)\n\n# create a regular grid for tuning mtry, trees, and min_n\nrf_grid &lt;- dials::grid_regular(mtry(range = c(1, 15)), # n predictors that will be randomly sampled at each split when creating tree models\n                               trees(range = c(200, 400)), # n of trees contained \n                               min_n(range = c(10, 30)) # min n of data points in a node required for the node to be split further\n                               )\n\n\n\n\nCode\n# tune hyperparameters for rf\nrf_res &lt;- tune::tune_grid(rf_wkflow, resamples = smart_folds, grid = rf_grid,\n                  control = control_grid(verbose = TRUE))\n\n# save rf results\nsave(rf_res, file = \"data/inter_data/rf_res.rda\")\n\n\n\n\nGradient-Boosted Tree\n\n\nCode\n# set up model\nbt_mod &lt;- boost_tree(mtry = tune(),\n                     trees = tune(), \n                     learn_rate = tune()) %&gt;%\n  set_engine(\"xgboost\") %&gt;% \n  set_mode(\"regression\")\n\n# set up workflow\nbt_wkflow &lt;- workflow() %&gt;% \n  add_model(bt_mod) %&gt;% \n  add_recipe(smart_recipe)\n\n# create a regular grid for tuning mtry, trees, and learning rate\nbt_grid &lt;- grid_regular(mtry(range = c(1, 15)), \n                        trees(range = c(200, 400)),\n                        learn_rate(range = c(-10, -1)),\n                        levels = 5)\n\n\n\n\nCode\n# tune hyperparameters for bt\nbt_res &lt;- tune_grid(bt_wkflow, resamples = smart_folds, grid = bt_grid, control = control_grid(verbose = TRUE))\n\n# save rf results\nsave(bt_res, file = \"data/inter_data/bt_res.rda\")"
  },
  {
    "objectID": "posts/2024-03-22-machine-learning-weather-energy/index.html#model-results",
    "href": "posts/2024-03-22-machine-learning-weather-energy/index.html#model-results",
    "title": "Predicting Residential Energy Usage based on Weather",
    "section": "Model Results",
    "text": "Model Results\nAfter all of the models were built, I compared their performance by evaluating the root mean squared error (RMSE) values of each. The RMSE (root mean squared error) measures the magnitude of error between predicted and actual values - lower RMSE values therefore reflect better model performance. Some of the autoplots also displayed the \\(R^2\\) values of the models, which explains the variance of the actual observed values where 1 is a perfect fit - higher \\(R^2\\) values therefore reflect better model performance.\nI use the autoplot function to view each model’s RMSE and the show_best function from the tune package to determine the best-performing models from those that I tuned.\n\nLinear Regression\nThere are no hyperparameters to tune in linear regression models, so I only developed one version of this model type. The mean RMSE across all 5 folds of the training data was 0.787 with a standard error of 0.00552.\n\n\nCode\n# load lm results\n#load(here::here(\"posts/2024-03-22-machine-learning-weather-energy/data/inter_data/lm_res.rda\"))\n\n# https://drive.google.com/file/d/1hcnd63aVF3jjjY6kEjwye84XoU4wrwt_/view?usp=sharing\n# define file id from google drive link\nfile_id &lt;- \"1hcnd63aVF3jjjY6kEjwye84XoU4wrwt_\"\n\n# download file and load into R\ndrive_download(as_id(file_id), overwrite = TRUE)\n\n\n! Using an auto-discovered, cached token.\n\n\n  To suppress this message, modify your code or options to clearly consent to\n  the use of a cached token.\n\n\n  See gargle's \"Non-interactive auth\" vignette for more details:\n\n\n  &lt;https://gargle.r-lib.org/articles/non-interactive-auth.html&gt;\n\n\nℹ The googledrive package is using a cached token for 'kristinart21@gmail.com'.\n\n\nFile downloaded:\n\n\n• 'lm_res.rda' &lt;id: 1hcnd63aVF3jjjY6kEjwye84XoU4wrwt_&gt;\n\n\nSaved locally as:\n\n\n• 'lm_res.rda'\n\n\nCode\n# load into R\nload(\"lm_res.rda\")\n\n# show best model\nlm_res %&gt;% tune::show_best(metric = \"rmse\")\n\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   0.787     5 0.00552 Preprocessor1_Model1\n\n\nCode\n# save best model results\nlm_best &lt;- lm_res %&gt;% tune::show_best(metric = \"rmse\") %&gt;% slice(1)\n\n\n\n\nK-Nearest Neighbor\nIn K-Nearest Neighbor models, we can tune the K hyperparameter, which specified the number of neighbors that should be considered when evaluating an observation’s expected value. I tuned my K-Nearest Model for 5 values of K between 1 to 10.\nBased on the autoplot, RMSE decreases and \\(R^2\\) increases as the value of K increases. The best performing model version had K = 10 with a mean RMSE across all 5 folds of 1.06 and a standard error of 0.00465.\n\n\nCode\n# load knn results\n#load(here::here(\"posts/2024-03-22-machine-learning-weather-energy/data/inter_data/knn_res.rda\"))\n\n# https://drive.google.com/file/d/1sTymXM0bfOHraxbZoZcTHd2a5sLTs_Ca/view?usp=drive_link\n# define file id from google drive link\nfile_id &lt;- \"1sTymXM0bfOHraxbZoZcTHd2a5sLTs_Ca\"\n\n# download file and load into R\ndrive_download(as_id(file_id), overwrite = TRUE)\n\n\nFile downloaded:\n\n\n• 'knn_res.rda' &lt;id: 1sTymXM0bfOHraxbZoZcTHd2a5sLTs_Ca&gt;\n\n\nSaved locally as:\n\n\n• 'knn_res.rda'\n\n\nCode\n# load into R\nload(\"knn_res.rda\")\n\n# plot\nknn_res %&gt;%\n  autoplot() +\n  theme_minimal() +\n  theme(text = element_text(family = \"lato\"))\n\n\n\n\n\n\n\n\n\nCode\n# show best models\nknn_res %&gt;% tune::show_best(metric = \"rmse\")\n\n\n# A tibble: 5 × 7\n  neighbors .metric .estimator  mean     n std_err .config             \n      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1        10 rmse    standard    1.06     5 0.00465 Preprocessor1_Model5\n2         7 rmse    standard    1.10     5 0.00456 Preprocessor1_Model4\n3         5 rmse    standard    1.13     5 0.00439 Preprocessor1_Model3\n4         3 rmse    standard    1.16     5 0.00424 Preprocessor1_Model2\n5         1 rmse    standard    1.19     5 0.00395 Preprocessor1_Model1\n\n\nCode\n# save best model results\nknn_best &lt;- knn_res %&gt;% tune::show_best(metric = \"rmse\") %&gt;% slice(1)\n\n\n\n\nElastic Net Regression\nIn Elastic Net Regression models, we can tune the penalty and mixture hyperparameters, which specify the …, respectively. I tuned my Elastic Net Regression models for 10 levels of penalty between 0.01 to 3 and 10 levels of mixture between 0 to 1. When mixture = 0, the model is actually performing a ridge regression and when mixture = 1, the model is performing a lasso regression.\nBased on the autoplot, RMSE decreases and \\(R^2\\) increases as the values of mixture, or the proportion of lasso penalty, and penalty, or the amount of regularization, both decrease. The best performing model version had mixture = 0.111, which means it is much closer to being a ridge regression. The best model also had penalty = 0.01, a mean RMSE across all folds of 0.788, and a standard error of 0.00552.\n\n\nCode\n# load en results\n#load(here::here(\"posts/2024-03-22-machine-learning-weather-energy/data/inter_data/en_res.rda\"))\n\n#https://drive.google.com/file/d/1SG3bzqmu669tUFdJleBxV8DIpbM34pxN/view?usp=drive_link\n# define file id from google drive link\nfile_id &lt;- \"1SG3bzqmu669tUFdJleBxV8DIpbM34pxN\"\n\n# download file and load into R\ndrive_download(as_id(file_id), overwrite = TRUE)\n\n\nFile downloaded:\n\n\n• 'en_res.rda' &lt;id: 1SG3bzqmu669tUFdJleBxV8DIpbM34pxN&gt;\n\n\nSaved locally as:\n\n\n• 'en_res.rda'\n\n\nCode\n# load into R\nload(\"en_res.rda\")\n\n# plot\nen_res %&gt;%\n  autoplot() +\n  theme_minimal() +\n  theme(text = element_text(family = \"lato\"))\n\n\n\n\n\n\n\n\n\nCode\n# show best models\nen_res %&gt;% tune::show_best(metric = \"rmse\")\n\n\n# A tibble: 5 × 8\n  penalty mixture .metric .estimator  mean     n std_err .config               \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n1    0.01   0.111 rmse    standard   0.788     5 0.00552 Preprocessor1_Model011\n2    0.01   0.222 rmse    standard   0.788     5 0.00552 Preprocessor1_Model021\n3    0.01   0.333 rmse    standard   0.789     5 0.00552 Preprocessor1_Model031\n4    0.01   0.444 rmse    standard   0.789     5 0.00552 Preprocessor1_Model041\n5    0.01   0     rmse    standard   0.789     5 0.00547 Preprocessor1_Model001\n\n\nCode\n# save best model results\nen_best &lt;- en_res %&gt;% tune::show_best(metric = \"rmse\") %&gt;% slice(1)\n\n\n\n\nRandom Forest\nIn random forest models, we can tune the mtry, trees, and min_n hyperparameters. mtry represents the number of parameters that can be randomly chosen from at each split of the tree. When mtry is less than 1, that means the tree will have no parameters to choose from. When mtry = 15, the tree has access to all of the predictors at each split, which is the same as bagging. I cannot use a mtry greater than 15 because my model recipe only includes 15 predictors, so I used 3 values between 1 to 15. trees represents the total number of trees to include in the forest ensemble. I used 3 values of trees between 200 to 400. min_n represents the minimum number of observations that need to be in a node in order for it to be split further. I used min_n values between 10 to 30 during the tuning process.**\nBased on the autoplot the number of trees does not make much of a difference in model performance, as all the colored lines are virtually on top of each other. The minimal node size did not appear to make much of a difference either, although it looks like min_n = 30 had a slightly lower RMSE than the lower values. The models that had access to 8 parameters at every split consistently performed the best. The best model had mtry = 8, trees = 300, and min_n = 30 with a mean RMSE across all folds of 0.760 and a standard error of 0.00563.\n\n\nCode\n# load rf results\n#load(here::here(\"posts/2024-03-22-machine-learning-weather-energy/data/inter_data/rf_res.rda\"))\n\n# https://drive.google.com/file/d/1D35V6XB1rZNdxx0WrcwS4uVeOSZKeeE6/view?usp=drive_link\n# define file id from google drive link\nfile_id &lt;- \"1D35V6XB1rZNdxx0WrcwS4uVeOSZKeeE6\"\n\n# download file and load into R\ndrive_download(as_id(file_id), overwrite = TRUE)\n\n\nFile downloaded:\n\n\n• 'rf_res.rda' &lt;id: 1D35V6XB1rZNdxx0WrcwS4uVeOSZKeeE6&gt;\n\n\nSaved locally as:\n\n\n• 'rf_res.rda'\n\n\nCode\n# load into R\nload(\"rf_res.rda\")\n\n# plot\nrf_res %&gt;% autoplot() + \n  theme_minimal() +\n  theme(text = element_text(family = \"lato\"))\n\n\n\n\n\n\n\n\n\nCode\n# show best rf models\nrf_res %&gt;% tune::show_best(metric = \"rmse\") \n\n\n# A tibble: 5 × 9\n   mtry trees min_n .metric .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1     8   300    30 rmse    standard   0.760     5 0.00563 Preprocessor1_Model23\n2     8   400    20 rmse    standard   0.760     5 0.00562 Preprocessor1_Model17\n3     8   400    30 rmse    standard   0.760     5 0.00562 Preprocessor1_Model26\n4     8   400    10 rmse    standard   0.760     5 0.00562 Preprocessor1_Model08\n5     8   200    30 rmse    standard   0.760     5 0.00562 Preprocessor1_Model20\n\n\nCode\n# save best model results\nrf_best &lt;- rf_res %&gt;% tune::show_best(metric = \"rmse\") %&gt;% slice(1)\n\n\n\n\nGradient-Boosted Tree\nIn Gradient-Boosted Decision Tree models, we can tune the mtry, trees, min_n, and learn_rate hyperparameters. The first three were described above in the Random Forest model results and the last one, learn_rate, represents how fast the boosted tree changes with each iteration. When learn_rate = 0, the tree doesn’t learn at all and at small values of learn_rate, the tree learns very slowly. I tuned my Gradient-Boosted Tree for 5 values of learn_rate between \\(10^{-10}\\) to \\(10^{-1}\\). Since this hyperparameter is the most impactful for gradient-boosted trees, I opted not to tune an alternative hyperparameter, min_n to reduce the computational power required. I did try out 5 values of mtry between 1 to 15 and 5 values of trees between 200 to 400 as well though.\nBased on the autoplot, models with larger learning rates had the best performance. The number of trees did not make as significant a difference and the number of parameters that were available at every split did not make as much of a difference in most models, although models with mtry &lt; 4 had slightly worse performance. The best performing model had mtry = 15, trees = 200, and learn_rate = 0.1 with a mean RMSE across all folds of 0.755 and a standard error of 0.00568.\n\n\nCode\n# load bt results\n#load(here::here(\"posts/2024-03-22-machine-learning-weather-energy/data/inter_data/bt_res.rda\"))\n\n# https://drive.google.com/file/d/1btCSIyH50_q2OVKrinDxviv1QEN_nk_o/view?usp=drive_link\n# define file id from google drive link\nfile_id &lt;- \"1btCSIyH50_q2OVKrinDxviv1QEN_nk_o\"\n\n# download file and load into R\ndrive_download(as_id(file_id), overwrite = TRUE)\n\n\nFile downloaded:\n\n\n• 'bt_res.rda' &lt;id: 1btCSIyH50_q2OVKrinDxviv1QEN_nk_o&gt;\n\n\nSaved locally as:\n\n\n• 'bt_res.rda'\n\n\nCode\n# load into R\nload(\"bt_res.rda\")\n\n# plot\nbt_res %&gt;% autoplot() + \n  theme_minimal() +\n  theme(text = element_text(family = \"lato\"))\n\n\n\n\n\n\n\n\n\nCode\n# show best rf models\nbt_res %&gt;% tune::show_best(metric = \"rmse\") \n\n\n# A tibble: 5 × 9\n   mtry trees learn_rate .metric .estimator  mean     n std_err .config         \n  &lt;int&gt; &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;           \n1    15   200        0.1 rmse    standard   0.755     5 0.00568 Preprocessor1_M…\n2    11   200        0.1 rmse    standard   0.755     5 0.00561 Preprocessor1_M…\n3    15   250        0.1 rmse    standard   0.755     5 0.00570 Preprocessor1_M…\n4    11   250        0.1 rmse    standard   0.755     5 0.00562 Preprocessor1_M…\n5     8   250        0.1 rmse    standard   0.755     5 0.00563 Preprocessor1_M…\n\n\nCode\n# save best model results\nbt_best &lt;- bt_res %&gt;% tune::show_best(metric = \"rmse\") %&gt;% slice(1)"
  },
  {
    "objectID": "posts/2024-03-22-machine-learning-weather-energy/index.html#best-model-results",
    "href": "posts/2024-03-22-machine-learning-weather-energy/index.html#best-model-results",
    "title": "Predicting Residential Energy Usage based on Weather",
    "section": "Best Model Results",
    "text": "Best Model Results\nOnce I combined the lowest RMSE from each model type, it was clear that a Gradient-Boosted Tree is the winner! It has a slightly lower RMSE than the runner-ups and predicted hourly_average_power_kw values that deviate from the actual observed values by approximately 0.755 kw on average. As mentioned above, the best performing model had mtry = 15, trees = 200, and learn_rate = 0.1 with a mean RMSE across all folds of 0.755 and a standard error of 0.00568.\n\n\nCode\n# combine best performance results from each model type\nall_rmse &lt;- tibble(Model = c(\"Linear Regression\", \"K-Nearest Neighbor\", \"Elastic Net Regression\", \"Random Forest\", \"Gradient-Boosted Trees\"), RMSE = c(lm_best$mean, knn_best$mean, en_best$mean, rf_best$mean, bt_best$mean)) %&gt;% \n  mutate(RMSE = round(RMSE, 3)) %&gt;% \n  arrange(RMSE)\n\nall_rmse %&gt;% \n  kable() %&gt;% \n  kable_styling(full_width = F, font = \"lato\") \n\n\n\n\n\nModel\nRMSE\n\n\n\n\nGradient-Boosted Trees\n0.755\n\n\nRandom Forest\n0.760\n\n\nLinear Regression\n0.787\n\n\nElastic Net Regression\n0.788\n\n\nK-Nearest Neighbor\n1.060\n\n\n\n\n\n\n\nNext I finalized the workflow using the hyperparameters from the best Gradient-Boosted Tree model and fit it to the entire training dataset.\n\n\nCode\n# save best model \nbest_mod &lt;- bt_res %&gt;% \n  tune::select_best(metric = \"rmse\", mtries, trees, learn_rate)\n\n# finalize workflow with best model\nfinal_wkflow &lt;- tune::finalize_workflow(bt_wkflow, best_mod)\n\n# fit model to training data\nfinal_fit_mod &lt;- parsnip::fit(final_wkflow, smart_train)\n\n\nOne cool thing about tree-based models is we can visualize which predictors were the most significant drivers of the outcome through a variable importance plot (VIP). Based on the VIP for the best model, the PC1 feature was the most important predictor variable by far; remember, the PC1 feature was extracted from the original temperature, apparent_temperature, and dew_point parameters, which were highly correlated (&gt; 90%) to each other. The time parameter was the second most important variable, followed by hour and months. This result makes sense but surprised me! I was expecting more weather-related parameters to show up in the top 10.\n\n\nCode\n# create variable importance plot using training data\nfinal_fit_mod %&gt;% \n  workflowsets::extract_fit_parsnip() %&gt;% \n  vip::vip(aesthetics = list(fill = \"#DAA49A\", color = \"#875053\")) +\n  theme_minimal() +\n  theme(text = element_text(family = \"lato\"))\n\n\n\n\n\n\n\n\n\nNext, it was finally time to introduce the best model to data its never seen before! I ran the model on the testing dataset to see how well it could predict values it was not trained on. The model’s RMSE on the testing dataset was 0.835, which is only slightly worse than the mean RMSE from the training process. This indicates that the training RMSE across 5-folds was a pretty good indicator of the model’s overall performance.\n\n\nCode\n# assess model performance on entire testing set\nfinal_mod_test &lt;- augment(final_fit_mod, smart_test) %&gt;% \n  rmse(truth = hourly_average_power_kw, estimate = .pred) %&gt;% \n  print()\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.835\n\n\nWhen I plotted the predicted values of power usage against the actual observed values, it was clear that the model does not predict high values well at all; in fact, the model did not predict any power usage values higher than 3.75 kW. This is due to that strong positive skew in the outcome variable, which means even our best model was only trained with a handful of observations for the higher power usage values. Only the values that fall directly on the diagonal line in the plot below were accurately predicted by the model.\n\n\nCode\n# plot predicted vs. actual values from testing data\naugment(final_fit_mod, smart_test) %&gt;% \n  ggplot(aes(x = .pred, y = hourly_average_power_kw)) +\n  geom_point(color = \"#DAA49A\", alpha = 0.2) +\n  geom_abline(lty = 2) +\n  coord_obs_pred() +\n  labs(title = \"Predicted vs. Actual Values of Hourly Average Power Usage (kW)\",\n       y = \"Actual Hourly Average Power Usage (kW)\",\n       y = \"Predicted Hourly Average Power Usage (kW)\") +\n  theme_minimal()+\n  theme(text = element_text(family = \"lato\"),\n        plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Update this with fake or untested data for blog post when have mroe time. \n# For fun, I used the model to predict residential power usage during the warmest and coolest observations in smart df. \n# filter to observations with highest and lowest temperature\nextreme_temps &lt;- smart_mod %&gt;%\n  filter(row_number() == which.max(temperature) | row_number() == which.min(temperature))\n\nfinal_predictions &lt;- augment(final_fit_mod, extreme_temps) %&gt;% \n  select(.pred, hourly_average_power_kw, temperature, everything())\n\nfinal_predictions"
  },
  {
    "objectID": "posts/2024-03-22-machine-learning-weather-energy/index.html#conclusion",
    "href": "posts/2024-03-22-machine-learning-weather-energy/index.html#conclusion",
    "title": "Predicting Residential Energy Usage based on Weather",
    "section": "Conclusion",
    "text": "Conclusion\nThe best model for predicting residential energy usage based on weather and time was a Gradient-Boosted Tree model. A Random Forest model did nearly as well, with only a small difference in RMSE separating the two. This is no surprise since decision tree-based models tend to work really well on many types of data. This is because tree-based models are highly flexible and non-parametric, meaning they do not assume any parametric constraints on the outcome variable. Interestingly, the Elastic Net Regression and Linear Regression models had a pretty similar performance to these top tree-based models, indicating that they also do a decent job of predicting residential energy usage; these model types could be more useful to a user who is willing to accept a little higher error in order to use models that are much less computationally expensive. The K-Nearest Neighbor model had the worst predictive power, which makes sense since KNN models tend to do poorly on data with too many predictors, or high dimensions.\nOverall, the best model did an okay job at predicting residential energy usage. The actual values of the outcome variable, hourly_average_power_kw, ranged between 0 to 11.5 kW. On average, the best model’s predicted values were about 0.835 kW off from the actual observed values based on the testing RMSE; this means the model’s average error was about 7% of the entire range. The model could be improved by adding in more data values that have high energy usage values. By normalizing the distribution of the outcome variable in this way, I could improve the model’s learning and potentially improve its performance.\nIt was interesting to see that the principal component of temperature, apparent_temperature, and dew_point is the most important predictor of residential energy usage. Since apparent temperature itself is also a factor of relative humidity and wind speed, this principal component feature may represent all the key weather predictors due to their interrelatedness. Therefore, it makes sense that time variables (hour, time, month) would be the next most important predictors in the VIP rather than some of the general weather predictors that remained (descriptive weather summary, wind-bearing, etc.). This makes intuitive sense and it was fun to see it explained quantitatively. I learned a lot through this project and am looking forward to exploring more energy data with machine learning algorithms!"
  },
  {
    "objectID": "posts/2024-03-22-machine-learning-weather-energy/index.html#footnotes",
    "href": "posts/2024-03-22-machine-learning-weather-energy/index.html#footnotes",
    "title": "Predicting Residential Energy Usage based on Weather",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThorve et al., 2023. https://www.nature.com/articles/s41597-022-01914-1#Tab1↩︎\nFikru and Gautier, 2015. https://www.sciencedirect.com/science/article/pii/S030626191500046X#ab005↩︎\nBarker, S. UMass Smart* Dataset - 2017 release. UMassTraceRepository https://traces.cs.umass.edu/index.php/smart/smart (2017).↩︎"
  }
]