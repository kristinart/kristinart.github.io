{
  "hash": "34c348e00162e9ae1fedb697a5ce54fb",
  "result": {
    "markdown": "---\ntitle: \"Predicting Residential Energy Usage based on Weather\"\ndescription: \"Machine learning models to predict energy use\"\nauthor: Kristin Art\ndate: 03-22/2024\ncategories: [R, Quarto, Git, Energy, Machine Learning] # self-defined categories\n#citation: \n#  url: https://samanthacsik.github.io/posts/2022-10-24-my-blog-post/ \nimage: preview.png\ndraft: false # setting this to `true` will prevent your post from appearing on your listing page until you're ready!\n#bibliography: references.bib\ncode-fold: true\ntoc-depth: 3\ntoc-title: Contents\ntoc-expand: true\nwebsite:\n  back-to-top-navigation: true\n---\n\n\n<meta http-equiv='cache-control' content='no-cache'>\n\n<meta http-equiv='expires' content='0'>\n\n<meta http-equiv='pragma' content='no-cache'>\n\n\n::: {.cell}\n<style type=\"text/css\">\n.panel-tabset .nav-item {\n  font-size: 18px;\n  font-style: bold\n  \n}\n\n\n</style>\n:::\n\n\n::: column-body-outset\n![Conceptual diagram of the relationship between weather and residential energy. Photograph sourced from https://vectormine.com/item/weather-impact-on-energy-consumption-and-household-usage-outline-concept/.](preview.png)\n:::\n\n# Predicting Residential Energy Usage based on Weather\n\n#### This blog post includes an analysis in which I build and compare machine learning models that predict residential energy usage from weather patterns. The full analysis is available in this [Github repository](https://github.com/kristinart/predicting-residential-energy-usage).\n\nClick on the tabs below to read the analysis!\n\n::: panel-tabset\n# Introduction\nWeather influences how much energy we use in our homes. The relationship between weather and energy consumption can therefore inform how to best plan and manage energy infrastructure to meet high energy demand events. [^1] Understanding this relationship at a fine temporal and spatial scale could allow us to mitigate the extent of weather-related energy strain and impact on people. This is especially relevant in today's day and age, as climate change alters regional temperatures and increases the frequency of extreme weather events. Improving the efficiency of energy consumption will work in tandem with grid modernization to meet climate and sustainability goals.\n\n[^1]: Thorve et al., 2023. https://www.nature.com/articles/s41597-022-01914-1#Tab1\n\nIn this project, I investigate how energy use within residential homes varies based on weather. Past research has shown that there is significant variation in residential energy use within a day. [^2] This intuitively makes sense, as most people turn their lights on at night and use more cooking appliances around mealtimes. It's also well-known that more energy is consumed during the winter months in cold regions due to the generation of heat for warmth. Here, I use high-resolution data from residential apartments to build models that predict hourly energy consumption based on weather patterns.\n\n[^2]: Fikru and Gautier, 2015. https://www.sciencedirect.com/science/article/pii/S030626191500046X#ab005\n\n### Research Question\n\nWhat weather patterns are good predictors of residential energy usage in Massachussets homes?\n\n### Data\n\nI used data downloaded from the [University of Massachusetts (UMASS) Trace Repository](https://traces.cs.umass.edu/index.php/smart/smart). [^3] In particular, I used the Apartment dataset from the 2017 release of the UMASS Smart\\* Dataset, which is a project that aims to optimize home energy consumption. The Apartment dataset contains minute-level electricity data and local weather conditions from 114 anonymous apartments in Massachussetts collected between 2014-2016.\n\n[^3]: Barker, S. UMass Smart\\* Dataset - 2017 release. UMassTraceRepository https://traces.cs.umass.edu/index.php/smart/smart (2017).\n\nThere are 114 individual data files with electricity data corresponding to individual apartments for each year, meaning there are a total of 342 files. There are also 3 data files containing weather data for each year.\n\n### Project Approach\n\nBefore I dive right into the project, here is a brief overview of my approach: \n\n- I began by loading, tidying, and exploring the data. During this phase, I aggregated the per-minute consumption data into hourly averages and randomly sampled a subset of the apartments to reduce the computational power required to perform the analysis. I also removed variables with a large amount of missing observations and performed a casewise deletion for smaller cases of missingness. \n\n- Then I visually explored the remaining variables to determine whether I needed to make any specific adjustments in my models - I did find that three predictors were highly correlated (\\> 90%), which informed my decision to reduce them into one principal component before running my models. \n\n- Next, I split my clean data into training and testing datasets that were stratified on the outcome variable, split the training dataset into 5-folds for cross validation, and specified my model recipe. \n\n- I then built, tuned, and compared the following model types: Linear Regression, K-Nearest Neighbor, Elastic Net Regression, Random Forest, and Gradient-Boosted Tree. \n\n- I evaluated the performance of all model iterations based on root mean squared error (RMSE), finalized the workflow for the best model, and fit it to the testing dataset to determine how well it could predict energy consumption based on new data. \n\nNow click on the tabs to get into the details!\n\n# EDA\n\n## Load and Tidy Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(janitor)\nlibrary(ggplot2)\nlibrary(kableExtra)\nlibrary(here)\nlibrary(lubridate)\nlibrary(purrr)\nlibrary(naniar)\nlibrary(corrplot)\nlibrary(showtext)\nlibrary(googledrive)\ntidymodels_prefer()\noptions(googledrive_quiet = TRUE) \n\n# set seed to reproduce results\nset.seed(2244)\n\n# load fonts\nfont_add_google(name = \"Lato\", family = \"lato\")\nshowtext_auto()\n```\n:::\n\n\nSince there were a large amount of individual files that needed to be loaded, aggregated, and cleaned, I began by defining two functions:\n\n1)  The first function, `tidy_weather_data`, loads and aggregates data from all files that contain weather information, converts the column headers to lower snake case, and adds columns for time parameters (datetime, date, year, month, and hour) using the `tidyverse`, `janitor`, and `lubridate` packages. I decided to separate out the year, month, and hour columns to be their own predictors because I believe energy usage may vary based on them. I also filtered the dataset to only contain dates from noon on October 14, 2014 onward, as that is the first datetime for which data exists in the apartments dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# define list of weather files\nweather_files <- list.files(here::here(\"data/apartment-weather\"), full.names = TRUE) \n\n# define function to process and clean weather data\ntidy_weather_data <- function(file_path) {\n\n  df <- read_csv(file_path) %>% \n    janitor::clean_names() %>% \n    mutate(datetime = lubridate::as_datetime(time, origin = \"1970-01-01\", tz = \"America/New_York\"),  # Convert unix timestamp to Eastern datetime \n           date = lubridate::date(datetime),\n           hour = lubridate::hour(datetime),\n           month = lubridate::month(datetime),\n           year = lubridate::year(datetime)) %>% \n    filter(datetime >= lubridate::ymd_hms(\"2014-10-15 12:00:00\")) # filter data to startdate of apartment data\n  \n  return(df)\n}\n\n# apply function over weather files \nweather_data <- purrr::map_dfr(weather_files, tidy_weather_data)\n\n# inspect result df\nhead(weather_data)\nsummary(weather_data)\n```\n:::\n\n\n2)  The second function, `tidy_apartment_data`, loads and aggregates data from all files that contain electricity data, converts the column headers to lower snake case, and adds columns for time parameters (datetime, date, year, month, and hour) using the `tidyverse`, `janitor`, and `lubridate` packages. I also added a new column to the dataframe containing the unique apartment identification numbers, which were included in the file names. Lastly, I summarized the raw minute-level data into hourly average power use in kiloWatts to reduce the computational power required.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# define list of apartment files\napt_files <- list.files(here::here(\"data/apartment\"), pattern = \".csv\", full.names = TRUE, recursive = TRUE) \n\n# define function to process and clean apartment data\ntidy_apartment_data <- function(file_path) {\n  df <- read_csv(file_path, col_names = c(\"datetime\", \"power_kw\"), col_types = cols(datetime = col_datetime(), power_kw = col_double())) %>%\n    janitor::clean_names() %>%\n    mutate(apt_id = as.numeric(stringr::str_extract(basename(file_path), \"Apt(\\\\d+)_\\\\d{4}\") %>% stringr::str_extract(\"(?<=Apt)\\\\d+\")),\n           datetime = lubridate::ymd_hms(datetime),\n           date = lubridate::date(datetime),\n           hour = lubridate::hour(datetime),\n           month = lubridate::month(datetime),\n           year = lubridate::year(datetime)) %>%\n    group_by(date, hour, apt_id) %>%\n    summarize(hourly_average_power_kw = as.numeric(mean(as.numeric(power_kw), na.rm = TRUE))) %>%\n    ungroup()\n  \n  return(df)\n\n}\n\n# apply function over all apartment files \napt_data <- purrr::map_dfr(apt_files, tidy_apartment_data )\n\n# inspect result df\nhead(apt_data)\nsummary(apt_data)\n```\n:::\n\n\nAfter loading, aggregating, and cleaning all of the data (good job to my computer), I combined the weather dataset with the apartments dataset by joining them based on the common date and hour columns. I also defined the month, year, and apartment IDs as factors to make them easier to plot later on. Lastly, I randomly sampled 50 out of the 114 apartments in an effort to decrease the computational power and run time required for my machine learning models.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# define random apartment ids to use for models\napt_sample <- sample(1:114, 50, replace = FALSE)\n\n# combine weather and apartment data\nsmart_df <- apt_data %>% \n  full_join(weather_data, by = c(\"date\", \"hour\"), relationship = \"many-to-many\") %>% \n  mutate_at(vars(hour,hourly_average_power_kw, temperature, humidity, visibility, apparent_temperature, pressure, wind_speed, cloud_cover, wind_bearing, precip_intensity, dew_point, precip_probability), as.numeric) %>% \n  mutate_at(vars(month, year, apt_id), as.factor) %>% \n  filter(apt_id %in% apt_sample)\n\n# save combined df\nsave(smart_df, file = \"data/inter_data/smart_df.csv\")\n```\n:::\n\n\n## Exploratory Data Analysis\n\nOnce all my data was in one dataframe, my first real step was to figure out how much of it was missing. Here I used the `vis_miss()` function from the `naniar` package to visualize any missing values.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# load combined df\nload(here::here(\"posts/2024-03-22-machine-learning-weather-energy/data/inter_data/smart_df.csv\"))\n\n# visualize missing data\nsmart_df %>%\n  naniar::vis_miss(warn_large_data = FALSE) +\n  theme(text = element_text(family = \"lato\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/visualize nas-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\nSurprisingly, the dataset was near-complete! Only 1.2% of it was missing (this might be a new record for me). Nearly all of the missing values were from the `cloud_cover` column. I wonder why this variable was missing so many observations in an otherwise comprehensive dataset - maybe cloud cover relied on manual human measurement while the other variables were automatically measured by instruments.\n\nSince the `cloud_cover` variable itself was missing 13% of observations and was not one of the most impactful predictor variables, I decided to drop the entire variable from the dataset. This way, I avoided losing 13% of the entire dataset like I would if I performed a complete case/ pairwise deletion.\n\nThe rest of the variables in the dataset were missing between 0-1% of their values. Since this is such a small proportion, I decided to performa complete case/ pairwise deletion across the entire dataset. If the proportion was higher, I would have imputed the missing values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# remove variables with missing data and non-useful data\nsmart_mod <- smart_df %>% \n  select(-cloud_cover) %>% \n  drop_na()\n```\n:::\n\n\nOnce all the missing values were taken care of, I took a peek at the data through descriptive and summary statistics.\n\nMy final dataframe had 20 variables and 953,578 observations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# explore data  \nsmart_mod %>% dim()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 953578     20\n```\n:::\n:::\n\n\nThe column names of my dataframe were:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsmart_mod %>% names()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"date\"                    \"hour\"                   \n [3] \"apt_id\"                  \"hourly_average_power_kw\"\n [5] \"temperature\"             \"icon\"                   \n [7] \"humidity\"                \"visibility\"             \n [9] \"summary\"                 \"apparent_temperature\"   \n[11] \"pressure\"                \"wind_speed\"             \n[13] \"time\"                    \"wind_bearing\"           \n[15] \"precip_intensity\"        \"dew_point\"              \n[17] \"precip_probability\"      \"datetime\"               \n[19] \"month\"                   \"year\"                   \n```\n:::\n:::\n\n\nIt looked like the data type for all the variables are appropriate. Most of the variables were numeric while the `summary` and `icon` variables were categorical. The `apartment ID`, `month`, and `year` were all factors because I defined them to be factors above. Lastly, the `datetime` and `date` columns are POSIXct and Date objects, respectively.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsmart_mod %>% str()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntibble [953,578 Ã— 20] (S3: tbl_df/tbl/data.frame)\n $ date                   : Date[1:953578], format: \"2014-10-15\" \"2014-10-15\" ...\n $ hour                   : num [1:953578] 12 13 14 15 16 17 18 19 20 21 ...\n $ apt_id                 : Factor w/ 114 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ hourly_average_power_kw: num [1:953578] 0 0 0 0 0 0 0 0 0 0 ...\n $ temperature            : num [1:953578] 74.2 75.3 75.9 75.9 74.7 ...\n $ icon                   : chr [1:953578] \"partly-cloudy-day\" \"partly-cloudy-day\" \"cloudy\" \"clear-day\" ...\n $ humidity               : num [1:953578] 0.73 0.71 0.68 0.67 0.7 0.72 0.76 0.79 0.81 0.83 ...\n $ visibility             : num [1:953578] 9.56 9.29 10 10 9.79 10 9.95 9.95 9.91 8.86 ...\n $ summary                : chr [1:953578] \"Partly Cloudy\" \"Partly Cloudy\" \"Overcast\" \"Clear\" ...\n $ apparent_temperature   : num [1:953578] 74.2 75.3 75.9 75.9 74.7 ...\n $ pressure               : num [1:953578] 1019 1018 1017 1017 1016 ...\n $ wind_speed             : num [1:953578] 10.07 10.19 10.45 11.05 9.19 ...\n $ time                   : num [1:953578] 1.41e+09 1.41e+09 1.41e+09 1.41e+09 1.41e+09 ...\n $ wind_bearing           : num [1:953578] 183 178 179 174 178 162 152 154 147 146 ...\n $ precip_intensity       : num [1:953578] 0.0023 0.0011 0.0022 0 0.0011 0 0 0 0 0.001 ...\n $ dew_point              : num [1:953578] 65 65.2 64.8 64.2 64.3 ...\n $ precip_probability     : num [1:953578] 0.06 0.01 0.06 0 0.01 0 0 0 0 0.01 ...\n $ datetime               : POSIXct[1:953578], format: \"2014-10-15 12:00:00\" \"2014-10-15 13:00:00\" ...\n $ month                  : Factor w/ 12 levels \"1\",\"2\",\"3\",\"4\",..: 10 10 10 10 10 10 10 10 10 10 ...\n $ year                   : Factor w/ 3 levels \"2014\",\"2015\",..: 1 1 1 1 1 1 1 1 1 1 ...\n```\n:::\n:::\n\n\nThe summary statistics for all the variables are shown in the table below:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsmart_mod %>% summary() %>% \n  kbl(caption = \"Summary Statistics for all Variables in the Smart* Dataset\") %>% \n  kable_styling(full_width = F, font = \"lato\") %>% \n  scroll_box(width = \"100%\", height = \"200px\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div style=\"border: 1px solid #ddd; padding: 0px; overflow-y: scroll; height:200px; overflow-x: scroll; width:100%; \"><table class=\"table\" style=\"font-size: latopx; width: auto !important; margin-left: auto; margin-right: auto;\">\n<caption style=\"font-size: initial !important;\">Summary Statistics for all Variables in the Smart* Dataset</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;position: sticky; top:0; background-color: #FFFFFF;\">   </th>\n   <th style=\"text-align:left;position: sticky; top:0; background-color: #FFFFFF;\">      date </th>\n   <th style=\"text-align:left;position: sticky; top:0; background-color: #FFFFFF;\">      hour </th>\n   <th style=\"text-align:left;position: sticky; top:0; background-color: #FFFFFF;\">     apt_id </th>\n   <th style=\"text-align:left;position: sticky; top:0; background-color: #FFFFFF;\"> hourly_average_power_kw </th>\n   <th style=\"text-align:left;position: sticky; top:0; background-color: #FFFFFF;\">  temperature </th>\n   <th style=\"text-align:left;position: sticky; top:0; background-color: #FFFFFF;\">     icon </th>\n   <th style=\"text-align:left;position: sticky; top:0; background-color: #FFFFFF;\">    humidity </th>\n   <th style=\"text-align:left;position: sticky; top:0; background-color: #FFFFFF;\">   visibility </th>\n   <th style=\"text-align:left;position: sticky; top:0; background-color: #FFFFFF;\">   summary </th>\n   <th style=\"text-align:left;position: sticky; top:0; background-color: #FFFFFF;\"> apparent_temperature </th>\n   <th style=\"text-align:left;position: sticky; top:0; background-color: #FFFFFF;\">    pressure </th>\n   <th style=\"text-align:left;position: sticky; top:0; background-color: #FFFFFF;\">   wind_speed </th>\n   <th style=\"text-align:left;position: sticky; top:0; background-color: #FFFFFF;\">      time </th>\n   <th style=\"text-align:left;position: sticky; top:0; background-color: #FFFFFF;\">  wind_bearing </th>\n   <th style=\"text-align:left;position: sticky; top:0; background-color: #FFFFFF;\"> precip_intensity </th>\n   <th style=\"text-align:left;position: sticky; top:0; background-color: #FFFFFF;\">   dew_point </th>\n   <th style=\"text-align:left;position: sticky; top:0; background-color: #FFFFFF;\"> precip_probability </th>\n   <th style=\"text-align:left;position: sticky; top:0; background-color: #FFFFFF;\">    datetime </th>\n   <th style=\"text-align:left;position: sticky; top:0; background-color: #FFFFFF;\">     month </th>\n   <th style=\"text-align:left;position: sticky; top:0; background-color: #FFFFFF;\">   year </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:left;\"> Min.   :2014-10-15 </td>\n   <td style=\"text-align:left;\"> Min.   : 0.0 </td>\n   <td style=\"text-align:left;\"> 92     : 19379 </td>\n   <td style=\"text-align:left;\"> Min.   : 0.0000 </td>\n   <td style=\"text-align:left;\"> Min.   :-13.05 </td>\n   <td style=\"text-align:left;\"> Length:953578 </td>\n   <td style=\"text-align:left;\"> Min.   :0.130 </td>\n   <td style=\"text-align:left;\"> Min.   : 0.290 </td>\n   <td style=\"text-align:left;\"> Length:953578 </td>\n   <td style=\"text-align:left;\"> Min.   :-32.99 </td>\n   <td style=\"text-align:left;\"> Min.   : 986.1 </td>\n   <td style=\"text-align:left;\"> Min.   : 0.020 </td>\n   <td style=\"text-align:left;\"> Min.   :1.413e+09 </td>\n   <td style=\"text-align:left;\"> Min.   :  0.0 </td>\n   <td style=\"text-align:left;\"> Min.   :0.000000 </td>\n   <td style=\"text-align:left;\"> Min.   :-27.69 </td>\n   <td style=\"text-align:left;\"> Min.   :0.00000 </td>\n   <td style=\"text-align:left;\"> Min.   :2014-10-15 08:00:00.00 </td>\n   <td style=\"text-align:left;\"> 11     :108150 </td>\n   <td style=\"text-align:left;\"> 2014: 93127 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:left;\"> 1st Qu.:2015-05-01 </td>\n   <td style=\"text-align:left;\"> 1st Qu.: 6.0 </td>\n   <td style=\"text-align:left;\"> 36     : 19068 </td>\n   <td style=\"text-align:left;\"> 1st Qu.: 0.2112 </td>\n   <td style=\"text-align:left;\"> 1st Qu.: 34.49 </td>\n   <td style=\"text-align:left;\"> Class :character </td>\n   <td style=\"text-align:left;\"> 1st Qu.:0.530 </td>\n   <td style=\"text-align:left;\"> 1st Qu.: 9.190 </td>\n   <td style=\"text-align:left;\"> Class :character </td>\n   <td style=\"text-align:left;\"> 1st Qu.: 29.49 </td>\n   <td style=\"text-align:left;\"> 1st Qu.:1012.0 </td>\n   <td style=\"text-align:left;\"> 1st Qu.: 3.630 </td>\n   <td style=\"text-align:left;\"> 1st Qu.:1.430e+09 </td>\n   <td style=\"text-align:left;\"> 1st Qu.:158.0 </td>\n   <td style=\"text-align:left;\"> 1st Qu.:0.000000 </td>\n   <td style=\"text-align:left;\"> 1st Qu.: 24.25 </td>\n   <td style=\"text-align:left;\"> 1st Qu.:0.00000 </td>\n   <td style=\"text-align:left;\"> 1st Qu.:2015-05-01 02:00:00.00 </td>\n   <td style=\"text-align:left;\"> 10     : 94277 </td>\n   <td style=\"text-align:left;\"> 2015:439100 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:left;\"> Median :2015-11-15 </td>\n   <td style=\"text-align:left;\"> Median :12.0 </td>\n   <td style=\"text-align:left;\"> 38     : 19068 </td>\n   <td style=\"text-align:left;\"> Median : 0.9015 </td>\n   <td style=\"text-align:left;\"> Median : 48.62 </td>\n   <td style=\"text-align:left;\"> Mode  :character </td>\n   <td style=\"text-align:left;\"> Median :0.700 </td>\n   <td style=\"text-align:left;\"> Median :10.000 </td>\n   <td style=\"text-align:left;\"> Mode  :character </td>\n   <td style=\"text-align:left;\"> Median : 46.43 </td>\n   <td style=\"text-align:left;\"> Median :1017.2 </td>\n   <td style=\"text-align:left;\"> Median : 5.860 </td>\n   <td style=\"text-align:left;\"> Median :1.448e+09 </td>\n   <td style=\"text-align:left;\"> Median :220.0 </td>\n   <td style=\"text-align:left;\"> Median :0.000000 </td>\n   <td style=\"text-align:left;\"> Median : 37.82 </td>\n   <td style=\"text-align:left;\"> Median :0.00000 </td>\n   <td style=\"text-align:left;\"> Median :2015-11-15 18:00:00.00 </td>\n   <td style=\"text-align:left;\"> 12     : 92501 </td>\n   <td style=\"text-align:left;\"> 2016:421351 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:left;\"> Mean   :2015-11-14 </td>\n   <td style=\"text-align:left;\"> Mean   :11.5 </td>\n   <td style=\"text-align:left;\"> 40     : 19068 </td>\n   <td style=\"text-align:left;\"> Mean   : 1.1568 </td>\n   <td style=\"text-align:left;\"> Mean   : 48.90 </td>\n   <td style=\"text-align:left;\"> NA </td>\n   <td style=\"text-align:left;\"> Mean   :0.676 </td>\n   <td style=\"text-align:left;\"> Mean   : 9.103 </td>\n   <td style=\"text-align:left;\"> NA </td>\n   <td style=\"text-align:left;\"> Mean   : 46.20 </td>\n   <td style=\"text-align:left;\"> Mean   :1017.2 </td>\n   <td style=\"text-align:left;\"> Mean   : 6.539 </td>\n   <td style=\"text-align:left;\"> Mean   :1.448e+09 </td>\n   <td style=\"text-align:left;\"> Mean   :207.4 </td>\n   <td style=\"text-align:left;\"> Mean   :0.003222 </td>\n   <td style=\"text-align:left;\"> Mean   : 37.48 </td>\n   <td style=\"text-align:left;\"> Mean   :0.06097 </td>\n   <td style=\"text-align:left;\"> Mean   :2015-11-15 09:39:56.80 </td>\n   <td style=\"text-align:left;\"> 1      : 76800 </td>\n   <td style=\"text-align:left;\"> NA </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:left;\"> 3rd Qu.:2016-05-31 </td>\n   <td style=\"text-align:left;\"> 3rd Qu.:17.0 </td>\n   <td style=\"text-align:left;\"> 53     : 19068 </td>\n   <td style=\"text-align:left;\"> 3rd Qu.: 1.7686 </td>\n   <td style=\"text-align:left;\"> 3rd Qu.: 64.76 </td>\n   <td style=\"text-align:left;\"> NA </td>\n   <td style=\"text-align:left;\"> 3rd Qu.:0.850 </td>\n   <td style=\"text-align:left;\"> 3rd Qu.:10.000 </td>\n   <td style=\"text-align:left;\"> NA </td>\n   <td style=\"text-align:left;\"> 3rd Qu.: 64.76 </td>\n   <td style=\"text-align:left;\"> 3rd Qu.:1022.3 </td>\n   <td style=\"text-align:left;\"> 3rd Qu.: 8.650 </td>\n   <td style=\"text-align:left;\"> 3rd Qu.:1.465e+09 </td>\n   <td style=\"text-align:left;\"> 3rd Qu.:294.0 </td>\n   <td style=\"text-align:left;\"> 3rd Qu.:0.000000 </td>\n   <td style=\"text-align:left;\"> 3rd Qu.: 54.31 </td>\n   <td style=\"text-align:left;\"> 3rd Qu.:0.00000 </td>\n   <td style=\"text-align:left;\"> 3rd Qu.:2016-05-31 12:00:00.00 </td>\n   <td style=\"text-align:left;\"> 5      : 74400 </td>\n   <td style=\"text-align:left;\"> NA </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:left;\"> Max.   :2016-12-28 </td>\n   <td style=\"text-align:left;\"> Max.   :23.0 </td>\n   <td style=\"text-align:left;\"> 109    : 19068 </td>\n   <td style=\"text-align:left;\"> Max.   :79.1174 </td>\n   <td style=\"text-align:left;\"> Max.   : 93.78 </td>\n   <td style=\"text-align:left;\"> NA </td>\n   <td style=\"text-align:left;\"> Max.   :0.980 </td>\n   <td style=\"text-align:left;\"> Max.   :10.000 </td>\n   <td style=\"text-align:left;\"> NA </td>\n   <td style=\"text-align:left;\"> Max.   : 98.45 </td>\n   <td style=\"text-align:left;\"> Max.   :1044.5 </td>\n   <td style=\"text-align:left;\"> Max.   :24.940 </td>\n   <td style=\"text-align:left;\"> Max.   :1.483e+09 </td>\n   <td style=\"text-align:left;\"> Max.   :359.0 </td>\n   <td style=\"text-align:left;\"> Max.   :0.426900 </td>\n   <td style=\"text-align:left;\"> Max.   : 75.29 </td>\n   <td style=\"text-align:left;\"> Max.   :0.90000 </td>\n   <td style=\"text-align:left;\"> Max.   :2016-12-28 22:00:00.00 </td>\n   <td style=\"text-align:left;\"> 7      : 74400 </td>\n   <td style=\"text-align:left;\"> NA </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:left;\"> NA </td>\n   <td style=\"text-align:left;\"> NA </td>\n   <td style=\"text-align:left;\"> (Other):838859 </td>\n   <td style=\"text-align:left;\"> NA </td>\n   <td style=\"text-align:left;\"> NA </td>\n   <td style=\"text-align:left;\"> NA </td>\n   <td style=\"text-align:left;\"> NA </td>\n   <td style=\"text-align:left;\"> NA </td>\n   <td style=\"text-align:left;\"> NA </td>\n   <td style=\"text-align:left;\"> NA </td>\n   <td style=\"text-align:left;\"> NA </td>\n   <td style=\"text-align:left;\"> NA </td>\n   <td style=\"text-align:left;\"> NA </td>\n   <td style=\"text-align:left;\"> NA </td>\n   <td style=\"text-align:left;\"> NA </td>\n   <td style=\"text-align:left;\"> NA </td>\n   <td style=\"text-align:left;\"> NA </td>\n   <td style=\"text-align:left;\"> NA </td>\n   <td style=\"text-align:left;\"> (Other):433050 </td>\n   <td style=\"text-align:left;\"> NA </td>\n  </tr>\n</tbody>\n</table></div>\n\n`````\n:::\n:::\n\n\nLastly, here's a look at the first few rows of the data in case you want to get a feel for it:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsmart_mod %>% head() %>% \n  kbl(caption = \"First 6 Rows of the Smart* Dataset\") %>% \n  kable_styling(full_width = F, font = \"lato\") %>% \n  scroll_box(width = \"100%\", height = \"200px\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div style=\"border: 1px solid #ddd; padding: 0px; overflow-y: scroll; height:200px; overflow-x: scroll; width:100%; \"><table class=\"table\" style=\"font-size: latopx; width: auto !important; margin-left: auto; margin-right: auto;\">\n<caption style=\"font-size: initial !important;\">First 6 Rows of the Smart* Dataset</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;position: sticky; top:0; background-color: #FFFFFF;\"> date </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> hour </th>\n   <th style=\"text-align:left;position: sticky; top:0; background-color: #FFFFFF;\"> apt_id </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> hourly_average_power_kw </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> temperature </th>\n   <th style=\"text-align:left;position: sticky; top:0; background-color: #FFFFFF;\"> icon </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> humidity </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> visibility </th>\n   <th style=\"text-align:left;position: sticky; top:0; background-color: #FFFFFF;\"> summary </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> apparent_temperature </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> pressure </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> wind_speed </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> time </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> wind_bearing </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> precip_intensity </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> dew_point </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> precip_probability </th>\n   <th style=\"text-align:left;position: sticky; top:0; background-color: #FFFFFF;\"> datetime </th>\n   <th style=\"text-align:left;position: sticky; top:0; background-color: #FFFFFF;\"> month </th>\n   <th style=\"text-align:left;position: sticky; top:0; background-color: #FFFFFF;\"> year </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> 2014-10-15 </td>\n   <td style=\"text-align:right;\"> 12 </td>\n   <td style=\"text-align:left;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 74.20 </td>\n   <td style=\"text-align:left;\"> partly-cloudy-day </td>\n   <td style=\"text-align:right;\"> 0.73 </td>\n   <td style=\"text-align:right;\"> 9.56 </td>\n   <td style=\"text-align:left;\"> Partly Cloudy </td>\n   <td style=\"text-align:right;\"> 74.20 </td>\n   <td style=\"text-align:right;\"> 1018.98 </td>\n   <td style=\"text-align:right;\"> 10.07 </td>\n   <td style=\"text-align:right;\"> 1413388800 </td>\n   <td style=\"text-align:right;\"> 183 </td>\n   <td style=\"text-align:right;\"> 0.0023 </td>\n   <td style=\"text-align:right;\"> 65.00 </td>\n   <td style=\"text-align:right;\"> 0.06 </td>\n   <td style=\"text-align:left;\"> 2014-10-15 12:00:00 </td>\n   <td style=\"text-align:left;\"> 10 </td>\n   <td style=\"text-align:left;\"> 2014 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 2014-10-15 </td>\n   <td style=\"text-align:right;\"> 13 </td>\n   <td style=\"text-align:left;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 75.32 </td>\n   <td style=\"text-align:left;\"> partly-cloudy-day </td>\n   <td style=\"text-align:right;\"> 0.71 </td>\n   <td style=\"text-align:right;\"> 9.29 </td>\n   <td style=\"text-align:left;\"> Partly Cloudy </td>\n   <td style=\"text-align:right;\"> 75.32 </td>\n   <td style=\"text-align:right;\"> 1017.95 </td>\n   <td style=\"text-align:right;\"> 10.19 </td>\n   <td style=\"text-align:right;\"> 1413392400 </td>\n   <td style=\"text-align:right;\"> 178 </td>\n   <td style=\"text-align:right;\"> 0.0011 </td>\n   <td style=\"text-align:right;\"> 65.21 </td>\n   <td style=\"text-align:right;\"> 0.01 </td>\n   <td style=\"text-align:left;\"> 2014-10-15 13:00:00 </td>\n   <td style=\"text-align:left;\"> 10 </td>\n   <td style=\"text-align:left;\"> 2014 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 2014-10-15 </td>\n   <td style=\"text-align:right;\"> 14 </td>\n   <td style=\"text-align:left;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 75.91 </td>\n   <td style=\"text-align:left;\"> cloudy </td>\n   <td style=\"text-align:right;\"> 0.68 </td>\n   <td style=\"text-align:right;\"> 10.00 </td>\n   <td style=\"text-align:left;\"> Overcast </td>\n   <td style=\"text-align:right;\"> 75.91 </td>\n   <td style=\"text-align:right;\"> 1017.11 </td>\n   <td style=\"text-align:right;\"> 10.45 </td>\n   <td style=\"text-align:right;\"> 1413396000 </td>\n   <td style=\"text-align:right;\"> 179 </td>\n   <td style=\"text-align:right;\"> 0.0022 </td>\n   <td style=\"text-align:right;\"> 64.76 </td>\n   <td style=\"text-align:right;\"> 0.06 </td>\n   <td style=\"text-align:left;\"> 2014-10-15 14:00:00 </td>\n   <td style=\"text-align:left;\"> 10 </td>\n   <td style=\"text-align:left;\"> 2014 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 2014-10-15 </td>\n   <td style=\"text-align:right;\"> 15 </td>\n   <td style=\"text-align:left;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 75.86 </td>\n   <td style=\"text-align:left;\"> clear-day </td>\n   <td style=\"text-align:right;\"> 0.67 </td>\n   <td style=\"text-align:right;\"> 10.00 </td>\n   <td style=\"text-align:left;\"> Clear </td>\n   <td style=\"text-align:right;\"> 75.86 </td>\n   <td style=\"text-align:right;\"> 1016.71 </td>\n   <td style=\"text-align:right;\"> 11.05 </td>\n   <td style=\"text-align:right;\"> 1413399600 </td>\n   <td style=\"text-align:right;\"> 174 </td>\n   <td style=\"text-align:right;\"> 0.0000 </td>\n   <td style=\"text-align:right;\"> 64.21 </td>\n   <td style=\"text-align:right;\"> 0.00 </td>\n   <td style=\"text-align:left;\"> 2014-10-15 15:00:00 </td>\n   <td style=\"text-align:left;\"> 10 </td>\n   <td style=\"text-align:left;\"> 2014 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 2014-10-15 </td>\n   <td style=\"text-align:right;\"> 16 </td>\n   <td style=\"text-align:left;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 74.74 </td>\n   <td style=\"text-align:left;\"> clear-day </td>\n   <td style=\"text-align:right;\"> 0.70 </td>\n   <td style=\"text-align:right;\"> 9.79 </td>\n   <td style=\"text-align:left;\"> Clear </td>\n   <td style=\"text-align:right;\"> 74.74 </td>\n   <td style=\"text-align:right;\"> 1016.49 </td>\n   <td style=\"text-align:right;\"> 9.19 </td>\n   <td style=\"text-align:right;\"> 1413403200 </td>\n   <td style=\"text-align:right;\"> 178 </td>\n   <td style=\"text-align:right;\"> 0.0011 </td>\n   <td style=\"text-align:right;\"> 64.34 </td>\n   <td style=\"text-align:right;\"> 0.01 </td>\n   <td style=\"text-align:left;\"> 2014-10-15 16:00:00 </td>\n   <td style=\"text-align:left;\"> 10 </td>\n   <td style=\"text-align:left;\"> 2014 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 2014-10-15 </td>\n   <td style=\"text-align:right;\"> 17 </td>\n   <td style=\"text-align:left;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 74.14 </td>\n   <td style=\"text-align:left;\"> partly-cloudy-day </td>\n   <td style=\"text-align:right;\"> 0.72 </td>\n   <td style=\"text-align:right;\"> 10.00 </td>\n   <td style=\"text-align:left;\"> Partly Cloudy </td>\n   <td style=\"text-align:right;\"> 74.14 </td>\n   <td style=\"text-align:right;\"> 1016.22 </td>\n   <td style=\"text-align:right;\"> 9.86 </td>\n   <td style=\"text-align:right;\"> 1413406800 </td>\n   <td style=\"text-align:right;\"> 162 </td>\n   <td style=\"text-align:right;\"> 0.0000 </td>\n   <td style=\"text-align:right;\"> 64.48 </td>\n   <td style=\"text-align:right;\"> 0.00 </td>\n   <td style=\"text-align:left;\"> 2014-10-15 17:00:00 </td>\n   <td style=\"text-align:left;\"> 10 </td>\n   <td style=\"text-align:left;\"> 2014 </td>\n  </tr>\n</tbody>\n</table></div>\n\n`````\n:::\n:::\n\n\n## Visual Exploratory Data Analysis\n\nNext, I began my favorite type of exploratory analysis - visualization!\n\n### Correlation Plot\n\nFirst off, I explored how correlated all of the numeric variables were by using the `corrplot()` function from the `corrplot` package to visualize a correlation matrix. It showed me that `temperature`, `apparent_temperature`, and `dew point` were highly positively correlated, which makes sense since they are physically related; since they were so highly correlated (\\> 90%), I decide to use a principal components analysis (PCA) in my model recipe below to collapse them into 1 feature instead of 3. Interestingly, these three variables were negatively correlated with `hourly_average_power_kw`, which is the outcome variable of interest. Another interesting finding is that `visibility` was negatively correlated to `humidity`, `precipitation intensity`, and `precipitation probability` - this makes sense since it is hard to see far while it's raining. The other correlations were also logical since weather variables are typically all related.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# correlation plot of all variables\nsmart_mod %>%\n  select(where(is.numeric)) %>%\n  cor() %>%\n  corrplot(method = 'number', order = 'FPC', type = 'lower', family = \"lato\", number.cex=0.6, bg = \"grey80\") \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/correlation plot-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n### Power Usage Distribution\n\nNext, I explored the distribution of `hourly_average_power_kw`, which is the outcome variable of interest. The outcome variable was highly positively skewed, as the vast majority of observations (order of $10^5$ - $10^6$) for each bin were between 0 - 10 kiloWatts. There were only a handful of observations (order of $10^1$) for each bin between 15 - 232 kiloWatts.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# histogram of energy usage\nggplot(data = smart_mod, aes(x = hourly_average_power_kw))+\n  geom_histogram(fill = \"#DAA49A\", col = \"#875053\", bins = 150)+\n  labs(x = \"Power Usage (kW)\", y = \"Count\")+\n  theme_minimal()+\n  theme(text = element_text(family = \"lato\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/power histogram-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n### Power Usage by Month\n\nI visualized the relationship between power usage and month by making a box-and-whisket plot. As expected, power usage is lowest during the warm months (June - September) and highest during the cold months (November - February); this makes sense since most people crank up the heat in the winter months to stay warm. Interestingly, there were quite a few outliers for all of the months, which could mean some apartments use more energy in general.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# boxplot of energy usage against month\nggplot(data = smart_mod, aes(x = factor(month, labels = month.name), y = hourly_average_power_kw, group = month))+\n  geom_boxplot(fill = \"#DAA49A\", col = \"#875053\")+\n  scale_y_continuous(limits = c(0,12))+\n  #geom_jitter(alpha = 0.4, col = \"#DAA49A\")+\n  labs(x = \"Month\", y = \"Power (kW)\")+\n  theme_minimal()+\n  theme(text = element_text(family = \"lato\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/monthly boxplot-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n### Temperature Distribution\n\nNext, I explored the distribution of `temperature`, which I expect would have a significant impact on energy use. Temprature had a normal distribution with a slight left tail, indicating a small negative skew.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# histogram of temperature\nggplot(data = smart_mod, aes(x = temperature))+\n  # geom_histogram(aes(y = ..density..), bins = 50, fill = \"#DAA49A\", col = \"#875053\")+\n  # geom_density(linewidth = 1.5)+\n  geom_histogram(bins = 50, fill = \"#DAA49A\", col = \"#875053\")+\n  labs(x = \"Temperature (deg F)\", y = \"Count\")+\n  theme_minimal()+\n  theme(text = element_text(family = \"lato\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/temp histogram-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n# Model Set-Up\n\nNext, I prepared my data for the machine learning models. Here, I randomly split the data into training and testing datasets, split the training dataset into 5 folds for k-fold cross validation, and specified a model recipe.\n\n### Split Training and Testing Data\n\nFirst off, I split the full dataset into training and testing datasets. Like the names imply, the training dataset will be used to train the models while the testing dataset will be used to test the predictive power of the models at the very end. I split the data using the `initial_split()` function from the `rsample` package. The split is stratified on the outcome variable, `hourly_average_power_kw`, to ensure that both the training and the testing datasets have roughly the same distribution of `hourly_average_power_kw`. I split the full dataset so that 3/4 of it becomes the training dataset and the remaining 1/4 becomes the testing dataset. This was to ensure there is a good amount of data for training while still retaining enough for substantial testing.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# split data\nsmart_split <- rsample::initial_split(smart_mod, \n                                        prop = 0.75,\n                                        strata = hourly_average_power_kw)\n\n# assign splits to train and test objects\nsmart_train <- rsample::training(smart_split)\nsmart_test <- rsample::testing(smart_split)\n```\n:::\n\n\n### K-Fold Cross Validation\n\nI also performed a k-fold cross validation on my entire training set with k = 5. This splits the entire training set into 5 folds that each consist of a mini-training, or analysis set, and a mini-testing, or assessment set. Each of my models will be trained on the analysis sets and tested on the assessment sets of each fold and the mean performance metric across all folds will be reported. I used the `vfold` function from `rsample` to split the training dataset into 5 folds and stratified on the outcome variable once again to ensure that each subset of the data has the same distribution of `hourly_average_power_kw`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# split data for k-fold CV\nsmart_folds <- rsample::vfold_cv(smart_train, v = 5, strata = hourly_average_power_kw)\n```\n:::\n\n\n### Build Recipe\n\nThen I specified a recipe for the models to use. I used the `recipes` package to do things like define the model recipe, dummy code the categorical variables, center all predictors, scale all predictors, and reduce the dimensions of those highly correlated predictors I noticed during the EDA (`temperature`, `apparent_temperature`, and `dew_point`).\n\nMy recipe tells my models to predict `hourly_average_power_kw` as a function of `temperature` + `humidity` + `visibility` + `summary` + `apparent_temperature` + `pressure` + `wind_speed` + `wind_bearing` + `precip_intensity` + `dew_point` + `precip_probability` + `year` + `month` + `hour` + `time`. The results from the pre-processing steps I specified are shown below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# define recipe\nsmart_recipe <- recipes::recipe(hourly_average_power_kw ~ temperature + humidity + visibility + summary + apparent_temperature + pressure + wind_speed + wind_bearing + precip_intensity + dew_point + precip_probability + year + month + hour + time, data = smart_train) %>% \n  recipes::step_dummy(all_nominal_predictors()) %>% # dummy code categorical variables\n  recipes::step_normalize(all_numeric_predictors(), -all_nominal_predictors()) %>% # center and scale numeric predictors only\n  recipes::step_pca(c(\"temperature\", \"apparent_temperature\", \"dew_point\"), num_comp = 1) # convert highly correlated variables (>90) into 1 principal component\n\n#apply/view recipe\nsmart_recipe %>% \n  recipes::prep() %>% \n  recipes::bake(new_data = smart_train) %>% \n  head() %>% \n  kable() %>% \n  kable_styling(full_width = F, font = \"lato\") %>% \n  scroll_box(width = \"100%\", height = \"200px\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div style=\"border: 1px solid #ddd; padding: 0px; overflow-y: scroll; height:200px; overflow-x: scroll; width:100%; \"><table class=\"table\" style=\"font-size: latopx; width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> humidity </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> visibility </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> pressure </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> wind_speed </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> wind_bearing </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> precip_intensity </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> precip_probability </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> hour </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> time </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> hourly_average_power_kw </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> summary_Breezy.and.Foggy </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> summary_Breezy.and.Mostly.Cloudy </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> summary_Breezy.and.Overcast </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> summary_Breezy.and.Partly.Cloudy </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> summary_Clear </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> summary_Drizzle </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> summary_Drizzle.and.Breezy </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> summary_Dry </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> summary_Flurries </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> summary_Foggy </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> summary_Heavy.Rain </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> summary_Heavy.Snow </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> summary_Humid.and.Overcast </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> summary_Humid.and.Partly.Cloudy </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> summary_Light.Rain </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> summary_Light.Rain.and.Breezy </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> summary_Light.Snow </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> summary_Mostly.Cloudy </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> summary_Overcast </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> summary_Partly.Cloudy </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> summary_Rain </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> summary_Rain.and.Breezy </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> summary_Snow </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> year_X2015 </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> year_X2016 </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> month_X2 </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> month_X3 </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> month_X4 </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> month_X5 </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> month_X6 </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> month_X7 </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> month_X8 </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> month_X9 </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> month_X10 </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> month_X11 </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> month_X12 </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> PC1 </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 0.2820793 </td>\n   <td style=\"text-align:right;\"> 0.2571194 </td>\n   <td style=\"text-align:right;\"> 0.2295259 </td>\n   <td style=\"text-align:right;\"> 0.9074681 </td>\n   <td style=\"text-align:right;\"> -0.2347835 </td>\n   <td style=\"text-align:right;\"> -0.0560576 </td>\n   <td style=\"text-align:right;\"> -0.0067788 </td>\n   <td style=\"text-align:right;\"> 0.0723027 </td>\n   <td style=\"text-align:right;\"> -1.727829 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> -0.0101725 </td>\n   <td style=\"text-align:right;\"> -0.007095 </td>\n   <td style=\"text-align:right;\"> -0.007095 </td>\n   <td style=\"text-align:right;\"> -0.0378107 </td>\n   <td style=\"text-align:right;\"> -1.727704 </td>\n   <td style=\"text-align:right;\"> -0.1460816 </td>\n   <td style=\"text-align:right;\"> -0.0073847 </td>\n   <td style=\"text-align:right;\"> -0.0073847 </td>\n   <td style=\"text-align:right;\"> -0.0608575 </td>\n   <td style=\"text-align:right;\"> -0.0465443 </td>\n   <td style=\"text-align:right;\"> -0.029337 </td>\n   <td style=\"text-align:right;\"> -0.0203483 </td>\n   <td style=\"text-align:right;\"> -0.0074788 </td>\n   <td style=\"text-align:right;\"> -0.0068951 </td>\n   <td style=\"text-align:right;\"> -0.2423889 </td>\n   <td style=\"text-align:right;\"> -0.0126264 </td>\n   <td style=\"text-align:right;\"> -0.1116211 </td>\n   <td style=\"text-align:right;\"> -0.0788855 </td>\n   <td style=\"text-align:right;\"> -0.1086493 </td>\n   <td style=\"text-align:right;\"> 2.7684555 </td>\n   <td style=\"text-align:right;\"> -0.1128537 </td>\n   <td style=\"text-align:right;\"> -0.0123463 </td>\n   <td style=\"text-align:right;\"> -0.0618335 </td>\n   <td style=\"text-align:right;\"> -0.9242423 </td>\n   <td style=\"text-align:right;\"> -0.8889373 </td>\n   <td style=\"text-align:right;\"> -0.278143 </td>\n   <td style=\"text-align:right;\"> -0.2909624 </td>\n   <td style=\"text-align:right;\"> -0.2860174 </td>\n   <td style=\"text-align:right;\"> -0.2909087 </td>\n   <td style=\"text-align:right;\"> -0.2860689 </td>\n   <td style=\"text-align:right;\"> -0.2906768 </td>\n   <td style=\"text-align:right;\"> -0.2912252 </td>\n   <td style=\"text-align:right;\"> -0.2853847 </td>\n   <td style=\"text-align:right;\"> 3.018419 </td>\n   <td style=\"text-align:right;\"> -0.3573269 </td>\n   <td style=\"text-align:right;\"> -0.3275356 </td>\n   <td style=\"text-align:right;\"> -2.267436 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 0.1779755 </td>\n   <td style=\"text-align:right;\"> 0.1048700 </td>\n   <td style=\"text-align:right;\"> 0.0987204 </td>\n   <td style=\"text-align:right;\"> 0.9383314 </td>\n   <td style=\"text-align:right;\"> -0.2826580 </td>\n   <td style=\"text-align:right;\"> -0.1275158 </td>\n   <td style=\"text-align:right;\"> -0.2912672 </td>\n   <td style=\"text-align:right;\"> 0.2167757 </td>\n   <td style=\"text-align:right;\"> -1.727647 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> -0.0101725 </td>\n   <td style=\"text-align:right;\"> -0.007095 </td>\n   <td style=\"text-align:right;\"> -0.007095 </td>\n   <td style=\"text-align:right;\"> -0.0378107 </td>\n   <td style=\"text-align:right;\"> -1.727704 </td>\n   <td style=\"text-align:right;\"> -0.1460816 </td>\n   <td style=\"text-align:right;\"> -0.0073847 </td>\n   <td style=\"text-align:right;\"> -0.0073847 </td>\n   <td style=\"text-align:right;\"> -0.0608575 </td>\n   <td style=\"text-align:right;\"> -0.0465443 </td>\n   <td style=\"text-align:right;\"> -0.029337 </td>\n   <td style=\"text-align:right;\"> -0.0203483 </td>\n   <td style=\"text-align:right;\"> -0.0074788 </td>\n   <td style=\"text-align:right;\"> -0.0068951 </td>\n   <td style=\"text-align:right;\"> -0.2423889 </td>\n   <td style=\"text-align:right;\"> -0.0126264 </td>\n   <td style=\"text-align:right;\"> -0.1116211 </td>\n   <td style=\"text-align:right;\"> -0.0788855 </td>\n   <td style=\"text-align:right;\"> -0.1086493 </td>\n   <td style=\"text-align:right;\"> 2.7684555 </td>\n   <td style=\"text-align:right;\"> -0.1128537 </td>\n   <td style=\"text-align:right;\"> -0.0123463 </td>\n   <td style=\"text-align:right;\"> -0.0618335 </td>\n   <td style=\"text-align:right;\"> -0.9242423 </td>\n   <td style=\"text-align:right;\"> -0.8889373 </td>\n   <td style=\"text-align:right;\"> -0.278143 </td>\n   <td style=\"text-align:right;\"> -0.2909624 </td>\n   <td style=\"text-align:right;\"> -0.2860174 </td>\n   <td style=\"text-align:right;\"> -0.2909087 </td>\n   <td style=\"text-align:right;\"> -0.2860689 </td>\n   <td style=\"text-align:right;\"> -0.2906768 </td>\n   <td style=\"text-align:right;\"> -0.2912252 </td>\n   <td style=\"text-align:right;\"> -0.2853847 </td>\n   <td style=\"text-align:right;\"> 3.018419 </td>\n   <td style=\"text-align:right;\"> -0.3573269 </td>\n   <td style=\"text-align:right;\"> -0.3275356 </td>\n   <td style=\"text-align:right;\"> -2.335619 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 0.0218198 </td>\n   <td style=\"text-align:right;\"> 0.5052295 </td>\n   <td style=\"text-align:right;\"> -0.0079559 </td>\n   <td style=\"text-align:right;\"> 1.0052019 </td>\n   <td style=\"text-align:right;\"> -0.2730831 </td>\n   <td style=\"text-align:right;\"> -0.0620124 </td>\n   <td style=\"text-align:right;\"> -0.0067788 </td>\n   <td style=\"text-align:right;\"> 0.3612487 </td>\n   <td style=\"text-align:right;\"> -1.727465 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> -0.0101725 </td>\n   <td style=\"text-align:right;\"> -0.007095 </td>\n   <td style=\"text-align:right;\"> -0.007095 </td>\n   <td style=\"text-align:right;\"> -0.0378107 </td>\n   <td style=\"text-align:right;\"> -1.727704 </td>\n   <td style=\"text-align:right;\"> -0.1460816 </td>\n   <td style=\"text-align:right;\"> -0.0073847 </td>\n   <td style=\"text-align:right;\"> -0.0073847 </td>\n   <td style=\"text-align:right;\"> -0.0608575 </td>\n   <td style=\"text-align:right;\"> -0.0465443 </td>\n   <td style=\"text-align:right;\"> -0.029337 </td>\n   <td style=\"text-align:right;\"> -0.0203483 </td>\n   <td style=\"text-align:right;\"> -0.0074788 </td>\n   <td style=\"text-align:right;\"> -0.0068951 </td>\n   <td style=\"text-align:right;\"> -0.2423889 </td>\n   <td style=\"text-align:right;\"> -0.0126264 </td>\n   <td style=\"text-align:right;\"> -0.1116211 </td>\n   <td style=\"text-align:right;\"> -0.0788855 </td>\n   <td style=\"text-align:right;\"> 9.2039128 </td>\n   <td style=\"text-align:right;\"> -0.3612117 </td>\n   <td style=\"text-align:right;\"> -0.1128537 </td>\n   <td style=\"text-align:right;\"> -0.0123463 </td>\n   <td style=\"text-align:right;\"> -0.0618335 </td>\n   <td style=\"text-align:right;\"> -0.9242423 </td>\n   <td style=\"text-align:right;\"> -0.8889373 </td>\n   <td style=\"text-align:right;\"> -0.278143 </td>\n   <td style=\"text-align:right;\"> -0.2909624 </td>\n   <td style=\"text-align:right;\"> -0.2860174 </td>\n   <td style=\"text-align:right;\"> -0.2909087 </td>\n   <td style=\"text-align:right;\"> -0.2860689 </td>\n   <td style=\"text-align:right;\"> -0.2906768 </td>\n   <td style=\"text-align:right;\"> -0.2912252 </td>\n   <td style=\"text-align:right;\"> -0.2853847 </td>\n   <td style=\"text-align:right;\"> 3.018419 </td>\n   <td style=\"text-align:right;\"> -0.3573269 </td>\n   <td style=\"text-align:right;\"> -0.3275356 </td>\n   <td style=\"text-align:right;\"> -2.355355 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 0.2300274 </td>\n   <td style=\"text-align:right;\"> 0.5052295 </td>\n   <td style=\"text-align:right;\"> -0.1209819 </td>\n   <td style=\"text-align:right;\"> 0.8534573 </td>\n   <td style=\"text-align:right;\"> -0.4358565 </td>\n   <td style=\"text-align:right;\"> -0.1930191 </td>\n   <td style=\"text-align:right;\"> -0.3481648 </td>\n   <td style=\"text-align:right;\"> 0.7946676 </td>\n   <td style=\"text-align:right;\"> -1.726919 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> -0.0101725 </td>\n   <td style=\"text-align:right;\"> -0.007095 </td>\n   <td style=\"text-align:right;\"> -0.007095 </td>\n   <td style=\"text-align:right;\"> -0.0378107 </td>\n   <td style=\"text-align:right;\"> -1.727704 </td>\n   <td style=\"text-align:right;\"> -0.1460816 </td>\n   <td style=\"text-align:right;\"> -0.0073847 </td>\n   <td style=\"text-align:right;\"> -0.0073847 </td>\n   <td style=\"text-align:right;\"> -0.0608575 </td>\n   <td style=\"text-align:right;\"> -0.0465443 </td>\n   <td style=\"text-align:right;\"> -0.029337 </td>\n   <td style=\"text-align:right;\"> -0.0203483 </td>\n   <td style=\"text-align:right;\"> -0.0074788 </td>\n   <td style=\"text-align:right;\"> -0.0068951 </td>\n   <td style=\"text-align:right;\"> -0.2423889 </td>\n   <td style=\"text-align:right;\"> -0.0126264 </td>\n   <td style=\"text-align:right;\"> -0.1116211 </td>\n   <td style=\"text-align:right;\"> -0.0788855 </td>\n   <td style=\"text-align:right;\"> -0.1086493 </td>\n   <td style=\"text-align:right;\"> 2.7684555 </td>\n   <td style=\"text-align:right;\"> -0.1128537 </td>\n   <td style=\"text-align:right;\"> -0.0123463 </td>\n   <td style=\"text-align:right;\"> -0.0618335 </td>\n   <td style=\"text-align:right;\"> -0.9242423 </td>\n   <td style=\"text-align:right;\"> -0.8889373 </td>\n   <td style=\"text-align:right;\"> -0.278143 </td>\n   <td style=\"text-align:right;\"> -0.2909624 </td>\n   <td style=\"text-align:right;\"> -0.2860174 </td>\n   <td style=\"text-align:right;\"> -0.2909087 </td>\n   <td style=\"text-align:right;\"> -0.2860689 </td>\n   <td style=\"text-align:right;\"> -0.2906768 </td>\n   <td style=\"text-align:right;\"> -0.2912252 </td>\n   <td style=\"text-align:right;\"> -0.2853847 </td>\n   <td style=\"text-align:right;\"> 3.018419 </td>\n   <td style=\"text-align:right;\"> -0.3573269 </td>\n   <td style=\"text-align:right;\"> -0.3275356 </td>\n   <td style=\"text-align:right;\"> -2.249099 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 0.5943907 </td>\n   <td style=\"text-align:right;\"> 0.4770352 </td>\n   <td style=\"text-align:right;\"> -0.0981227 </td>\n   <td style=\"text-align:right;\"> 0.1924680 </td>\n   <td style=\"text-align:right;\"> -0.5124558 </td>\n   <td style=\"text-align:right;\"> -0.1930191 </td>\n   <td style=\"text-align:right;\"> -0.3481648 </td>\n   <td style=\"text-align:right;\"> 1.0836135 </td>\n   <td style=\"text-align:right;\"> -1.726555 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> -0.0101725 </td>\n   <td style=\"text-align:right;\"> -0.007095 </td>\n   <td style=\"text-align:right;\"> -0.007095 </td>\n   <td style=\"text-align:right;\"> -0.0378107 </td>\n   <td style=\"text-align:right;\"> -1.727704 </td>\n   <td style=\"text-align:right;\"> -0.1460816 </td>\n   <td style=\"text-align:right;\"> -0.0073847 </td>\n   <td style=\"text-align:right;\"> -0.0073847 </td>\n   <td style=\"text-align:right;\"> -0.0608575 </td>\n   <td style=\"text-align:right;\"> -0.0465443 </td>\n   <td style=\"text-align:right;\"> -0.029337 </td>\n   <td style=\"text-align:right;\"> -0.0203483 </td>\n   <td style=\"text-align:right;\"> -0.0074788 </td>\n   <td style=\"text-align:right;\"> -0.0068951 </td>\n   <td style=\"text-align:right;\"> -0.2423889 </td>\n   <td style=\"text-align:right;\"> -0.0126264 </td>\n   <td style=\"text-align:right;\"> -0.1116211 </td>\n   <td style=\"text-align:right;\"> -0.0788855 </td>\n   <td style=\"text-align:right;\"> -0.1086493 </td>\n   <td style=\"text-align:right;\"> 2.7684555 </td>\n   <td style=\"text-align:right;\"> -0.1128537 </td>\n   <td style=\"text-align:right;\"> -0.0123463 </td>\n   <td style=\"text-align:right;\"> -0.0618335 </td>\n   <td style=\"text-align:right;\"> -0.9242423 </td>\n   <td style=\"text-align:right;\"> -0.8889373 </td>\n   <td style=\"text-align:right;\"> -0.278143 </td>\n   <td style=\"text-align:right;\"> -0.2909624 </td>\n   <td style=\"text-align:right;\"> -0.2860174 </td>\n   <td style=\"text-align:right;\"> -0.2909087 </td>\n   <td style=\"text-align:right;\"> -0.2860689 </td>\n   <td style=\"text-align:right;\"> -0.2906768 </td>\n   <td style=\"text-align:right;\"> -0.2912252 </td>\n   <td style=\"text-align:right;\"> -0.2853847 </td>\n   <td style=\"text-align:right;\"> 3.018419 </td>\n   <td style=\"text-align:right;\"> -0.3573269 </td>\n   <td style=\"text-align:right;\"> -0.3275356 </td>\n   <td style=\"text-align:right;\"> -2.089715 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 0.6984945 </td>\n   <td style=\"text-align:right;\"> 0.4544797 </td>\n   <td style=\"text-align:right;\"> -0.1044725 </td>\n   <td style=\"text-align:right;\"> 0.0381514 </td>\n   <td style=\"text-align:right;\"> -0.5794801 </td>\n   <td style=\"text-align:right;\"> -0.1930191 </td>\n   <td style=\"text-align:right;\"> -0.3481648 </td>\n   <td style=\"text-align:right;\"> 1.2280865 </td>\n   <td style=\"text-align:right;\"> -1.726373 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> -0.0101725 </td>\n   <td style=\"text-align:right;\"> -0.007095 </td>\n   <td style=\"text-align:right;\"> -0.007095 </td>\n   <td style=\"text-align:right;\"> -0.0378107 </td>\n   <td style=\"text-align:right;\"> -1.727704 </td>\n   <td style=\"text-align:right;\"> -0.1460816 </td>\n   <td style=\"text-align:right;\"> -0.0073847 </td>\n   <td style=\"text-align:right;\"> -0.0073847 </td>\n   <td style=\"text-align:right;\"> -0.0608575 </td>\n   <td style=\"text-align:right;\"> -0.0465443 </td>\n   <td style=\"text-align:right;\"> -0.029337 </td>\n   <td style=\"text-align:right;\"> -0.0203483 </td>\n   <td style=\"text-align:right;\"> -0.0074788 </td>\n   <td style=\"text-align:right;\"> -0.0068951 </td>\n   <td style=\"text-align:right;\"> -0.2423889 </td>\n   <td style=\"text-align:right;\"> -0.0126264 </td>\n   <td style=\"text-align:right;\"> -0.1116211 </td>\n   <td style=\"text-align:right;\"> -0.0788855 </td>\n   <td style=\"text-align:right;\"> -0.1086493 </td>\n   <td style=\"text-align:right;\"> 2.7684555 </td>\n   <td style=\"text-align:right;\"> -0.1128537 </td>\n   <td style=\"text-align:right;\"> -0.0123463 </td>\n   <td style=\"text-align:right;\"> -0.0618335 </td>\n   <td style=\"text-align:right;\"> -0.9242423 </td>\n   <td style=\"text-align:right;\"> -0.8889373 </td>\n   <td style=\"text-align:right;\"> -0.278143 </td>\n   <td style=\"text-align:right;\"> -0.2909624 </td>\n   <td style=\"text-align:right;\"> -0.2860174 </td>\n   <td style=\"text-align:right;\"> -0.2909087 </td>\n   <td style=\"text-align:right;\"> -0.2860689 </td>\n   <td style=\"text-align:right;\"> -0.2906768 </td>\n   <td style=\"text-align:right;\"> -0.2912252 </td>\n   <td style=\"text-align:right;\"> -0.2853847 </td>\n   <td style=\"text-align:right;\"> 3.018419 </td>\n   <td style=\"text-align:right;\"> -0.3573269 </td>\n   <td style=\"text-align:right;\"> -0.3275356 </td>\n   <td style=\"text-align:right;\"> -2.058875 </td>\n  </tr>\n</tbody>\n</table></div>\n\n`````\n:::\n:::\n\n\n# Model Building\n\nThen it was finally time to build the predictive models! I used 5 different machine learning algorithms to do so: Linear Regression, K-Nearest Neighbor, Elastic Net Regression, Random Forest, and Gradient-Boosted Trees. All of the model types aside from the Linear Regression had specific hyperparameters that I could tune; hyperparameters are parameters external to the actual modeled data that control the learning process and performance of a model.\n\nThe general steps for building a model using `tidymodels` are:\n\n1)  Specify the model type, computational engine, model mode, and hyperparameters to tune (if applicable). Since my problem involves predicting a continuous outcome variable, I always set the mode to `regression` during this step.\n\n2)  Set up a workflow by combining the model specifications from step 1 with the model recipe.\n\n3)  Create a tuning grid with a range of values for each hyperparameter of the model. Then train and evaluate versions of the model that use different combinations of the hyperparameters using the training data set. Since I have a lot of data and this step can take a long time to run, I saved the model tuning results as .Rds files to avoid re-running the model tuning.\n\n4)  Compare the performance metric across all model versions and select the best one. Finalize the workflow with the best model and its specific hyperparameters.\n\n5)  Fit the final model to the testing dataset to evaluate its predictive performance on new data.\n\nSteps 1-3 were included in the code chunks below for each model type I explored. Step 4 was included in the next section, `Model Results`, and step 5 was included in the `Best Model Results` section below.\n\n### Linear Regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# define model engine and mode\nlm_mod <- parsnip::linear_reg() %>% \n  parsnip::set_engine(\"lm\")\n\n# set up workflow\nlm_wkflow <- workflows::workflow() %>% \n  workflows::add_model(lm_mod) %>% \n  workflows::add_recipe(smart_recipe)\n\n# fit single lm model across all folds of training set\nlm_res <- tune::fit_resamples(lm_wkflow, resamples = smart_folds)\n\n# save en results\nsave(lm_res, file = \"data/inter_data/lm_res.rda\")\n```\n:::\n\n\n### K-Nearest Neighbor\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# define model engine and mode\nknn_mod <- parsnip::nearest_neighbor(neighbors = tune()) %>% \n  parsnip::set_engine(\"kknn\") %>% \n  parsnip::set_mode(\"regression\")\n\n# set up workflow\nknn_wkflow <- workflow() %>% \n  workflows::add_model(knn_mod) %>% \n  workflows::add_recipe(smart_recipe)\n\n# set up grid to tune neighbors\nknn_grid <- grid_regular(neighbors(range = c(1, 10)), levels = 5)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# tune neighbors for knn\nknn_res <- tune::tune_grid(knn_wkflow, grid = knn_grid, resamples = smart_folds,\n                  control = control_grid(verbose = TRUE))\n\n# save en results\nsave(knn_res, file = \"data/inter_data/knn_res.rda\")\n```\n:::\n\n\n### Elastic Net Regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set up model\nen_mod <- parsnip::linear_reg(penalty = tune(), mixture = tune()) %>%\n  parsnip::set_engine(\"glmnet\") %>%\n  parsnip::set_mode(\"regression\")\n\n# set up workflow\nen_wkflow <- workflows::workflow() %>%\n  workflows::add_model(en_mod) %>%\n  workflows::add_recipe(smart_recipe)\n\n# create a regular grid for tuning penalty and mixture \nen_grid <- dials::grid_regular(penalty(range = c(0.01,3), trans = identity_trans()), mixture(range = c(0, 1)), levels = 10)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# tune hyperparameters for en\nen_res <- tune::tune_grid(en_wkflow, grid = en_grid, resamples = smart_folds, \n                          control = control_grid(verbose = TRUE))\n\n# save en results\nsave(en_res, file = \"data/inter_data/en_res.rda\")\n```\n:::\n\n\n### Random Forest\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set up model\nrf_mod <- parsnip::rand_forest(mtry = tune(), \n                               trees = tune(),\n                               min_n = tune()) %>%\n  set_engine(\"ranger\", importance = \"impurity\") %>% \n  set_mode(\"regression\")\n\n# set up workflow\nrf_wkflow <- workflow() %>% \n  add_model(rf_mod) %>% \n  add_recipe(smart_recipe)\n\n# create a regular grid for tuning mtry, trees, and min_n\nrf_grid <- dials::grid_regular(mtry(range = c(1, 15)), # n predictors that will be randomly sampled at each split when creating tree models\n                               trees(range = c(200, 400)), # n of trees contained \n                               min_n(range = c(10, 30)) # min n of data points in a node required for the node to be split further\n                               )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# tune hyperparameters for rf\nrf_res <- tune::tune_grid(rf_wkflow, resamples = smart_folds, grid = rf_grid,\n                  control = control_grid(verbose = TRUE))\n\n# save rf results\nsave(rf_res, file = \"data/inter_data/rf_res.rda\")\n```\n:::\n\n\n### Gradient-Boosted Tree\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set up model\nbt_mod <- boost_tree(mtry = tune(),\n                     trees = tune(), \n                     learn_rate = tune()) %>%\n  set_engine(\"xgboost\") %>% \n  set_mode(\"regression\")\n\n# set up workflow\nbt_wkflow <- workflow() %>% \n  add_model(bt_mod) %>% \n  add_recipe(smart_recipe)\n\n# create a regular grid for tuning mtry, trees, and learning rate\nbt_grid <- grid_regular(mtry(range = c(1, 15)), \n                        trees(range = c(200, 400)),\n                        learn_rate(range = c(-10, -1)),\n                        levels = 5)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# tune hyperparameters for bt\nbt_res <- tune_grid(bt_wkflow, resamples = smart_folds, grid = bt_grid, control = control_grid(verbose = TRUE))\n\n# save rf results\nsave(bt_res, file = \"data/inter_data/bt_res.rda\")\n```\n:::\n\n\n# Model Results\n\nAfter all of the models were built, I compared their performance by evaluating the root mean squared error (RMSE) values of each. The RMSE (root mean squared error) measures the magnitude of error between predicted and actual values - lower RMSE values therefore reflect better model performance. Some of the autoplots also displayed the $R^2$ values of the models, which explains the variance of the actual observed values where 1 is a perfect fit - higher $R^2$ values therefore reflect better model performance.\n\nI use the `autoplot` function to view each model's RMSE and the `show_best` function from the `tune` package to determine the best-performing models from those that I tuned.\n\n### Linear Regression\n\nThere are no hyperparameters to tune in linear regression models, so I only developed one version of this model type. The mean RMSE across all 5 folds of the training data was `0.787` with a standard error of `0.00552`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load lm results\n#load(here::here(\"posts/2024-03-22-machine-learning-weather-energy/data/inter_data/lm_res.rda\"))\n\n# authenticate google drive api\noptions(gargle_oauth_email = \"kristinart21@gmail.com\")\n\n# https://drive.google.com/file/d/1hcnd63aVF3jjjY6kEjwye84XoU4wrwt_/view?usp=sharing\n# define file id from google drive link\nfile_id <- \"1hcnd63aVF3jjjY6kEjwye84XoU4wrwt_\"\n\n# download file and load into R\ndrive_download(as_id(file_id), overwrite = TRUE)\n\n# load into R\nload(\"lm_res.rda\")\n\n# show best model\nlm_res %>% tune::show_best(metric = \"rmse\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 Ã— 6\n  .metric .estimator  mean     n std_err .config             \n  <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   0.787     5 0.00552 Preprocessor1_Model1\n```\n:::\n\n```{.r .cell-code}\n# save best model results\nlm_best <- lm_res %>% tune::show_best(metric = \"rmse\") %>% slice(1)\n```\n:::\n\n\n### K-Nearest Neighbor\n\nIn K-Nearest Neighbor models, we can tune the `K` hyperparameter, which specified the number of neighbors that should be considered when evaluating an observation's expected value. I tuned my K-Nearest Model for 5 values of `K` between 1 to 10.\n\nBased on the autoplot, RMSE decreases and $R^2$ increases as the value of `K` increases. The best performing model version had `K = 10` with a mean RMSE across all 5 folds of `1.06` and a standard error of `0.00465`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# load knn results\n#load(here::here(\"posts/2024-03-22-machine-learning-weather-energy/data/inter_data/knn_res.rda\"))\n\n# https://drive.google.com/file/d/1sTymXM0bfOHraxbZoZcTHd2a5sLTs_Ca/view?usp=drive_link\n# define file id from google drive link\nfile_id <- \"1sTymXM0bfOHraxbZoZcTHd2a5sLTs_Ca\"\n\n# download file and load into R\ndrive_download(as_id(file_id), overwrite = TRUE)\n\n# load into R\nload(\"knn_res.rda\")\n\n# plot\nknn_res %>%\n  autoplot() +\n  theme_minimal() +\n  theme(text = element_text(family = \"lato\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){fig-align='center' width=100%}\n:::\n\n```{.r .cell-code}\n# show best models\nknn_res %>% tune::show_best(metric = \"rmse\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 Ã— 7\n  neighbors .metric .estimator  mean     n std_err .config             \n      <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1        10 rmse    standard    1.06     5 0.00465 Preprocessor1_Model5\n2         7 rmse    standard    1.10     5 0.00456 Preprocessor1_Model4\n3         5 rmse    standard    1.13     5 0.00439 Preprocessor1_Model3\n4         3 rmse    standard    1.16     5 0.00424 Preprocessor1_Model2\n5         1 rmse    standard    1.19     5 0.00395 Preprocessor1_Model1\n```\n:::\n\n```{.r .cell-code}\n# save best model results\nknn_best <- knn_res %>% tune::show_best(metric = \"rmse\") %>% slice(1)\n```\n:::\n\n\n### Elastic Net Regression\n\nIn Elastic Net Regression models, we can tune the `penalty` and `mixture` hyperparameters, which specify the ..., respectively. I tuned my Elastic Net Regression models for 10 levels of `penalty` between 0.01 to 3 and 10 levels of `mixture` between 0 to 1. When `mixture = 0`, the model is actually performing a ridge regression and when `mixture = 1`, the model is performing a lasso regression.\n\nBased on the autoplot, RMSE decreases and $R^2$ increases as the values of `mixture`, or the proportion of lasso penalty, and `penalty`, or the amount of regularization, both decrease. The best performing model version had `mixture = 0.111`, which means it is much closer to being a ridge regression. The best model also had `penalty = 0.01`, a mean RMSE across all folds of `0.788`, and a standard error of `0.00552`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# load en results\n#load(here::here(\"posts/2024-03-22-machine-learning-weather-energy/data/inter_data/en_res.rda\"))\n\n#https://drive.google.com/file/d/1SG3bzqmu669tUFdJleBxV8DIpbM34pxN/view?usp=drive_link\n# define file id from google drive link\nfile_id <- \"1SG3bzqmu669tUFdJleBxV8DIpbM34pxN\"\n\n# download file and load into R\ndrive_download(as_id(file_id), overwrite = TRUE)\n\n# load into R\nload(\"en_res.rda\")\n\n# plot\nen_res %>%\n  autoplot() +\n  theme_minimal() +\n  theme(text = element_text(family = \"lato\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){fig-align='center' width=100%}\n:::\n\n```{.r .cell-code}\n# show best models\nen_res %>% tune::show_best(metric = \"rmse\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 Ã— 8\n  penalty mixture .metric .estimator  mean     n std_err .config               \n    <dbl>   <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                 \n1    0.01   0.111 rmse    standard   0.788     5 0.00552 Preprocessor1_Model011\n2    0.01   0.222 rmse    standard   0.788     5 0.00552 Preprocessor1_Model021\n3    0.01   0.333 rmse    standard   0.789     5 0.00552 Preprocessor1_Model031\n4    0.01   0.444 rmse    standard   0.789     5 0.00552 Preprocessor1_Model041\n5    0.01   0     rmse    standard   0.789     5 0.00547 Preprocessor1_Model001\n```\n:::\n\n```{.r .cell-code}\n# save best model results\nen_best <- en_res %>% tune::show_best(metric = \"rmse\") %>% slice(1)\n```\n:::\n\n\n### Random Forest\n\nIn random forest models, we can tune the `mtry`, `trees`, and `min_n` hyperparameters. `mtry` represents the number of parameters that can be randomly chosen from at each split of the tree. When `mtry` is less than 1, that means the tree will have no parameters to choose from. When `mtry = 15`, the tree has access to all of the predictors at each split, which is the same as bagging. I cannot use a `mtry` greater than 15 because my model recipe only includes 15 predictors, so I used 3 values between 1 to 15. `trees` represents the total number of trees to include in the forest ensemble. I used 3 values of trees between 200 to 400. `min_n` represents the minimum number of observations that need to be in a node in order for it to be split further. I used `min_n` values between 10 to 30 during the tuning process.\\*\\*\n\nBased on the autoplot the number of trees does not make much of a difference in model performance, as all the colored lines are virtually on top of each other. The minimal node size did not appear to make much of a difference either, although it looks like `min_n = 30` had a slightly lower RMSE than the lower values. The models that had access to 8 parameters at every split consistently performed the best. The best model had `mtry = 8`, `trees = 300`, and `min_n = 30` with a mean RMSE across all folds of `0.760` and a standard error of `0.00563`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# load rf results\n#load(here::here(\"posts/2024-03-22-machine-learning-weather-energy/data/inter_data/rf_res.rda\"))\n\n# https://drive.google.com/file/d/1D35V6XB1rZNdxx0WrcwS4uVeOSZKeeE6/view?usp=drive_link\n# define file id from google drive link\nfile_id <- \"1D35V6XB1rZNdxx0WrcwS4uVeOSZKeeE6\"\n\n# download file and load into R\ndrive_download(as_id(file_id), overwrite = TRUE)\n\n# load into R\nload(\"rf_res.rda\")\n\n# plot\nrf_res %>% autoplot() + \n  theme_minimal() +\n  theme(text = element_text(family = \"lato\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){fig-align='center' width=100%}\n:::\n\n```{.r .cell-code}\n# show best rf models\nrf_res %>% tune::show_best(metric = \"rmse\") \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 Ã— 9\n   mtry trees min_n .metric .estimator  mean     n std_err .config              \n  <int> <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1     8   300    30 rmse    standard   0.760     5 0.00563 Preprocessor1_Model23\n2     8   400    20 rmse    standard   0.760     5 0.00562 Preprocessor1_Model17\n3     8   400    30 rmse    standard   0.760     5 0.00562 Preprocessor1_Model26\n4     8   400    10 rmse    standard   0.760     5 0.00562 Preprocessor1_Model08\n5     8   200    30 rmse    standard   0.760     5 0.00562 Preprocessor1_Model20\n```\n:::\n\n```{.r .cell-code}\n# save best model results\nrf_best <- rf_res %>% tune::show_best(metric = \"rmse\") %>% slice(1)\n```\n:::\n\n\n### Gradient-Boosted Tree\n\nIn Gradient-Boosted Decision Tree models, we can tune the `mtry`, `trees`, `min_n`, and `learn_rate` hyperparameters. The first three were described above in the Random Forest model results and the last one, `learn_rate`, represents how fast the boosted tree changes with each iteration. When `learn_rate = 0`, the tree doesn't learn at all and at small values of `learn_rate`, the tree learns very slowly. I tuned my Gradient-Boosted Tree for 5 values of `learn_rate` between $10^{-10}$ to $10^{-1}$. Since this hyperparameter is the most impactful for gradient-boosted trees, I opted not to tune an alternative hyperparameter, `min_n` to reduce the computational power required. I did try out 5 values of `mtry` between 1 to 15 and 5 values of `trees` between 200 to 400 as well though.\n\nBased on the autoplot, models with larger learning rates had the best performance. The number of trees did not make as significant a difference and the number of parameters that were available at every split did not make as much of a difference in most models, although models with `mtry < 4` had slightly worse performance. The best performing model had `mtry = 15`, `trees = 200`, and `learn_rate = 0.1` with a mean RMSE across all folds of `0.755` and a standard error of `0.00568`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# load bt results\n#load(here::here(\"posts/2024-03-22-machine-learning-weather-energy/data/inter_data/bt_res.rda\"))\n\n# https://drive.google.com/file/d/1btCSIyH50_q2OVKrinDxviv1QEN_nk_o/view?usp=drive_link\n# define file id from google drive link\nfile_id <- \"1btCSIyH50_q2OVKrinDxviv1QEN_nk_o\"\n\n# download file and load into R\ndrive_download(as_id(file_id), overwrite = TRUE)\n\n# load into R\nload(\"bt_res.rda\")\n\n# plot\nbt_res %>% autoplot() + \n  theme_minimal() +\n  theme(text = element_text(family = \"lato\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-16-1.png){fig-align='center' width=100%}\n:::\n\n```{.r .cell-code}\n# show best rf models\nbt_res %>% tune::show_best(metric = \"rmse\") \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 Ã— 9\n   mtry trees learn_rate .metric .estimator  mean     n std_err .config         \n  <int> <int>      <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>           \n1    15   200        0.1 rmse    standard   0.755     5 0.00568 Preprocessor1_Mâ€¦\n2    11   200        0.1 rmse    standard   0.755     5 0.00561 Preprocessor1_Mâ€¦\n3    15   250        0.1 rmse    standard   0.755     5 0.00570 Preprocessor1_Mâ€¦\n4    11   250        0.1 rmse    standard   0.755     5 0.00562 Preprocessor1_Mâ€¦\n5     8   250        0.1 rmse    standard   0.755     5 0.00563 Preprocessor1_Mâ€¦\n```\n:::\n\n```{.r .cell-code}\n# save best model results\nbt_best <- bt_res %>% tune::show_best(metric = \"rmse\") %>% slice(1)\n```\n:::\n\n\n# Best Model Results\n\nOnce I combined the lowest RMSE from each model type, it was clear that a Gradient-Boosted Tree was the winner! It has a slightly lower RMSE than the runner-ups and predicted `hourly_average_power_kw` values that deviate from the actual observed values by approximately 0.755 kw on average. As mentioned above, the best performing model had `mtry = 15`, `trees = 200`, and `learn_rate = 0.1` with a mean RMSE across all folds of `0.755` and a standard error of `0.00568`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# combine best performance results from each model type\nall_rmse <- tibble(Model = c(\"Linear Regression\", \"K-Nearest Neighbor\", \"Elastic Net Regression\", \"Random Forest\", \"Gradient-Boosted Trees\"), RMSE = c(lm_best$mean, knn_best$mean, en_best$mean, rf_best$mean, bt_best$mean)) %>% \n  mutate(RMSE = round(RMSE, 3)) %>% \n  arrange(RMSE)\n\nall_rmse %>% \n  kable() %>% \n  kable_styling(full_width = F, font = \"lato\") \n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"font-size: latopx; width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Model </th>\n   <th style=\"text-align:right;\"> RMSE </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Gradient-Boosted Trees </td>\n   <td style=\"text-align:right;\"> 0.755 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Random Forest </td>\n   <td style=\"text-align:right;\"> 0.760 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Linear Regression </td>\n   <td style=\"text-align:right;\"> 0.787 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Elastic Net Regression </td>\n   <td style=\"text-align:right;\"> 0.788 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> K-Nearest Neighbor </td>\n   <td style=\"text-align:right;\"> 1.060 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nNext I finalized the workflow using the hyperparameters from the best Gradient-Boosted Tree model and fit it to the entire training dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# save best model \nbest_mod <- bt_res %>% \n  tune::select_best(metric = \"rmse\", mtries, trees, learn_rate)\n\n# finalize workflow with best model\nfinal_wkflow <- tune::finalize_workflow(bt_wkflow, best_mod)\n\n# fit model to training data\nfinal_fit_mod <- parsnip::fit(final_wkflow, smart_train)\n```\n:::\n\n\nOne cool thing about tree-based models is we can visualize which predictors were the most significant drivers of the outcome through a variable importance plot (VIP). Based on the VIP for the best model, the PC1 feature was the most important predictor variable by far; remember, the PC1 feature was extracted from the original `temperature`, `apparent_temperature`, and `dew_point` parameters, which were highly correlated (\\> 90%) to each other. The `time` parameter was the second most important variable, followed by `hour` and `months`. This result makes sense but surprised me! I was expecting more weather-related parameters to show up in the top 10.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# create variable importance plot using training data\nfinal_fit_mod %>% \n  workflowsets::extract_fit_parsnip() %>% \n  vip::vip(aesthetics = list(fill = \"#DAA49A\", color = \"#875053\")) +\n  theme_minimal() +\n  theme(text = element_text(family = \"lato\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-19-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\nNext, it was finally time to introduce the best model to data its never seen before! I ran the model on the testing dataset to see how well it could predict values it was not trained on. The model's RMSE on the testing dataset was 0.835, which is only slightly worse than the mean RMSE from the training process. This indicates that the training RMSE across 5-folds was a pretty good indicator of the model's overall performance.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# assess model performance on entire testing set\nfinal_mod_test <- augment(final_fit_mod, smart_test) %>% \n  rmse(truth = hourly_average_power_kw, estimate = .pred) %>% \n  print()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 Ã— 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard       0.835\n```\n:::\n:::\n\n\nWhen I plotted the predicted values of power usage against the actual observed values, it was clear that the model does not predict high values well at all; in fact, the model did not predict any power usage values higher than 3.75 kW. This is due to that strong positive skew in the outcome variable, which means even our best model was only trained with a handful of observations for the higher power usage values. Only the values that fall directly on the diagonal line in the plot below were accurately predicted by the model.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# plot predicted vs. actual values from testing data\naugment(final_fit_mod, smart_test) %>% \n  ggplot(aes(x = .pred, y = hourly_average_power_kw)) +\n  geom_point(color = \"#DAA49A\", alpha = 0.2) +\n  geom_abline(lty = 2) +\n  coord_obs_pred() +\n  labs(title = \"Predicted vs. Actual Values of Hourly Average Power Usage (kW)\",\n       y = \"Actual Hourly Average Power Usage (kW)\",\n       y = \"Predicted Hourly Average Power Usage (kW)\") +\n  theme_minimal()+\n  theme(text = element_text(family = \"lato\"),\n        plot.title = element_text(hjust = 0.5))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-21-1.png){fig-align='center' width=100%}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Update this with fake or untested data for blog post when have mroe time. \n# For fun, I used the model to predict residential power usage during the warmest and coolest observations in smart df. \n# filter to observations with highest and lowest temperature\nextreme_temps <- smart_mod %>%\n  filter(row_number() == which.max(temperature) | row_number() == which.min(temperature))\n\nfinal_predictions <- augment(final_fit_mod, extreme_temps) %>% \n  select(.pred, hourly_average_power_kw, temperature, everything())\n\nfinal_predictions \n```\n:::\n\n\n## Conclusion\n\nThe best model for predicting residential energy usage based on weather and time was a Gradient-Boosted Tree model. A Random Forest model did nearly as well, with only a small difference in RMSE separating the two. This is no surprise since decision tree-based models tend to work really well on many types of data. This is because tree-based models are highly flexible and non-parametric, meaning they do not assume any parametric constraints on the outcome variable. Interestingly, the Elastic Net Regression and Linear Regression models had a pretty similar performance to these top tree-based models, indicating that they also do a decent job of predicting residential energy usage; these model types could be more useful to a user who is willing to accept a little higher error in order to use models that are much less computationally expensive. The K-Nearest Neighbor model had the worst predictive power, which makes sense since KNN models tend to do poorly on data with too many predictors, or high dimensions.\n\nOverall, the best model did an okay job at predicting residential energy usage. The actual values of the outcome variable, `hourly_average_power_kw`, ranged between 0 to 11.5 kW. On average, the best model's predicted values were about 0.835 kW off from the actual observed values based on the testing RMSE; this means the model's average error was about 7% of the entire range. The model could be improved by adding in more data values that have high energy usage values. By normalizing the distribution of the outcome variable in this way, I could improve the model's learning and potentially improve its performance.\n\nIt was interesting to see that the principal component of `temperature`, `apparent_temperature`, and `dew_point` is the most important predictor of residential energy usage. Since apparent temperature itself is also a factor of relative humidity and wind speed, this principal component feature may represent all the key weather predictors due to their interrelatedness. Therefore, it makes sense that time variables (hour, time, month) would be the next most important predictors in the VIP rather than some of the general weather predictors that remained (descriptive weather summary, wind-bearing, etc.). This makes intuitive sense and it was fun to see it explained quantitatively. I learned a lot through this project and am looking forward to exploring more energy data with machine learning algorithms!\n\n:::",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}